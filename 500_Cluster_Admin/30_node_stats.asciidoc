
=== 개별 node의 모니터링

`Cluster-health`는 cluster에서 매우 높은 수준의 개요 부분의 일부분이다.  ((("clusters", "administration", "monitoring individual nodes")))((("nodes", "monitoring individual nodes")))
`node-stats` API는 또 다른 일부분이다. ((("Node Stats API", id="ix_NodeStats", range="startofrange")))
이것은 cluster의 각 node에 대한 복잡한 통계의 배열을 제공한다.

`Node-stats` 는 출력에 익숙해질 때까지 계속 지켜봐도, 어느 통계가 가장 중요한지 모를 정도로,
많은 통계를 제공한다. 모니터 해야 하는 가장 중요한 통계를 강조할 것이다.(그러나 어떤 통계가
필요할지 알 수 없기 때문에, 제공하는 모든 통계를 기록할 것--또는 Marvel 을 사용할 것--을 권장한다.)

`node-stats` API는 다음과 같이 실행할 수 있다:

[source,bash]
----
GET _nodes/stats
----

출력의 윗부분부터 시작해 보면, cluster의 이름과 첫 번째 node를 볼 수 있다:

[source,js]
----
{
   "cluster_name": "elasticsearch_zach",
   "nodes": {
      "UNr6ZMf5Qk-YCPA_L18BOQ": {
         "timestamp": 1408474151742,
         "name": "Zach",
         "transport_address": "inet[zacharys-air/192.168.1.131:9300]",
         "host": "zacharys-air",
         "ip": [
            "inet[zacharys-air/192.168.1.131:9300]",
            "NONE"
         ],
...
----

node는 node의 UUID를 key로 해서 hash에 나열되어 있다. node의 네트워크 속성에 대한 일부
정보(transport address, host 등)가 표시된다. 이들 정보는 cluster에 가입되지 않은 node를
발견하는 문제를 디버깅하는데 유용하다. 가끔씩, 사용된 port가 올바르지 않거나 node가 올바르지
않은 IP address/interface에 바인딩 되어 있는 것을 알 수 있다.

==== indices 섹션

`indices` 섹션은 특정 node에 있는 모든 index에 대한 집계 통계를 나열한다: ((("indices", "indices section in Node Stats API")))

[source,js]
----
    "indices": {
        "docs": {
           "count": 6163666,
           "deleted": 0
        },
        "store": {
           "size_in_bytes": 2301398179,
           "throttle_time_in_millis": 122850
        },
----

응답된 통계들은 다음 섹션안에 묶여져 있다:
- `docs` 는 이 node에 있는 document의 수와 아직 segment에서 제거되지 않은
삭제된(deleted) document의 수를 나타낸다.

- `store`는 node에 의해 사용된 물리적 저장소의 크기를 가리킨다. 이 통계는 primary와
replica shard 모두를 포함한다. 만약 throttle time이 너무 크면, 그것은 디스크 throttling(조절)
이 너무 낮게 설정되어 있다는 것을 나타내는 지표이다. ( <<segments-and-merging>>에서 언급할 것이다.)

[source,js]
----
        "indexing": {
           "index_total": 803441,
           "index_time_in_millis": 367654,
           "index_current": 99,
           "delete_total": 0,
           "delete_time_in_millis": 0,
           "delete_current": 0
        },
        "get": {
           "total": 6,
           "time_in_millis": 2,
           "exists_total": 5,
           "exists_time_in_millis": 2,
           "missing_total": 1,
           "missing_time_in_millis": 0,
           "current": 0
        },
        "search": {
           "open_contexts": 0,
           "query_total": 123,
           "query_time_in_millis": 531,
           "query_current": 0,
           "fetch_total": 3,
           "fetch_time_in_millis": 55,
           "fetch_current": 0
        },
        "merges": {
           "current": 0,
           "current_docs": 0,
           "current_size_in_bytes": 0,
           "total": 1128,
           "total_time_in_millis": 21338523,
           "total_docs": 7241313,
           "total_size_in_bytes": 5724869463
        },
----

- `indexing` 은 색인된 document의 수를 나타낸다. 이 값은 단순하게 증가하는 counter이고;
docs가 삭제되어도 감소하지 않는다. 그것은 업데이트 같은 것을 포함하여, 내부적으로 발생하는
_index_ 연산에도 항상 증가한다.
+
또한, 색인 횟수, 현재 색인된 document의 수, 그리고 삭제에 대한 유사한 통계가 나열된다.

- `get` 는 ID로 GET하는(get-by-ID) 통계에 대한 통계를 나타낸다. 이것은 단일 document에
 대한 `GET` 과 `HEAD` 요청을 포함한다.

- `search` 는 node가 시작되고 나서, 활성화된 검색(`open_contexts`)의 수, 총 query의 수,
 query에 소요된 시간을 나타낸다. `query_time_in_millis / query_total` 의 비율은 query의
효율성을 나타내는 지표로 사용된다. 비율이 클수록, 각 query에는 더 많은 시간이 소요되고,
조정이나 최적화를 고려해야 한다.
+
fetch 통계는 query 프로세스(query-then-fetch의 _fetch_)의 후반부에 대한 세부사항이다.
만약 fetch가 query보다 더 많은 시간이 소요된다면, 이것은 느린 디스크, 가져올 document가
매우 많다거나 잠재적으로 페이지를 계산하기에 너무 큰(예를 들면 `size: 10000`) 검색
결과라는 것을 나타내는 지표가 될 수 있다.

- `merges` 는 Lucene segment 병합에 대한 정보를 포함하고 있다. 현재 활성화된 병합의 수,
포함된 document의 수, 병합될 segment의 누적 크기, 병합에 소요된 총 시간을 나타낸다.
+
merge 통계는 cluster가 쓰는 동작에 부하가 많은 경우 중요할 수 있다. 병합은 많은 양의
디스크 I/O와 CPU 자원을 소모한다. index가 쓰는 동작에 부하가 많고 큰 병합 수를 봤다면,
<<indexing-performance>>를 읽어 보자.
+
Note: 업데이트와 삭제는 병합 수를 너무 많이 늘리는데 한다. 왜냐하면 그것이
결국에는 병합되어야 하는 segment의 단편화( _fragmentation_ )를 발생시키기 때문이다.
Note: updates and deletes will contribute to large merge numbers too, since they
cause segment _fragmentation_ that needs to be merged out eventually.

[source,js]
----
        "filter_cache": {
           "memory_size_in_bytes": 48,
           "evictions": 0
        },
        "id_cache": {
           "memory_size_in_bytes": 0
        },
        "fielddata": {
           "memory_size_in_bytes": 0,
           "evictions": 0
        },
        "segments": {
           "count": 319,
           "memory_in_bytes": 65812120
        },
        ...
----

- `filter_cache` 는 cached filter bitset에 사용된 메모리의 양과 filter가 축출된 횟수를
나타낸다. evictions이 크다는 것은 filter cache 크기를 증가시켜야 하거나, filter가 잘
caching되지 않는다(data 표현인 `now` 로 caching하는 것처럼, 높은 cardinality 때문에)는
것을 _가리킬 수 있다_.
+
그러나, evictions은 측정하기 어려운 통계이다. filter는 기본적으로 segment별로 cache된다.
그리고, filter를 작은 segment에서 축출하는 것은 큰 segment보다 훨씬 적은 비용이 소모된다.
다수의 추출도 가능하지만, 그것은 query의 성능에 거의 영향을 미치지 않는 작은 segment에서
발생한다.
+
evictions 통계를 대략적인 지침으로 사용하자. 큰 숫자를 봤다면, filter가 잘 caching되고
있는지 filter를 확인해 보자. 작은 segment에서조차, 계속 축출되는 filter는 적절한 cache된
filter보다 훨씬 덜 효과적이다.

- `id_cache`는 부모-자식 mapping에 의해 사용된 메모리의 양을 나타낸다. 부모-자식을
사용하면, `id_cache` 는 그 관계를 유지하기 위해 메모리에 조인(in-memory-join) 테이블을
유지한다. 이 통계는 사용된 메모리의 양을 나타낸다. parent/child docs의 수는 명백한
선형관계이기 때문에, 이 메모리 사용량에 거의 영향을 줄 수 없다. 그러나, 그것은 heap에 존재한다.
따라서, 그것을 계속 지켜보는 것이 좋다.

- `field_data` 는 fielddata ((("fielddata", "statistics on"))) 에 의해 사용된 메모리를 나타낸다.
이것은 집계, 정렬 등에 사용된다. evictions의 수도 있다. `filter_cache`와 달리, 여기에서 evictions
수는 매우 유용하다. 그것은 0이거나 0에 매우 근접해야 한다. field data는 cache가 아니기 때문에, 모든
evictions은 매우 비싸고, 피해야 한다. 여기에서 evictions을 본다면, 메모리 환경, fielddata limits,
query나 이것 모두를 다시 검토해야 한다.

- `segments`는 이 node에서 현재 제공되는 Lucene segment가 몇 개인지를 나타낸다. ((("segments", "number served by a node")))
이것은 중요한 숫자가 될 수 있다. 대부분의 index는 수십억 개의 document를 가진 크기가
수 TB(terrabyte)일지라도, 50&#x2013;150개 정도의 segment를 가진다. 많은 segment는 병합에
문제(예를 들어, 병합이 segment 생성을 알지 못함)가 있음을 나타낼 수 있다.
이 통계는 node에 있는 모든 index의 총 합이라는 것을 알아두자.
+
`memory` 통계는 Lucene segment 자체에 사용되는 메모리의 양을 나타낸다.((("memory", "statistics on")))
이것은 posting lists, dictionaries, bloom filters 같은 낮은 수준의 데이터 구조를 포함한다.
매우 많은 segment는 이러한 데이터 구조에 대한 오버헤드 손실의 양이 증가할 것이다.
그리고 메모리 사용량은 해당 오버헤드를 측정하는 편리한 통계일 수 있다.

==== OS와 Process 섹션

`OS` and `Process` 는 사실 따로 설명할 필요가 없어, 자세히 설명하지 않을 것이다. ((("operating system (OS), statistics on")))
CPU와 부하 같은 기본적인 자원 통계를 나열한다. ((("process (Elasticsearch JVM), statistics on"))) `Process` 섹션은
Elasticsearch의 JVM 프로세스가 사용하는 것만을 보여주는 반면에,
`OS` 섹션은 전체 `OS`를 나타낸다.

이것은 분명히 유용한 통계이지만, 모니터링의 다른 곳에서도 흔히 측정된다.
일부 통계는 다음을 포함 한다:

- CPU
- Load
- Memory usage
- Swap usage
- Open file descriptors

==== JVM 섹션

`jvm` 섹션은 Elasticsearch를 실행하는 JVM 프로세스에 대한 몇 가지 중요한 정보를 포함하고
있다. ((("JVM (Java Virtual Machine)", "statistics on"))) 가장 중요한, Elasticsearch의 cluster의 안정성에 큰 영향을 미칠 garbage collection
의 세부 사항을 포함하고 있다.

[[garbage_collector_primer]]
.Garbage Collection Primer
**********************************
통계를 설명하기 전에, garbage collection의 충돌 과정을 설명하는 것은 유용하며,
그것은 Elasticsearch에 영향을 미친다. ((("garbage collection"))) JVM의 garbage collection에 익숙하다면,
아래로 넘어가도 무방하다.

Java는 프로그래머가 메모리의 할당과 해제를 직접 관리하지 않는다는 것을 의미하는
_garbage-collected_ 언어이다. 프로그래머는 단순하게 코드를 작성하고,
JVM(Java Virtual Machine)은 필요한 메모리의 할당하고, 나중에 더 이상 필요하지 않을 때
해당 메모리를 정리하는 과정을 관리한다.

메모리가 JVM 프로세스에 할당될 때,  _heap_ 이라는 커다란 덩어리(chunk)에 할당된다.
그 다음에 JVM은 heap을 _generations_ 라 불리는 2개의 다른 그룹으로 나눈다:

Young (or Eden)::
    새로이 초기화된 오브젝트가 할당되는 공간. young-gen은 보통 100 MB&#x2013;500 MB로
아주 작다. young-gen은 또한 두 개의 _survivor_ 공간을 포함한다.

Old::
    오래된 오브젝트가 저장되는 공간. 긴 시간 존재하고 있는 이들 오브젝트는 오랫동안 지속된다.
old-gen은 일반적으로 young-gen보다 매우 크다. 그리고 Elasticsearch의 node는 30GB나
되는 old-gen을 볼 수 있다.

오브젝트가 인스턴스화 될 때, 그것은 young-gen에 배치된다. young-gen이 가득 차면,
young-gen의 garbage collection(GC)이 시작된다. 아직 “살아있는” 오브젝트는 survivor 공간
중의 하나로 이동한다. 그리고 “죽은” 오브젝트는 제거된다. 어떤 오브젝트가 young-gen의
garbage collection에서 여러 번 살아남으면, old-gen에서 “tenured(거주)”하게 된다.

old-gen에서도 유사한 프로세스가 일어난다: 공간이 가득 차면, garbage collection이 시작되고,
“죽은” 오브젝트는 제거된다

공짜는 없다. 그러나 young-gen과 old-gen의 GC는 “stop the world”라는 단계를 가진다.
이 시간 동안, JVM은 오브젝트 그래프를 추적하고, “죽은” 오브젝트를 수집할 수 있도록,
말 그대로, 프로그램의 실행을 멈춘다. 이 “stop the world” 단계에서는 아무런 일도 발생하지
않는다. 요청은 서비스되지 않고, ping은 응답하지 않고, shard는 재할당되지 않는다.
말 그대로 세상이 멈춘다.

이것은 young-gen에서는 큰 문제가 아니다. 작은 크기는 garbage collection가 빠르게 실행된다는
것을 의미한다. 그러나, old-gen는 꽤 크고, 여기에서 느린 GC는 1초, 또는 심지어 15초의 정지
(서버 S/W로서는 받아들일 수 없는)를 의미한다.

JVM의 garbage collection는 _매우_ 정교한 알고리즘이며, 정지를 최소화하며 훌륭하게 작업을
하고 있다. 그리고, Elasticsearch는 지능적으로, 내부적으로 오브젝트를 재사용하고, 네트워크
버퍼를 재사용하고, <<doc-values>> 같은 기능을 제공함으로써, _garbage-collection 친화적_
이기 위해 매우 열심히 노력한다. 하지만 궁극적으로는, garbage collection의 빈도와
기간은 지켜봐야 할 통계 중 하나이다. 왜냐하면, 그것이 cluster 불안정성의 주범이기 때문이다.

긴 garbage collection를 자주 경험하는 cluster는 메모리가 충분하지 않은 부하가 많은
cluster일 것이다. 이러한 긴 garbage collection로 cluster에서 node가 잠시 동안 빠질 것이다.
이 불안정성은 Elasticsearch가 cluster의 균형을 맞추고 replica를 충분히 활용하기 위해,
shard를 빈번히 재배치하는 원인이 된다. 이것은 결국 네트워크 트래픽과 디스크 I/O를 증가시킨다.
동시에, cluster는 정상적인 색인과 query를 서비스하기 위해 시도하면서 부하를 증가시킨다.

짧게 이야기 하면, 긴 GCs는 좋지 않다. 그리고 가능한 한 최소화되어야 한다.
**********************************

garbage collection은 Elasticsearch에 너무나 중요하기 때문에, `node-stats` API의 부분을
속속들이 알고 있어야 한다:

[source,js]
----
        "jvm": {
            "timestamp": 1408556438203,
            "uptime_in_millis": 14457,
            "mem": {
               "heap_used_in_bytes": 457252160,
               "heap_used_percent": 44,
               "heap_committed_in_bytes": 1038876672,
               "heap_max_in_bytes": 1038876672,
               "non_heap_used_in_bytes": 38680680,
               "non_heap_committed_in_bytes": 38993920,

----

- `jvm` 섹션은 먼저, heap 메모리 사용에 대한 몇 가지 일반적인 통계를 나열한다.
사용된 heap의 양, commit된(실제로 프로세스에 할당된) 양, 커질 수 있는 heap의 최대 크기를
알 수 있다. 이상적으로, `heap_committed_in_bytes` 는 `heap_max_in_bytes` 와 동일해야 한다.
commit된 크기가 더 작다면, JVM은 결국 heap의 크기를 조정해야 한다. 그리고, 그것은 매우
비싼 프로세스다. 숫자가 동일하지 않다면, 그것을 올바르게 구성하는 방법에 대한 <<heap-sizing>>
을 참조하자.
+
`heap_used_percent` 통계는 계속 지켜봐야 할 유용한 숫자이다. Elasticsearch는 heap이
전체의 75%에 도달할 때, GC들을 시작하도록 구성되어 있다. node가 지속적으로
75% 이상이면, node는 _메모리 압박_ 을 경험하게 된다. 조만간 느린 GC들이 나타난다는 경고이다.
+
heap의 사용량이 지속적으로 85% 이상이면, 문제가 발생한 것이다. heap이 90&#x2013;95% 이상이면
낙관적으로 보더라도 10&#x2013;30 동안의 긴 GC들이 일어나고 있고, 최악의 경우에는
Out-of-Memory(OOM) Exception이 발생하는 끔찍한 성능상의 위험에 빠진 것이다.

[source,js]
----
   "pools": {
      "young": {
         "used_in_bytes": 138467752,
         "max_in_bytes": 279183360,
         "peak_used_in_bytes": 279183360,
         "peak_max_in_bytes": 279183360
      },
      "survivor": {
         "used_in_bytes": 34865152,
         "max_in_bytes": 34865152,
         "peak_used_in_bytes": 34865152,
         "peak_max_in_bytes": 34865152
      },
      "old": {
         "used_in_bytes": 283919256,
         "max_in_bytes": 724828160,
         "peak_used_in_bytes": 283919256,
         "peak_max_in_bytes": 724828160
      }
   }
},
----

- `young`, `survivor`, 그리고 `old` 섹션은 GC에 있는 각 generations안의 메모리
사용량에 대한 고장을 제공한다. 이러한 통계는 상대적인 크기를 계속 지켜보기에 편리하지만,
문제점을 디버깅하는 경우에는 그렇게 중요하지 않다.

[source,js]
----
"gc": {
   "collectors": {
      "young": {
         "collection_count": 13,
         "collection_time_in_millis": 923
      },
      "old": {
         "collection_count": 0,
         "collection_time_in_millis": 0
      }
   }
}
----

- `gc` 섹션은 GC의 수와 young과 old generations 양쪽 모두에 대한 누적 시간을 나타낸다.
대부분의 경우 young generation의 수는 무시할 수 있다: 이 숫자는 일반적으로 매우 큰데,
그것은 완벽하게 정상이다.
+
이와 반대로, old generation의 수는 매우 작게 유지되어야 하고, 작은 `collection_time_in_millis`
을 가져야 한다. 이것은 누적된 수이다. 따라서, 주의하기 시작할 정확한 숫자를 제공하기가 어렵다.
예를 들어 1년 동안 가동된 node일 경우, 그 node가 이상이 없더라도 매우 큰 숫자를 가진다.
이것이 marvel 같은 tool이 유용한 이유 중의 하나이다. _시간 별_ GC들의 수는 중요한 고려 사항이다.
+
GC에 걸리는 시간도 역시 중요하다. 예를 들어, garbage의 특정 양은 document를 색인하는
동안 만들어진다. 이것은 정상이며, 가끔씩 GC가 발생한다. 이러한 GC는 거의 항상 빠르고,
node에 거의 영향을 미치지 않는다. young generation는 1&#x2013;2ms 정도 소요되며,
old generation는 수백 ms 정도 소요된다. 이것은 10-second GCs와는 많이 차이가 난다.
+
최선의 충고는 garbage collection의 수와 기간을 주기적으로 수집하고(또는 marvel을 사용),
빈번한 GC들를 주의해야 한다는 것이다. 느린 GC에 대한 기록을 활성화할 수 있다. <<logging>>
에서 이야기하자.

==== Threadpool 섹션

Elasticsearch는 내부적으로 threadpool의 번호를 유지한다. ((("threadpools", "statistics on"))) 이들 threadpool 은 필요에 따라
서로 간에 작업을 전달하며, 작업을 끝내기 위해 협력한다. 일반적으로, threadpool 을 구성하거나
조정할 필요가 없다. 하지만, cluster가 어떻게 동작하고 있는지를 알아보기 위해, threadpool의
통계를 알아보는 것은 유용하다.

약 12개의 threadpool이 있다. 그러나 그들은 모두 동일한 형식을 공유한다.

[source,js]
----
  "index": {
     "threads": 1,
     "queue": 0,
     "active": 0,
     "rejected": 0,
     "largest": 1,
     "completed": 1
  }
----

각 threadpool은 구성된 thread의 수(`threads`), 그 thread 중 실제로 어떤 작업을 처리하고
있는 thread의 수(`active`), queue에 있는 작업 단위의 수(`queue`)를 나열한다.

queue가 그것의 한계까지 차면, 새로운 작업은 거부되기 시작하고, `rejected` 통계에서 거부된 것을
볼 수 있다. 이것은 cluster에서 어떤 자원에 대한 병목 현상이 시작되었다고 자주 나타나는 신호이다.
왜냐하면 가득 찬 queue는 node/cluster가 최대한의 속도로 처리하고 있지만, 들어오는 작업에
맞출 수가 없다는 것을 의미하기 때문이다.

.Bulk 거부
****
queue 거부가 발생하고 있다면, 그것은 아마도 bulk 색인 요청 때문일 것이다. ((("bulk API", "rejections of bulk requests")))
동시 import 프로세스를 사용하여, Elasticsearch에 많은 bulk 요청을 보내는 것은 쉽다.
많을수록 더 좋다. 정말?

실제로는, 각 cluster는 받아들이는데 특정 제한이 있다. 이 제한에 걸리면, queue는 채워지고,
bulk는 거부된다.

이것은 _좋은 일_ 이다. queue 거부는 배압(back-pressure)의 유용한 형태이다. 그것으로
cluster가 최대 용량에 있다는 것을 알 수 있다. 그리고 메모리에 있는 queue에 데이터를 계속
넣는 것보다 훨씬 났다. queue의 크기를 증가시키는 것이 성능을 향상시키는 것이 아니고 단지
문제를 숨길 뿐이다. cluster가 초당 10,000개의 document를 처리할 수 있다면, queue의 크기가
100 또는 10,000,000이든 관계없이, cluster는 여전히 초당 10,000개의 document를 처리할 수
있을 뿐이다.

queue는 단순히 성능 문제를 숨기고, 실제 데이터 손실의 위험을 가지고 있을 뿐이다. queue에
있는 모든 것은 의미상으로 아직 처리되지 않은 것이다. node가 다운되면, 모든 요청은 영원히
사라진다. 게다가 queue는 많은 메모리를 잡아먹는기에 이상적이지 않다.

전체 queue로부터 배압(back-pressure)을 처리하여, 응용프로그램에서 queue를 처리하는 것이
훨씬 더 낫다. bulk 거부가 발생했을 때, 아래와 같이 할 수 있다:

1. import thread를 3&#x2013;5초 정도 멈춘다.
2. bulk 응답에서 거부된 작업을 뽑아낸다. 왜냐하면, 대부분의 작업은 성공했을 가능성이 있기
때문이다. bulk 응답은 어느 것이 성공했는지 거부됐는지를 알려준다.
3. 거부된 작업만을 가진 새로운 bulk 요청을 보낸다.
4. 만약 다시 거부되면, 1단계부터 반복한다.

이 절차를 사용하면, 코드는 자연스럽게 cluster의 부하에 적응한다.

거부는 에러가 아니다. 나중에 다시 시도하라는 듯이다.
****

12개의 다른 threadpool이 있다. 대부분은 무시할 수 있지만, 몇 개는 계속 지켜보는 것이 좋다:

`indexing`::
    정상적인 색인 요청을 위한 threadpool

`bulk`::
    bulk 요청이 아닌 색인 요청과 구분되는 bulk 요청

`get`::
    별 Get (Get-by-ID) 연산

`search`::
    모든 검색과 query 요청

`merging`::
    Lucene의 병합을 관리하는 전용 threadpool

==== FS and Network 섹션

`node-stats` API를 계속 내려보면, filesystem에 대한 통계를 볼 수 있다((("filesystem, statistics on"))):
여유 공간, 데이터 디렉토리 경로, 디스크 I/O 상태, 그 밖에 것. 만일 디스크 여유 공간을
모니터링하지 않는다면, 여기에서 해당 통계를 얻을 수 있다. 디스크 I/O 통계도 편리하지만,
때로는 전문화된 command-line tool (예를 들어 `iostat`)이 더 유용하다.

명백히, 디스크 공간이 부족하면, Elasticsearch는 동작하는데 어려움이 있다. 따라서
그렇지 않는가 확인해야 한다.

네트워크 통계에도 역시 두 섹션 ((("network", "statistics on")))이 있다:

[source,js]
----
        "transport": {
            "server_open": 13,
            "rx_count": 11696,
            "rx_size_in_bytes": 1525774,
            "tx_count": 10282,
            "tx_size_in_bytes": 1440101928
         },
         "http": {
            "current_open": 4,
            "total_opened": 23
         },
----

- `transport` 는 _transport address_ 에 대한 매우 기본적인 통계를 나타낸다. 이것은
node간 통신(흔히 9300)과 TransportClient 이나 NodeClient 연결에 관한 것이다.
여기에 많은 연결이 있다고 걱정하지 말자. Elasticsearch는 node 사이에 많은 수의 연결을
유지한다.

- `http` 는 HTTP port(흔히 9200)에 대한 통계를 나타낸다. 끊임없이 증가하는, 아주 큰
`total_opened` 수를 본 경우에, 그것은 HTTP 클라이언트 중 하나가 keep-alive 연결을
사용하지 않는다는 것을 확인하는 신호이다. 지속적인 keep-alive 연결은 성능을 위해
중요하다. 왜냐하면, 소켓을 구축하고 없애는 것은 비싼 작업이다. (그리고 file
descriptors의 낭비이다.) 클라이언트가 적절하게 구성되어 있는지 확인하자.

==== Circuit Breaker

끝으로, 마직막 섹션: ((("fielddata circuit breaker"))) field data의 circuit breaker
( <<circuit-breaker>>에서 소개하였다.)에 대한 통계이다.

[role="pagebreak-before"]
[source,js]
----
         "fielddata_breaker": {
            "maximum_size_in_bytes": 623326003,
            "maximum_size": "594.4mb",
            "estimated_size_in_bytes": 0,
            "estimated_size": "0b",
            "overhead": 1.03,
            "tripped": 0
         }
----

여기에서 circuit breaker의 최대 크기를 확인할 수 있다. (예를들어 query가 더 많은
메모리를 사용하려 하는 경우에, circuit breaker가 동작하는 크기는 얼마인가) 또한,
circuit breaker가 동작한 횟수와 현재 설정된 “overhead”를 알 수 있다. 일부 query는
다른 것보다 추정하기가 어렵기 때문에, overhead는 추정치를 추가하는데 사용된다.

중요한 것은 `tripped` 단가이다. 이 숫자가 크거나 지속적으로 증가하면, query를
최적화하거나 더 많은 메모리를 확보(박스 별로, 또는 더 많은 node를 추가)해야 한다는
신호이다.((("Node Stats API", range="endofrange", startref="ix_NodeStats")))
