
<!DOCTYPE HTML>
<html lang="en-us">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>제품의 배포</title><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><link rel="home" href="index.html" title="Elasticsearch: The Definitive Guide" /><link rel="up" href="administration.html" title="관리, 모니터링, 배포" /><link rel="prev" href="cluster-admin.html" title="모니터링" /><link rel="next" href="post_deploy.html" title="배포 후" />
  
 	<meta http-equiv="content-type" content="text/html; charset=utf-8" />

 	<!-- RTP tag --> 
	<script type='text/javascript' async>
	(function(c,h,a,f,i,e){c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
	 c[a].a=i;c[a].e=e;var g=h.createElement("script");g.async=true;g.type="text/javascript";
	 g.src=f+'?aid='+i;var b=h.getElementsByTagName("script")[0];b.parentNode.insertBefore(g,b);
	 })(window,document,"rtp","//sjrtp2-cdn.marketo.com/rtp-api/v1/rtp.js","elasticco");
	 
	rtp('send','view');
	rtp('get', 'campaign',true);
	</script>
	<!-- End of RTP tag -->
	
	
 	<link rel="apple-touch-icon" sizes="57x57"  href="https://www.elastic.co/apple-icon-57x57.png?change=123">
<link rel="apple-touch-icon" sizes="60x60"  href="https://www.elastic.co/apple-icon-60x60.png?change=123">
<link rel="apple-touch-icon" sizes="72x72"  href="https://www.elastic.co/apple-icon-72x72.png?change=123">
<link rel="apple-touch-icon" sizes="76x76"  href="https://www.elastic.co/apple-icon-76x76.png?change=123">
<link rel="apple-touch-icon" sizes="114x114"  href="https://www.elastic.co/apple-icon-114x114.png?change=123">
<link rel="apple-touch-icon" sizes="120x120"  href="https://www.elastic.co/apple-icon-120x120.png?change=123">
<link rel="apple-touch-icon" sizes="144x144"  href="https://www.elastic.co/apple-icon-144x144.png?change=123">
<link rel="apple-touch-icon" sizes="152x152"  href="https://www.elastic.co/apple-icon-152x152.png?change=123">
<link rel="apple-touch-icon" sizes="180x180"  href="https://www.elastic.co/apple-icon-180x180.png?change=123">
<link rel="icon" type="image/png" sizes="192x192"   href="https://www.elastic.co/android-icon-192x192.png?change=123">
<link rel="icon" type="image/png" sizes="32x32"  href="https://www.elastic.co/favicon-32x32.png?change=123">
<link rel="icon" type="image/png" sizes="96x96"  href="https://www.elastic.co/favicon-96x96.png?change=123">
<link rel="icon" type="image/png" sizes="16x16"  href="https://www.elastic.co/favicon-16x16.png?change=123">
<link rel="manifest"  href="https://www.elastic.co/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<!-- DC tags section -->
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/" >
<link rel="schema.DCTERMS" href="http://purl.org/dc/terms/" >
<meta name="DC.title" content="Guide template" >

<!-- end DC tags -->

	<meta name="description" content="" />

	<link rel="canonical" href="" />

	<meta property="og:image" content="https://www.elastic.co/static/img/elastic-logo-200.png" />
	


	
	<link type="text/css" rel="stylesheet" href="https://static-www.elastic.co/static/css/skel.css?q=100" />
	<link type="text/css" rel="stylesheet" href="https://static-www.elastic.co/static/css/style.css?q=100" />	
	<script src="https://static-www.elastic.co/static/js/skel.min.js?q=100"></script>
	
		<script type="text/javascript">form_name = "" ; cdn_url="https://static-www.elastic.co";cdn_slug = "100"</script>
	
	<script src="https://static-www.elastic.co/static/js/init.js?q=100"></script>
	<script src="https://static-www.elastic.co/static/js/jquery.min.js?q=100"></script>
	<script src="https://static-www.elastic.co/static/js/jquery.cookie.js?q=100"></script>
	<!--<script type="text/javascript">
	    (function() {
	        var path = '//easy.myfonts.net/v2/js?sid=10336(font-family=Avenir+35+Light)&sid=10338(font-family=Avenir+55+Roman)&sid=10340(font-family=Avenir+85+Heavy)&sid=10344(font-family=Avenir+65+Medium)&key=nAD6PCOPrX',
	            protocol = ('https:' == document.location.protocol ? 'https:' : 'http:'),
	            trial = document.createElement('script');
	        trial.type = 'text/javascript';
	        trial.async = true;
	        trial.src = protocol + path;
	        var head = document.getElementsByTagName("head")[0];
	        head.appendChild(trial);
	    })();
	</script>-->

	
	<link type="text/css" rel="stylesheet"  href="https://www.elastic.co/static/css/prettify.css" />
	<script type="text/javascript"  src="https://www.elastic.co/static/js/prettify.js"></script>



<link rel="stylesheet" type="text/css" href="styles.css" />

</head>
<body >
	<!-- Google Tag Manager -->
	<script>dataLayer = [];</script><noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-58RLH5"
													  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
			new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
			j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
			'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-58RLH5');</script>
	<!-- End Google Tag Manager -->
	
	<div class="header-cont">
		<div id="header-wrapper">
			


	<script  src="https://www.elastic.co/static/js/jquery.tokeninput.js?change=123"></script>
	<link rel="stylesheet" type="text/css"  href="https://www.elastic.co/static/css/token-input.css">

<div class="shortcuts-wrapper">
	<div class="container">
		<!-- Shortcuts -->
		<nav id="shortcuts">
			<ul class="links">
				
					<li class="shortcut-1">
						<a href="/downloads" id="header_downloads" >
							<span class="txt">downloads</span>	
						</a>
					</li>
				
					<li class="shortcut-2">
						<a href="/guide" id="header_guide" >
							<span class="txt">docs</span>	
						</a>
					</li>
				
					<li class="shortcut-3">
						<a href="/subscriptions" id="header_subscriptions" >
							<span class="txt">support</span>	
						</a>
					</li>
				
					<li class="shortcut-4">
						<a href="https://discuss.elastic.co" id="header_discuss" target="_blank">
							<span class="txt">discuss</span>	
						</a>
					</li>
				
					<li class="shortcut-5">
						<a href="/contact" id="header_contact" >
							<span class="txt">contact</span>	
						</a>
					</li>
				
			</ul>
		</nav>
		<div class="languages-wrapper">
			<div class="global-language">
				<ul class="language">
					<!-- <li class="active-language"><a class="active" href="#">EN</a></li> -->
					
						<li><a href="#">EN</a></li>
					
				</ul>
				<ul class="all-languages">
					
						
						<li><a href="/guide_template">English</a></li>
					
						
						<li><a href="/fr/guide_template">French</a></li>
					
						
						<li><a href="/de/guide_template">German</a></li>
					
						
						<li><a href="/jp/guide_template">Japanese</a></li>
					
						
						<li><a href="/kr/guide_template">Korean</a></li>
					
				</ul>
			</div>
		</div>
	</div>
</div>
<div class="container w100">
	<div class="row 0%">
		<div class="12u">
			

<section class="first-container desktop-main-nav">
	<div class="row 0%">
		<div class="11u 6u(small) 6u(xsmall)">
			<div id="elastic">
				<h1>
					<a href="/">
						<div id="elastic-logo">
						<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="0 0 400 130" preserveAspectRatio="xMinYMin meet" enable-background="new 0 0 400 130" xml:space="preserve">
    <g>
    	<g>
    		<path fill="#FFFFFF" d="M122.5,67.8c0-10.3-6.4-19.2-15.9-22.7c0.4-2.2,0.6-4.3,0.6-6.6c0-19.1-15.5-34.6-34.6-34.6
    			c-11.1,0-21.5,5.3-28,14.4c-3.2-2.5-7.1-3.8-11.2-3.8c-10.1,0-18.4,8.2-18.4,18.4c0,2.2,0.4,4.4,1.1,6.4C6.5,42.6,0,51.8,0,62
    			c0,10.3,6.4,19.3,16,22.8c-0.4,2.1-0.6,4.3-0.6,6.6c0,19,15.5,34.5,34.5,34.5c11.2,0,21.5-5.4,28-14.4c3.2,2.5,7.2,3.9,11.3,3.9
    			c10.1,0,18.4-8.2,18.4-18.4c0-2.2-0.4-4.4-1.1-6.4C115.9,87.2,122.5,78,122.5,67.8z"/>
    		<g>
    			<path fill="#F4BD19" d="M47.9,56.3l27.3,12.5l27.6-24.2c0.4-2,0.6-4,0.6-6.1c0-17-13.8-30.8-30.8-30.8c-10.2,0-19.7,5-25.4,13.4
    				l-4.6,23.8L47.9,56.3z"/>
    		</g>
    		<g>
    			<path fill="#3CBEB1" d="M19.6,85.2c-0.4,2-0.6,4.1-0.6,6.2c0,17,13.9,30.9,30.9,30.9c10.3,0,19.8-5.1,25.6-13.5L80,85l-6.1-11.6
    				L46.5,60.9L19.6,85.2z"/>
    		</g>
    		<g>
    			<path fill="#E9478C" d="M19.4,37.9l18.7,4.4L42.3,21c-2.6-2-5.7-3-9-3c-8.1,0-14.8,6.6-14.8,14.8C18.5,34.5,18.8,36.3,19.4,37.9z
    				"/>
    		</g>
    		<g>
    			<path fill="#2C458F" d="M17.8,42.4C9.4,45.1,3.6,53.2,3.6,62c0,8.6,5.3,16.3,13.3,19.3l26.3-23.8l-4.8-10.3L17.8,42.4z"/>
    		</g>
    		<g>
    			<path fill="#95C63D" d="M80.3,108.7c2.6,2,5.7,3.1,8.9,3.1c8.1,0,14.8-6.6,14.8-14.8c0-1.8-0.3-3.5-0.9-5.1l-18.7-4.4L80.3,108.7
    				z"/>
    		</g>
    		<g>
    			<path fill="#176655" d="M84.1,82.6l20.6,4.8c8.4-2.8,14.2-10.8,14.2-19.6c0-8.6-5.3-16.2-13.3-19.3l-27,23.6L84.1,82.6z"/>
    		</g>
    	</g>
    	<path d="M162.8,76.3c0,0.1,0,0.6,0.1,1.4c0.1,0.8,0.3,1.8,0.6,3c0.3,1.1,0.8,2.4,1.4,3.7c0.6,1.3,1.4,2.5,2.5,3.6
    		c1,1.1,2.3,2,3.8,2.7c1.5,0.7,3.4,1.1,5.6,1.1c1.8,0,3.3-0.1,4.6-0.4c1.3-0.3,2.4-0.7,3.4-1.3c1-0.6,1.9-1.3,2.7-2.2
    		c0.8-0.9,1.6-1.9,2.4-3.1c0.5-0.7,1-1.2,1.6-1.5c0.6-0.3,1.2-0.4,1.8-0.4c1,0,2,0.4,2.8,1.1c0.9,0.7,1.3,1.7,1.3,2.8
    		c0,0.7-0.4,1.7-1.1,3.1c-0.7,1.4-1.9,2.8-3.6,4.1c-1.6,1.4-3.7,2.6-6.3,3.6c-2.6,1-5.7,1.5-9.5,1.5c-3.8,0-7.2-0.7-10.2-2.1
    		c-3-1.4-5.5-3.2-7.5-5.6c-2-2.3-3.6-5-4.7-8.1c-1.1-3.1-1.6-6.3-1.6-9.7c0-3.5,0.5-6.8,1.6-9.9c1.1-3.1,2.7-5.8,4.7-8.1
    		c2-2.3,4.5-4.1,7.4-5.5c2.9-1.3,6.1-2,9.6-2c3.3,0,6.3,0.5,9.2,1.5c2.8,1,5.3,2.4,7.4,4.3c2.1,1.9,3.7,4.1,5,6.8
    		c1.2,2.7,1.8,5.8,1.8,9.2c0,2.1-0.5,3.7-1.6,4.8c-1,1.1-2.2,1.6-3.6,1.6H162.8z M170.9,56.4c-1.5,0.6-2.7,1.4-3.7,2.4
    		c-1,0.9-1.8,2-2.4,3.1c-0.6,1.1-1,2.2-1.3,3.2c-0.3,1-0.5,1.8-0.5,2.5c-0.1,0.7-0.1,1.1-0.1,1.2h27.9c0-1.8-0.3-3.5-0.9-5.1
    		c-0.6-1.6-1.5-3-2.6-4.3c-1.2-1.2-2.7-2.2-4.5-3c-1.8-0.7-4-1.1-6.5-1.1C174.2,55.5,172.4,55.8,170.9,56.4z"/>
    	<path d="M214.3,94.6c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V35.1
    		c0-1.3,0.4-2.3,1.3-3.2c0.9-0.9,2-1.3,3.2-1.3c1.3,0,2.3,0.4,3.2,1.3c0.9,0.9,1.3,1.9,1.3,3.2V94.6z"/>
    	<path d="M229.8,63.5c-0.6,0.3-1.2,0.5-1.9,0.5c-1.2,0-2.2-0.4-3.2-1.1c-1-0.7-1.5-1.8-1.5-3.1c0-0.5,0.1-1.1,0.4-1.9
    		c0.5-1.2,1.2-2.4,2.1-3.6c0.9-1.2,2-2.2,3.5-3.1s3.3-1.7,5.4-2.2c2.2-0.5,4.8-0.8,7.9-0.8c5.9,0,10.5,1.2,14,3.5
    		c3.5,2.3,5.2,5.8,5.2,10.3v32.8c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2v-2
    		c-1.2,2-4.7,4.5-7.5,5.3c-2.8,0.8-5.7,1.2-8.8,1.2c-2,0-3.9-0.3-5.9-0.8c-2-0.5-3.8-1.4-5.3-2.5c-1.6-1.1-2.9-2.6-3.8-4.3
    		c-1-1.7-1.5-3.8-1.5-6.2c0-2.8,0.5-5.7,1.6-7.5c1.1-1.9,2.6-3.4,4.4-4.6c1.8-1.2,3.9-2.1,6.3-2.8c2.4-0.6,4.9-1.1,7.5-1.4
    		c3-0.3,5.4-0.7,7.2-1.2c1.7-0.4,3-0.9,3.9-1.5c0.9-0.6,1.4-1.2,1.6-2c0.2-0.7,0.3-1.6,0.3-2.5c0-1.1-0.3-2-0.9-2.8
    		c-0.6-0.8-1.4-1.5-2.4-2c-1-0.5-2.1-0.9-3.3-1.2s-2.4-0.4-3.5-0.4c-2.1,0-3.7,0.2-5,0.5c-1.3,0.3-2.3,0.8-3.1,1.4
    		c-0.8,0.6-1.5,1.3-1.9,2.1c-0.5,0.8-0.9,1.6-1.3,2.5C230.9,62.6,230.4,63.1,229.8,63.5z M250.6,74.4c-1.1,0.3-2.4,0.6-3.8,0.8
    		c-1.5,0.2-3,0.4-4.6,0.6c-1.6,0.2-3.1,0.5-4.6,0.9c-1.5,0.3-2.8,0.8-4,1.5c-1.2,0.7-2.2,1.5-2.9,2.5c-0.7,1-1.1,3-1.1,4.6
    		c0,1.3,0.3,2.3,0.8,3.1c0.5,0.8,1.2,1.5,2.1,2c0.9,0.5,1.8,0.8,2.9,1c1.1,0.2,2.2,0.3,3.3,0.3c1.7,0,3.3-0.2,5-0.6
    		c1.7-0.4,3.2-1,4.5-1.9s2.4-2,3.3-3.5c0.8-1.4,1.2-3.8,1.2-5.9v-6.1L250.6,74.4z"/>
    	<path d="M307.6,90.9c-1.1,2-2.7,3.6-4.6,4.8s-4.1,2.1-6.6,2.7c-2.5,0.5-5.2,0.8-7.9,0.8c-3.9,0-7.2-0.6-9.9-1.8
    		c-2.7-1.2-4.9-2.6-6.5-4.1c-1.7-1.6-2.9-3.1-3.7-4.7c-0.7-1.6-1.1-2.7-1.1-3.5c0-1.3,0.5-2.4,1.4-3.2c0.9-0.8,2-1.2,3.1-1.2
    		c0.7,0,1.3,0.2,2,0.6c0.6,0.4,1.1,1,1.5,1.9c1,2.5,2.5,4.5,4.7,6.2c2.2,1.7,5.2,2.5,9.3,2.5c1.8,0,3.4-0.2,4.7-0.6
    		c1.3-0.4,2.4-1,3.2-1.7c0.8-0.7,1.4-1.5,1.9-2.4c0.4-0.9,0.6-1.8,0.6-2.8c0-2-0.8-3.4-2.3-4.4c-1.5-1-3.5-1.7-5.8-2.3
    		c-2.3-0.6-4.8-1.1-7.5-1.6c-2.7-0.5-5.2-1.2-7.5-2.2c-2.3-0.9-4.2-2.3-5.8-4c-1.6-1.8-2.3-4.2-2.3-7.3c0-4.5,1.6-8,4.9-10.5
    		c3.3-2.5,7.8-3.8,13.7-3.8c3.3,0,6,0.3,8.3,1c2.2,0.7,4.1,1.6,5.5,2.7c0.6,0.5,1.2,1,1.8,1.7c0.6,0.7,1.2,1.4,1.8,2.2
    		c0.6,0.8,1,1.5,1.4,2.3c0.4,0.7,0.6,1.4,0.6,2c0,1.3-0.5,2.4-1.6,3.1c-1,0.8-2.2,1.2-3.4,1.2c-0.7,0-1.5-0.1-2.1-0.6
    		c-0.6-0.6-1-1.3-1.3-1.8c-0.5-1.1-1.1-2-1.7-2.8c-0.5-0.8-1.2-1.5-2-2c-0.8-0.5-1.8-0.9-3-1.2c-1.2-0.3-2.7-0.4-4.4-0.4
    		c-3.5,0-6,0.6-7.6,1.7c-1.6,1.1-2.4,2.5-2.4,4.2c0,1.3,0.5,2.3,1.4,3.1c0.9,0.8,2.1,1.4,3.7,2c1.5,0.5,3.3,1,5.2,1.4
    		c2,0.4,3.9,0.8,5.9,1.3c2,0.5,4,1.1,5.9,1.7c1.9,0.7,3.7,1.6,5.2,2.7c1.5,1.1,2.7,2.5,3.7,4.1c0.9,1.6,1.4,3.6,1.4,6
    		C309.4,86.4,308.8,88.9,307.6,90.9z"/>
    	<path d="M327.4,48.8h4.7c1.1,0,2,0.4,2.8,1.2c0.8,0.8,1.2,1.7,1.2,2.8c0,1.1-0.4,2.1-1.2,2.8s-1.7,1.1-2.8,1.1h-4.7v37.9
    		c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-2,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V56.7h-4.6c-1.1,0-2-0.4-2.8-1.1
    		c-0.8-0.7-1.2-1.7-1.2-2.8c0-1.1,0.4-2,1.2-2.8c0.8-0.8,1.7-1.2,2.8-1.2h4.6V39c0-1.3,0.4-2.3,1.3-3.2s1.9-1.3,3.2-1.3
    		c1.3,0,2.3,0.4,3.2,1.3s1.3,1.9,1.3,3.2V48.8z"/>
    	<path d="M341.5,32.1c1-1,2.2-1.5,3.7-1.5c1.4,0,2.6,0.5,3.6,1.5c1,1,1.5,2.2,1.5,3.6c0,1.4-0.5,2.6-1.5,3.6c-1,1-2.2,1.5-3.6,1.5
    		c-1.4,0-2.7-0.5-3.7-1.5c-1-1-1.5-2.2-1.5-3.6C340,34.2,340.5,33,341.5,32.1z M349.6,94.6c0,1.3-0.4,2.3-1.3,3.2
    		c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V53.3c0-1.3,0.4-2.3,1.3-3.2c0.9-0.9,2-1.3,3.2-1.3
    		c1.3,0,2.3,0.4,3.2,1.3c0.9,0.9,1.3,2,1.3,3.2V94.6z"/>
    	<path d="M369.1,97.3c-2.9-1.2-5.4-3-7.5-5.2c-2.1-2.2-3.8-4.9-5-8.1c-1.2-3.1-1.8-6.6-1.8-10.4c0-3.8,0.6-7.3,1.8-10.5
    		c1.2-3.1,2.9-5.8,5-8.1c2.1-2.2,4.6-4,7.5-5.2c2.9-1.2,6-1.9,9.4-1.9c3.4,0,6.4,0.5,9,1.5c2.5,1,4.6,2.2,6.2,3.6
    		c1.6,1.4,3.2,2.9,4,4.4c0.5,0.9,0.9,1.5,0.9,2.6c0,1.3-0.5,2.3-1.4,3.1c-0.9,0.7-2,1.1-3.1,1.1c-0.8,0-1.6-0.2-2.2-0.6
    		c-0.7-0.4-1.2-1.3-1.8-2.1c-1-1.4-0.5-0.6-1.1-1.5c-0.7-0.9-1.5-1.6-2.5-2.3c-1-0.7-2.2-1.2-3.5-1.6c-1.4-0.4-2.8-0.6-4.4-0.6
    		c-1.8,0-3.5,0.3-5.3,1c-1.8,0.7-3.3,1.7-4.7,3.1c-1.4,1.4-2.5,3.3-3.4,5.6s-1.3,5.1-1.3,8.4c0,3.3,0.4,6.1,1.3,8.4s2,4.2,3.4,5.6
    		c1.4,1.4,3,2.5,4.7,3.1c1.8,0.7,3.5,1,5.3,1c1.6,0,3.1-0.2,4.4-0.6c1.3-0.4,2.5-0.9,3.5-1.6c1-0.7,1.8-1.5,2.5-2.3
    		c0.7-0.9,1.1-1.8,1.4-2.7c0.3-1,0.8-1.7,1.5-2.1c0.7-0.4,1.4-0.6,2.2-0.6c1.1,0,2.2,0.4,3.1,1.1c0.9,0.7,1.4,1.7,1.4,3
    		c0,0.6-0.3,1.7-0.9,3.1c-0.6,1.5-1.6,2.9-3.1,4.4c-1.5,1.5-3.5,2.8-6.2,4c-2.6,1.1-6,1.7-10,1.7C375.1,99.2,372,98.5,369.1,97.3z"
    		/>
    </g>
</svg>
						</div>
					</a>
				</h1>
			</div>
			<nav id="nav">
				<ul>
					
					<li>
						
							
						
						<a href="/products" id="nav_products"   >Products</a>
						
						<ul class="dropdown">
							
								<li><a href="/products/elasticsearch" id="nav_elasticsearch" >elasticsearch</a></li>								
							
								<li><a href="/found" id="nav_hosted-elasticsearch" >elasticsearch as a service</a></li>								
							
								<li><a href="/products/logstash" id="nav_logstash" >logstash</a></li>								
							
								<li><a href="/products/kibana" id="nav_kibana" >kibana</a></li>								
							
								<li><a href="/products/beats" id="nav_beats" >beats</a></li>								
							
								<li><a href="/products/watcher" id="nav_watcher" >watcher</a></li>								
							
								<li><a href="/products/shield" id="nav_shield" >shield</a></li>								
							
								<li><a href="/products/marvel" id="nav_marvel" >marvel</a></li>								
							
								<li><a href="/products/hadoop" id="nav_hadoop" >hadoop</a></li>								
							
								<li><a href="/downloads" id="nav_downloads" >downloads</a></li>								
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/subscriptions" id="nav_subscriptions"   >Subscriptions</a>
						
						<ul class="dropdown">
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/learn" id="nav_learn"   >Learn</a>
						
						<ul class="dropdown">
							
								<li><a href="/guide" id="nav_guide" >docs</a></li>								
							
								<li><a href="/videos" id="nav_videos" >videos</a></li>								
							
								<li><a href="/training" id="nav_training" >training</a></li>								
							
								<li><a href="/services" id="nav_services" >services</a></li>								
							
								<li><a href="/blog" id="nav_learn_blog" >blog</a></li>								
							
								<li><a href="/elasticon" id="nav_learn_elasticon" >elastic{ON}</a></li>								
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/community" id="nav_community"   >Community</a>
						
						<ul class="dropdown">
							
								<li><a href="/community/meetups" id="nav_meetups" >meetups</a></li>								
							
								<li><a href="https://discuss.elastic.co" id="nav_discuss" target="_blank">discuss</a></li>								
							
								<li><a href="/community/codeofconduct" id="nav_codeofconduct" >code of conduct</a></li>								
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/use-cases" id="nav_usecases"   >Use Cases</a>
						
						<ul class="dropdown">
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/blog" id="nav_blog"   >Blog</a>
						
						<ul class="dropdown">
							
								<li><a href="/blog/category/news" id="blog_news" >News</a></li>								
							
								<li><a href="/blog/category/engineering" id="blog_engineering" >Engineering</a></li>								
							
								<li><a href="/blog/category/user-stories" id="blog_user-stories" >User Stories</a></li>								
							
								<li><a href="/blog/category/releases" id="blog_releases" >Releases</a></li>								
							
								<li><a href="/blog/archive" id="blog_archive" >Archive</a></li>								
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/about" id="nav_about"   >About</a>
						
						<ul class="dropdown">
							
								<li><a href="/about/leadership" id="nav_leadership" >leadership</a></li>								
							
								<li><a href="/about/board" id="nav_board" >board of directors</a></li>								
							
								<li><a href="/about/careers" id="nav_careers" >careers</a></li>								
							
								<li><a href="/about/partners" id="nav_partners" >partners</a></li>								
							
								<li><a href="/about/press" id="nav_press" >press</a></li>								
								
						</ul>							
					</li>
					
				</ul>
			</nav>			
			<!-- Searchbar Input box -->
			 <!-- <form method="get" action="/search" name="searchForm" class="header-search-form" id="searchMobileFrm">
			   <input type="text" id="search-header-autocomplete" name="q" class="form-control global-input" placeholder="elastic{search}">
			</form>
			<div class="mobile-auto-complete"></div> -->  
		</div>
		<div class="1u 6u(small) 6u(xsmall)">
			<div id="searchbar">
				<a href="#" class="button icon"></a>
			</div>
			<!-- <div class="nav-auto-complete"></div> -->
		</div>
	</div>
</section>
<div class="mobile-menu-wrapper">
	<div class="row 0%">
		<div class="8u 6u(small) 6u(xsmall)">
			<div id="elastic">
				<h1>
					<a href="/">
						<div id="elastic-logo">
						<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="0 0 400 130" preserveAspectRatio="xMinYMin meet" enable-background="new 0 0 400 130" xml:space="preserve">
    <g>
    	<g>
    		<path fill="#FFFFFF" d="M122.5,67.8c0-10.3-6.4-19.2-15.9-22.7c0.4-2.2,0.6-4.3,0.6-6.6c0-19.1-15.5-34.6-34.6-34.6
    			c-11.1,0-21.5,5.3-28,14.4c-3.2-2.5-7.1-3.8-11.2-3.8c-10.1,0-18.4,8.2-18.4,18.4c0,2.2,0.4,4.4,1.1,6.4C6.5,42.6,0,51.8,0,62
    			c0,10.3,6.4,19.3,16,22.8c-0.4,2.1-0.6,4.3-0.6,6.6c0,19,15.5,34.5,34.5,34.5c11.2,0,21.5-5.4,28-14.4c3.2,2.5,7.2,3.9,11.3,3.9
    			c10.1,0,18.4-8.2,18.4-18.4c0-2.2-0.4-4.4-1.1-6.4C115.9,87.2,122.5,78,122.5,67.8z"/>
    		<g>
    			<path fill="#F4BD19" d="M47.9,56.3l27.3,12.5l27.6-24.2c0.4-2,0.6-4,0.6-6.1c0-17-13.8-30.8-30.8-30.8c-10.2,0-19.7,5-25.4,13.4
    				l-4.6,23.8L47.9,56.3z"/>
    		</g>
    		<g>
    			<path fill="#3CBEB1" d="M19.6,85.2c-0.4,2-0.6,4.1-0.6,6.2c0,17,13.9,30.9,30.9,30.9c10.3,0,19.8-5.1,25.6-13.5L80,85l-6.1-11.6
    				L46.5,60.9L19.6,85.2z"/>
    		</g>
    		<g>
    			<path fill="#E9478C" d="M19.4,37.9l18.7,4.4L42.3,21c-2.6-2-5.7-3-9-3c-8.1,0-14.8,6.6-14.8,14.8C18.5,34.5,18.8,36.3,19.4,37.9z
    				"/>
    		</g>
    		<g>
    			<path fill="#2C458F" d="M17.8,42.4C9.4,45.1,3.6,53.2,3.6,62c0,8.6,5.3,16.3,13.3,19.3l26.3-23.8l-4.8-10.3L17.8,42.4z"/>
    		</g>
    		<g>
    			<path fill="#95C63D" d="M80.3,108.7c2.6,2,5.7,3.1,8.9,3.1c8.1,0,14.8-6.6,14.8-14.8c0-1.8-0.3-3.5-0.9-5.1l-18.7-4.4L80.3,108.7
    				z"/>
    		</g>
    		<g>
    			<path fill="#176655" d="M84.1,82.6l20.6,4.8c8.4-2.8,14.2-10.8,14.2-19.6c0-8.6-5.3-16.2-13.3-19.3l-27,23.6L84.1,82.6z"/>
    		</g>
    	</g>
    	<path d="M162.8,76.3c0,0.1,0,0.6,0.1,1.4c0.1,0.8,0.3,1.8,0.6,3c0.3,1.1,0.8,2.4,1.4,3.7c0.6,1.3,1.4,2.5,2.5,3.6
    		c1,1.1,2.3,2,3.8,2.7c1.5,0.7,3.4,1.1,5.6,1.1c1.8,0,3.3-0.1,4.6-0.4c1.3-0.3,2.4-0.7,3.4-1.3c1-0.6,1.9-1.3,2.7-2.2
    		c0.8-0.9,1.6-1.9,2.4-3.1c0.5-0.7,1-1.2,1.6-1.5c0.6-0.3,1.2-0.4,1.8-0.4c1,0,2,0.4,2.8,1.1c0.9,0.7,1.3,1.7,1.3,2.8
    		c0,0.7-0.4,1.7-1.1,3.1c-0.7,1.4-1.9,2.8-3.6,4.1c-1.6,1.4-3.7,2.6-6.3,3.6c-2.6,1-5.7,1.5-9.5,1.5c-3.8,0-7.2-0.7-10.2-2.1
    		c-3-1.4-5.5-3.2-7.5-5.6c-2-2.3-3.6-5-4.7-8.1c-1.1-3.1-1.6-6.3-1.6-9.7c0-3.5,0.5-6.8,1.6-9.9c1.1-3.1,2.7-5.8,4.7-8.1
    		c2-2.3,4.5-4.1,7.4-5.5c2.9-1.3,6.1-2,9.6-2c3.3,0,6.3,0.5,9.2,1.5c2.8,1,5.3,2.4,7.4,4.3c2.1,1.9,3.7,4.1,5,6.8
    		c1.2,2.7,1.8,5.8,1.8,9.2c0,2.1-0.5,3.7-1.6,4.8c-1,1.1-2.2,1.6-3.6,1.6H162.8z M170.9,56.4c-1.5,0.6-2.7,1.4-3.7,2.4
    		c-1,0.9-1.8,2-2.4,3.1c-0.6,1.1-1,2.2-1.3,3.2c-0.3,1-0.5,1.8-0.5,2.5c-0.1,0.7-0.1,1.1-0.1,1.2h27.9c0-1.8-0.3-3.5-0.9-5.1
    		c-0.6-1.6-1.5-3-2.6-4.3c-1.2-1.2-2.7-2.2-4.5-3c-1.8-0.7-4-1.1-6.5-1.1C174.2,55.5,172.4,55.8,170.9,56.4z"/>
    	<path d="M214.3,94.6c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V35.1
    		c0-1.3,0.4-2.3,1.3-3.2c0.9-0.9,2-1.3,3.2-1.3c1.3,0,2.3,0.4,3.2,1.3c0.9,0.9,1.3,1.9,1.3,3.2V94.6z"/>
    	<path d="M229.8,63.5c-0.6,0.3-1.2,0.5-1.9,0.5c-1.2,0-2.2-0.4-3.2-1.1c-1-0.7-1.5-1.8-1.5-3.1c0-0.5,0.1-1.1,0.4-1.9
    		c0.5-1.2,1.2-2.4,2.1-3.6c0.9-1.2,2-2.2,3.5-3.1s3.3-1.7,5.4-2.2c2.2-0.5,4.8-0.8,7.9-0.8c5.9,0,10.5,1.2,14,3.5
    		c3.5,2.3,5.2,5.8,5.2,10.3v32.8c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2v-2
    		c-1.2,2-4.7,4.5-7.5,5.3c-2.8,0.8-5.7,1.2-8.8,1.2c-2,0-3.9-0.3-5.9-0.8c-2-0.5-3.8-1.4-5.3-2.5c-1.6-1.1-2.9-2.6-3.8-4.3
    		c-1-1.7-1.5-3.8-1.5-6.2c0-2.8,0.5-5.7,1.6-7.5c1.1-1.9,2.6-3.4,4.4-4.6c1.8-1.2,3.9-2.1,6.3-2.8c2.4-0.6,4.9-1.1,7.5-1.4
    		c3-0.3,5.4-0.7,7.2-1.2c1.7-0.4,3-0.9,3.9-1.5c0.9-0.6,1.4-1.2,1.6-2c0.2-0.7,0.3-1.6,0.3-2.5c0-1.1-0.3-2-0.9-2.8
    		c-0.6-0.8-1.4-1.5-2.4-2c-1-0.5-2.1-0.9-3.3-1.2s-2.4-0.4-3.5-0.4c-2.1,0-3.7,0.2-5,0.5c-1.3,0.3-2.3,0.8-3.1,1.4
    		c-0.8,0.6-1.5,1.3-1.9,2.1c-0.5,0.8-0.9,1.6-1.3,2.5C230.9,62.6,230.4,63.1,229.8,63.5z M250.6,74.4c-1.1,0.3-2.4,0.6-3.8,0.8
    		c-1.5,0.2-3,0.4-4.6,0.6c-1.6,0.2-3.1,0.5-4.6,0.9c-1.5,0.3-2.8,0.8-4,1.5c-1.2,0.7-2.2,1.5-2.9,2.5c-0.7,1-1.1,3-1.1,4.6
    		c0,1.3,0.3,2.3,0.8,3.1c0.5,0.8,1.2,1.5,2.1,2c0.9,0.5,1.8,0.8,2.9,1c1.1,0.2,2.2,0.3,3.3,0.3c1.7,0,3.3-0.2,5-0.6
    		c1.7-0.4,3.2-1,4.5-1.9s2.4-2,3.3-3.5c0.8-1.4,1.2-3.8,1.2-5.9v-6.1L250.6,74.4z"/>
    	<path d="M307.6,90.9c-1.1,2-2.7,3.6-4.6,4.8s-4.1,2.1-6.6,2.7c-2.5,0.5-5.2,0.8-7.9,0.8c-3.9,0-7.2-0.6-9.9-1.8
    		c-2.7-1.2-4.9-2.6-6.5-4.1c-1.7-1.6-2.9-3.1-3.7-4.7c-0.7-1.6-1.1-2.7-1.1-3.5c0-1.3,0.5-2.4,1.4-3.2c0.9-0.8,2-1.2,3.1-1.2
    		c0.7,0,1.3,0.2,2,0.6c0.6,0.4,1.1,1,1.5,1.9c1,2.5,2.5,4.5,4.7,6.2c2.2,1.7,5.2,2.5,9.3,2.5c1.8,0,3.4-0.2,4.7-0.6
    		c1.3-0.4,2.4-1,3.2-1.7c0.8-0.7,1.4-1.5,1.9-2.4c0.4-0.9,0.6-1.8,0.6-2.8c0-2-0.8-3.4-2.3-4.4c-1.5-1-3.5-1.7-5.8-2.3
    		c-2.3-0.6-4.8-1.1-7.5-1.6c-2.7-0.5-5.2-1.2-7.5-2.2c-2.3-0.9-4.2-2.3-5.8-4c-1.6-1.8-2.3-4.2-2.3-7.3c0-4.5,1.6-8,4.9-10.5
    		c3.3-2.5,7.8-3.8,13.7-3.8c3.3,0,6,0.3,8.3,1c2.2,0.7,4.1,1.6,5.5,2.7c0.6,0.5,1.2,1,1.8,1.7c0.6,0.7,1.2,1.4,1.8,2.2
    		c0.6,0.8,1,1.5,1.4,2.3c0.4,0.7,0.6,1.4,0.6,2c0,1.3-0.5,2.4-1.6,3.1c-1,0.8-2.2,1.2-3.4,1.2c-0.7,0-1.5-0.1-2.1-0.6
    		c-0.6-0.6-1-1.3-1.3-1.8c-0.5-1.1-1.1-2-1.7-2.8c-0.5-0.8-1.2-1.5-2-2c-0.8-0.5-1.8-0.9-3-1.2c-1.2-0.3-2.7-0.4-4.4-0.4
    		c-3.5,0-6,0.6-7.6,1.7c-1.6,1.1-2.4,2.5-2.4,4.2c0,1.3,0.5,2.3,1.4,3.1c0.9,0.8,2.1,1.4,3.7,2c1.5,0.5,3.3,1,5.2,1.4
    		c2,0.4,3.9,0.8,5.9,1.3c2,0.5,4,1.1,5.9,1.7c1.9,0.7,3.7,1.6,5.2,2.7c1.5,1.1,2.7,2.5,3.7,4.1c0.9,1.6,1.4,3.6,1.4,6
    		C309.4,86.4,308.8,88.9,307.6,90.9z"/>
    	<path d="M327.4,48.8h4.7c1.1,0,2,0.4,2.8,1.2c0.8,0.8,1.2,1.7,1.2,2.8c0,1.1-0.4,2.1-1.2,2.8s-1.7,1.1-2.8,1.1h-4.7v37.9
    		c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-2,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V56.7h-4.6c-1.1,0-2-0.4-2.8-1.1
    		c-0.8-0.7-1.2-1.7-1.2-2.8c0-1.1,0.4-2,1.2-2.8c0.8-0.8,1.7-1.2,2.8-1.2h4.6V39c0-1.3,0.4-2.3,1.3-3.2s1.9-1.3,3.2-1.3
    		c1.3,0,2.3,0.4,3.2,1.3s1.3,1.9,1.3,3.2V48.8z"/>
    	<path d="M341.5,32.1c1-1,2.2-1.5,3.7-1.5c1.4,0,2.6,0.5,3.6,1.5c1,1,1.5,2.2,1.5,3.6c0,1.4-0.5,2.6-1.5,3.6c-1,1-2.2,1.5-3.6,1.5
    		c-1.4,0-2.7-0.5-3.7-1.5c-1-1-1.5-2.2-1.5-3.6C340,34.2,340.5,33,341.5,32.1z M349.6,94.6c0,1.3-0.4,2.3-1.3,3.2
    		c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V53.3c0-1.3,0.4-2.3,1.3-3.2c0.9-0.9,2-1.3,3.2-1.3
    		c1.3,0,2.3,0.4,3.2,1.3c0.9,0.9,1.3,2,1.3,3.2V94.6z"/>
    	<path d="M369.1,97.3c-2.9-1.2-5.4-3-7.5-5.2c-2.1-2.2-3.8-4.9-5-8.1c-1.2-3.1-1.8-6.6-1.8-10.4c0-3.8,0.6-7.3,1.8-10.5
    		c1.2-3.1,2.9-5.8,5-8.1c2.1-2.2,4.6-4,7.5-5.2c2.9-1.2,6-1.9,9.4-1.9c3.4,0,6.4,0.5,9,1.5c2.5,1,4.6,2.2,6.2,3.6
    		c1.6,1.4,3.2,2.9,4,4.4c0.5,0.9,0.9,1.5,0.9,2.6c0,1.3-0.5,2.3-1.4,3.1c-0.9,0.7-2,1.1-3.1,1.1c-0.8,0-1.6-0.2-2.2-0.6
    		c-0.7-0.4-1.2-1.3-1.8-2.1c-1-1.4-0.5-0.6-1.1-1.5c-0.7-0.9-1.5-1.6-2.5-2.3c-1-0.7-2.2-1.2-3.5-1.6c-1.4-0.4-2.8-0.6-4.4-0.6
    		c-1.8,0-3.5,0.3-5.3,1c-1.8,0.7-3.3,1.7-4.7,3.1c-1.4,1.4-2.5,3.3-3.4,5.6s-1.3,5.1-1.3,8.4c0,3.3,0.4,6.1,1.3,8.4s2,4.2,3.4,5.6
    		c1.4,1.4,3,2.5,4.7,3.1c1.8,0.7,3.5,1,5.3,1c1.6,0,3.1-0.2,4.4-0.6c1.3-0.4,2.5-0.9,3.5-1.6c1-0.7,1.8-1.5,2.5-2.3
    		c0.7-0.9,1.1-1.8,1.4-2.7c0.3-1,0.8-1.7,1.5-2.1c0.7-0.4,1.4-0.6,2.2-0.6c1.1,0,2.2,0.4,3.1,1.1c0.9,0.7,1.4,1.7,1.4,3
    		c0,0.6-0.3,1.7-0.9,3.1c-0.6,1.5-1.6,2.9-3.1,4.4c-1.5,1.5-3.5,2.8-6.2,4c-2.6,1.1-6,1.7-10,1.7C375.1,99.2,372,98.5,369.1,97.3z"
    		/>
    </g>
</svg>
						</div>
					</a>
				</h1>
			</div>
		</div>
		<div class="4u 6u(small) 6u(xsmall) text-right">
			<!-- <div class="mobile-menu-wrapper"> -->
				<ul class="m-shortcuts">
					
					<li><a href="#" class="m-search" id="header-search"></a></li>
					
					<li><a href="/guide" class="m-docs" id="mobile-docs"></a></li>
					
					<li><a href="/contact" class="m-contact" id="mobile-contact"></a></li>
					
				</ul>	
				<nav class="nav-menu">
					<div class="menu-button">
						<span></span>
					</div>
					<div class="nav-menu-list-wrap">
						<div class="nav-menu-list">
							
								<h3><a href="/products" id="nav_products" >Products</a></h3>

								
								<ul>
									
									<li><a href="/products/elasticsearch" id="nav_elasticsearch" >elasticsearch</a></li>
									
									<li><a href="/found" id="nav_hosted-elasticsearch" >elasticsearch as a service</a></li>
									
									<li><a href="/products/logstash" id="nav_logstash" >logstash</a></li>
									
									<li><a href="/products/kibana" id="nav_kibana" >kibana</a></li>
									
									<li><a href="/products/beats" id="nav_beats" >beats</a></li>
									
									<li><a href="/products/watcher" id="nav_watcher" >watcher</a></li>
									
									<li><a href="/products/shield" id="nav_shield" >shield</a></li>
									
									<li><a href="/products/marvel" id="nav_marvel" >marvel</a></li>
									
									<li><a href="/products/hadoop" id="nav_hadoop" >hadoop</a></li>
									
									<li><a href="/downloads" id="nav_downloads" >downloads</a></li>
									
								</ul>
								
							
								<h3><a href="/subscriptions" id="nav_subscriptions" >Subscriptions</a></h3>

								
							
								<h3><a href="/learn" id="nav_learn" >Learn</a></h3>

								
								<ul>
									
									<li><a href="/guide" id="nav_guide" >docs</a></li>
									
									<li><a href="/videos" id="nav_videos" >videos</a></li>
									
									<li><a href="/training" id="nav_training" >training</a></li>
									
									<li><a href="/services" id="nav_services" >services</a></li>
									
									<li><a href="/blog" id="nav_learn_blog" >blog</a></li>
									
									<li><a href="/elasticon" id="nav_learn_elasticon" >elastic{ON}</a></li>
									
								</ul>
								
							
								<h3><a href="/community" id="nav_community" >Community</a></h3>

								
								<ul>
									
									<li><a href="/community/meetups" id="nav_meetups" >meetups</a></li>
									
									<li><a href="/https:/discuss.elastic.co" id="nav_discuss" target="_blank">discuss</a></li>
									
									<li><a href="/community/codeofconduct" id="nav_codeofconduct" >code of conduct</a></li>
									
								</ul>
								
							
								<h3><a href="/use-cases" id="nav_usecases" >Use Cases</a></h3>

								
							
								<h3><a href="/blog" id="nav_blog" >Blog</a></h3>

								
								<ul>
									
									<li><a href="/blog/category/news" id="blog_news" >News</a></li>
									
									<li><a href="/blog/category/engineering" id="blog_engineering" >Engineering</a></li>
									
									<li><a href="/blog/category/user-stories" id="blog_user-stories" >User Stories</a></li>
									
									<li><a href="/blog/category/releases" id="blog_releases" >Releases</a></li>
									
									<li><a href="/blog/archive" id="blog_archive" >Archive</a></li>
									
								</ul>
								
							
								<h3><a href="/about" id="nav_about" >About</a></h3>

								
								<ul>
									
									<li><a href="/about/leadership" id="nav_leadership" >leadership</a></li>
									
									<li><a href="/about/board" id="nav_board" >board of directors</a></li>
									
									<li><a href="/about/careers" id="nav_careers" >careers</a></li>
									
									<li><a href="/about/partners" id="nav_partners" >partners</a></li>
									
									<li><a href="/about/press" id="nav_press" >press</a></li>
									
								</ul>
								
							
						</div>
					</div>
				</nav>
			<!-- </div> -->
		</div>
	</div>
</div>
<!-- Mobile Menu -->


		</div>
	</div>
</div>
<div class="header-search-wrapper">	
	<div class="container">
		<div class="big-search">
			<i class="big-search-icon"></i>
			<form method="get" action="/search" autocomplete="on" name="searchForm" id="searchfrm">
				<ul class="tags-wrapper">
					<li class="search-field"><input type="text" class="form-control global-input" id="autocomplete" name="q"></li>
				</ul>
			</form>
			<a href="#" class="header-search-cancel"></a>
		</div>
		<div class="nav-auto-complete"></div>
	</div>
</div>
<style>	
	
	
		.m-shortcuts li a.m-search {
			background: url("https://static-www.elastic.co/assets/blt174b13be89ff803c/m-search-icon.png?q=100") no-repeat scroll center center rgba(0, 0, 0, 0);
			background-size: contain;}
	
		.m-shortcuts li a.m-docs {
			background: url("https://static-www.elastic.co/assets/blt5ba03f594a7d610a/m-guide-icon.png?q=100") no-repeat scroll center center rgba(0, 0, 0, 0);
			background-size: contain;}
	
		.m-shortcuts li a.m-contact {
			background: url("https://static-www.elastic.co/assets/blt38c3e853c4b1174d/m-contact-icon.png?q=100") no-repeat scroll center center rgba(0, 0, 0, 0);
			background-size: contain;}
		
</style>
		</div>
	</div>
  	<div id="content">
  		
  			
            <div id="pageheader">
    <div class="container">
        <header>
            <h1><a href="/learn" title="Learn">Learn</a> |</h1>
            <h2><a href="/guide" title="Docs">Docs</a></h2>
        </header>
    </div>
</div>
<section id="guide">

            <!-- start body -->
<div class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elasticsearch: The Definitive Guide</a></span> » <span class="breadcrumb-link"><a href="administration.html">관리, 모니터링, 배포 </a></span> » <span class="breadcrumb-node">제품의 배포</span></div><div class="navheader"><span class="prev"><a href="cluster-admin.html">
              « 
              모니터링</a>
           
        </span><span class="next">
           
          <a href="post_deploy.html">배포 후
               »
            </a></span></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="deploy"></a>제품의 배포</h2></div></div></div><p>If you have made it this far in the book, hopefully you’ve learned a thing or
two about Elasticsearch and are ready to<a id="id-1.10.4.2.1" class="indexterm"></a> deploy your cluster to production.<a id="id-1.10.4.2.2" class="indexterm"></a>
<a id="id-1.10.4.2.3" class="indexterm"></a>
This chapter is not meant to be an exhaustive guide to running your cluster
in production, but it covers the key things to consider before putting
your cluster live.</p><p>Three main areas are covered:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Logistical considerations, such as hardware recommendations and deployment
strategies
</li><li class="listitem">
Configuration changes that are more suited to a production environment
</li><li class="listitem">
Post-deployment considerations, such as security, maximizing indexing performance,
and backups
</li></ul></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="hardware"></a>Hardware</h2></div></div></div><p>If you’ve been following the normal development path, you’ve probably been playing<a id="id-1.10.4.5.2.1" class="indexterm"></a>
<a id="id-1.10.4.5.2.2" class="indexterm"></a><a id="id-1.10.4.5.2.3" class="indexterm"></a>
with Elasticsearch on your laptop or on a small cluster of machines laying around.
But when it comes time to deploy Elasticsearch to production, there are a few
recommendations that you should consider.  Nothing is a hard-and-fast rule;
Elasticsearch is used for a wide range of tasks and on a bewildering array of
machines.  But these recommendations provide good starting points based on our experience with
production clusters.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_memory"></a>Memory</h3></div></div></div><p>If there is one resource that you will run out of first, it will likely be memory.<a id="id-1.10.4.5.3.2.1" class="indexterm"></a>
<a id="id-1.10.4.5.3.2.2" class="indexterm"></a><a id="id-1.10.4.5.3.2.3" class="indexterm"></a>
Sorting and aggregations can both be memory hungry, so enough heap space to
accommodate these is important.<a id="id-1.10.4.5.3.2.4" class="indexterm"></a>  Even when the heap is comparatively small,
extra memory can be given to the OS filesystem cache.  Because many data structures
used by Lucene are disk-based formats, Elasticsearch leverages the OS cache to
great effect.</p><p>A machine with 64 GB of RAM is the ideal sweet spot, but 32 GB and 16 GB machines
are also common.  Less than 8 GB tends to be counterproductive (you end up
needing many, many small machines), and greater than 64 GB has problems that we will
discuss in <a class="xref" href="deploy.html#heap-sizing" title="Heap: Sizing and Swapping">Heap: Sizing and Swapping</a>.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_cpus"></a>CPUs</h3></div></div></div><p>Most Elasticsearch deployments tend to be rather light on CPU requirements.  As
such,<a id="id-1.10.4.5.4.2.1" class="indexterm"></a><a id="id-1.10.4.5.4.2.2" class="indexterm"></a>
<a id="id-1.10.4.5.4.2.3" class="indexterm"></a> the exact processor setup matters less than the other resources.  You should
choose a modern processor with multiple cores.  Common clusters utilize two to eight
core machines.</p><p>If you need to choose between faster CPUs or more cores, choose more cores.  The
extra concurrency that multiple cores offers will far outweigh a slightly faster
clock speed.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_disks"></a>Disks</h3></div></div></div><p>Disks are important for all clusters,<a id="id-1.10.4.5.5.2.1" class="indexterm"></a><a id="id-1.10.4.5.5.2.2" class="indexterm"></a>
<a id="id-1.10.4.5.5.2.3" class="indexterm"></a> and doubly so for indexing-heavy clusters
(such as those that ingest log data).  Disks are the slowest subsystem in a server,
which means that write-heavy clusters can easily saturate their disks, which in
turn become the bottleneck of the cluster.</p><p>If you can afford SSDs, they are by far superior to any spinning media.  SSD-backed
nodes see boosts in both query and indexing performance.  If you can afford it,
SSDs are the way to go.</p><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong>Check Your I/O Scheduler</strong></p></div></div></div><p>If you are using SSDs, make sure your OS I/O scheduler is<a id="id-1.10.4.5.5.4.2.1" class="indexterm"></a> configured correctly.
When you write data to disk, the I/O scheduler decides when that data is
<span class="emphasis"><em>actually</em></span> sent to the disk.  The default under most *nix distributions is a
scheduler called <code class="literal">cfq</code> (Completely Fair Queuing).</p><p>This scheduler allocates <span class="emphasis"><em>time slices</em></span> to each process, and then optimizes the
delivery of these various queues to the disk.  It is optimized for spinning media:
the nature of rotating platters means it is more efficient to write data to disk
based on physical layout.</p><p>This is inefficient for SSD, however, since there are no spinning platters
involved.  Instead, <code class="literal">deadline</code> or <code class="literal">noop</code> should be used instead.  The deadline
scheduler optimizes based on how long writes have been pending, while <code class="literal">noop</code>
is just a simple FIFO queue.</p><p>This simple change can have dramatic impacts.  We’ve seen a 500-fold improvement
to write throughput just by using the correct scheduler.</p></div><p>If you use spinning media, try to obtain the fastest disks possible (high-performance server disks, 15k RPM drives).</p><p>Using RAID 0 is an effective way to increase disk speed, for both spinning disks
and SSD.  There is no need to use mirroring or parity variants of RAID, since
high availability is built into Elasticsearch via replicas.</p><p>Finally, avoid network-attached storage (NAS).  People routinely claim their
NAS solution is faster and more reliable than local drives.  Despite these claims,
we have never seen NAS live up to its hype.  NAS is often slower, displays
larger latencies with a wider deviation in average latency, and is a single
point of failure.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_network"></a>Network</h3></div></div></div><p>A fast and reliable network is obviously important to performance in a distributed<a id="id-1.10.4.5.6.2.1" class="indexterm"></a>
<a id="id-1.10.4.5.6.2.2" class="indexterm"></a><a id="id-1.10.4.5.6.2.3" class="indexterm"></a>
system.  Low latency helps ensure that nodes can communicate easily, while
high bandwidth helps shard movement and recovery.  Modern data-center networking
(1 GbE, 10 GbE) is sufficient for the vast majority of clusters.</p><p>Avoid clusters that span multiple data centers, even if the data centers are
colocated in close proximity.  Definitely avoid clusters that span large geographic
distances.</p><p>Elasticsearch clusters assume that all nodes are equal—not that half the nodes
are actually 150ms distant in another data center. Larger latencies tend to
exacerbate problems in distributed systems and make debugging and resolution
more difficult.</p><p>Similar to the NAS argument, everyone claims that their pipe between data centers is
robust and low latency. This is true—until it isn’t (a network failure will
happen eventually; you can count on it). From our experience, the hassle of
managing cross–data center clusters is simply not worth the cost.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_general_considerations"></a>General Considerations</h3></div></div></div><p>It is possible nowadays to obtain truly enormous machines:<a id="id-1.10.4.5.7.2.1" class="indexterm"></a>
<a id="id-1.10.4.5.7.2.2" class="indexterm"></a>  hundreds of gigabytes
of RAM with dozens of CPU cores.  Conversely, it is also possible to spin up
thousands of small virtual machines in cloud platforms such as EC2.  Which
approach is best?</p><p>In general, it is better to prefer medium-to-large boxes.  Avoid small machines,
because you don’t want to manage a cluster with a thousand nodes, and the overhead
of simply running Elasticsearch is more apparent on such small boxes.</p><p>At the same time, avoid the truly enormous machines.  They often lead to imbalanced
resource usage (for example, all the memory is being used, but none of the CPU) and can
add logistical complexity if you have to run multiple nodes per machine.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="_java_virtual_machine"></a>Java Virtual Machine</h2></div></div></div><p>You should always run the most recent version of the Java Virtual Machine (JVM),
unless otherwise stated on the Elasticsearch website.<a id="id-1.10.4.6.2.1" class="indexterm"></a>
<a id="id-1.10.4.6.2.2" class="indexterm"></a><a id="id-1.10.4.6.2.3" class="indexterm"></a><a id="id-1.10.4.6.2.4" class="indexterm"></a>  Elasticsearch, and in
particular Lucene, is a demanding piece of software.  The unit and integration
tests from Lucene often expose bugs in the JVM itself.  These bugs range from
mild annoyances to serious segfaults, so it is best to use the latest version
of the JVM where possible.</p><p>Java 7 is strongly preferred over Java 6.  Either Oracle or OpenJDK are acceptable. They are comparable in performance and stability.</p><p>If your application is written in Java and you are using the transport client
or node client, make sure the JVM running your application is identical to the
server JVM.  In few locations in Elasticsearch, Java’s native serialization
is used (IP addresses, exceptions, and so forth).  Unfortunately, Oracle has been known to
change the serialization format between minor releases, leading to strange errors.
This happens rarely, but it is best practice to keep the JVM versions identical
between client and server.</p><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong>Please Do Not Tweak JVM Settings</strong></p></div></div></div><p>The JVM exposes dozens (hundreds even!) of settings, parameters, and configurations.<a id="id-1.10.4.6.5.2.1" class="indexterm"></a>
<a id="id-1.10.4.6.5.2.2" class="indexterm"></a>
They allow you to tweak and tune almost every aspect of the JVM.</p><p>When a knob is encountered, it is human nature to want to turn it.  We implore
you to squash this desire and <span class="emphasis"><em>not</em></span> use custom JVM settings.  Elasticsearch is
a complex piece of software, and the current JVM settings have been tuned
over years of real-world usage.</p><p>It is easy to start turning knobs, producing opaque effects that are hard to measure,
and eventually detune your cluster into a slow, unstable mess.  When debugging
clusters, the first step is often to remove all custom configurations.  About
half the time, this alone restores stability and performance.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="_transport_client_versus_node_client"></a>Transport Client Versus Node Client</h2></div></div></div><p>If you are using Java, you may wonder when to use the transport client versus the
node client.<a id="id-1.10.4.7.2.1" class="indexterm"></a>
<a id="id-1.10.4.7.2.2" class="indexterm"></a><a id="id-1.10.4.7.2.3" class="indexterm"></a><a id="id-1.10.4.7.2.4" class="indexterm"></a>
<a id="id-1.10.4.7.2.5" class="indexterm"></a><a id="id-1.10.4.7.2.6" class="indexterm"></a>
<a id="id-1.10.4.7.2.7" class="indexterm"></a>  As discussed at the beginning of the book, the transport client
acts as a communication layer between the cluster and your application.  It knows
the API and can automatically round-robin between nodes, sniff the cluster for you,
and more. But it is <span class="emphasis"><em>external</em></span> to the cluster, similar to the REST clients.</p><p>The node client, on the other hand, is actually a node within the cluster (but
does not hold data, and cannot become master).  Because it is a node, it knows
the entire cluster state (where all the nodes reside, which shards live in which
nodes, and so forth). This means it can execute APIs with one less network hop.</p><p>There are uses-cases for both clients:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
The transport client is ideal if you want to decouple your application from the
cluster.  For example, if your application quickly creates and destroys
connections to the cluster, a transport client is much "lighter" than a node client,
since it is not part of a cluster.
</p><p class="simpara">Similarly, if you need to create thousands of connections, you don’t want to
have thousands of node clients join the cluster.  The TC will be a better choice.</p></li><li class="listitem">
On the flipside, if you need only a few long-lived, persistent connection
objects to the cluster, a node client can be a bit more efficient since it knows
the cluster layout.  But it ties your application into the cluster, so it may
pose problems from a firewall perspective.
</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="_configuration_management"></a>Configuration Management</h2></div></div></div><p>If you use configuration management already (Puppet, Chef, Ansible), you can skip this tip.<a id="id-1.10.4.8.2.1" class="indexterm"></a>
<a id="id-1.10.4.8.2.2" class="indexterm"></a><a id="id-1.10.4.8.2.3" class="indexterm"></a></p><p>If you don’t use configuration management tools yet, you should!  Managing
a handful of servers by <code class="literal">parallel-ssh</code> may work now, but it will become a nightmare
as you grow your cluster.  It is almost impossible to edit 30 configuration files
by hand without making a mistake.</p><p>Configuration management tools help make your cluster consistent by automating
the process of config changes.  It may take a little time to set up and learn,
but it will pay itself off handsomely over time.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="_important_configuration_changes"></a>Important Configuration Changes</h2></div></div></div><p>Elasticsearch ships with <span class="emphasis"><em>very good</em></span> defaults,<a id="id-1.10.4.9.2.2" class="indexterm"></a>
<a id="id-1.10.4.9.2.3" class="indexterm"></a><a id="id-1.10.4.9.2.4" class="indexterm"></a> especially when it comes to performance-
related settings and options.  When in doubt, just leave
the settings alone.  We have witnessed countless dozens of clusters ruined
by errant settings because the administrator thought he could turn a knob
and gain 100-fold improvement.</p><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>Please read this entire section!  All configurations presented are equally
important, and are not listed in any particular order.  Please read
through all configuration options and apply them to your cluster.</p></div></div><p>Other databases may require tuning, but by and large, Elasticsearch does not.
If you are hitting performance problems, the solution is usually better data
layout or more nodes.  There are very few "magic knobs" in Elasticsearch.
If there were, we’d have turned them already!</p><p>With that said, there are some <span class="emphasis"><em>logistical</em></span> configurations that should be changed
for production.  These changes are necessary either to make your life easier, or because
there is no way to set a good default (because it depends on your cluster layout).</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_assign_names"></a>Assign Names</h3></div></div></div><p>Elasticseach by default starts a cluster named <code class="literal">elasticsearch</code>. <a id="id-1.10.4.9.6.2.2" class="indexterm"></a>
<a id="id-1.10.4.9.6.2.3" class="indexterm"></a> It is wise
to rename your production cluster to something else, simply to prevent accidents
whereby someone’s laptop joins the cluster.  A simple change to <code class="literal">elasticsearch_production</code>
can save a lot of heartache.</p><p>This can be changed in your <code class="literal">elasticsearch.yml</code> file:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-yaml">cluster.name: elasticsearch_production</pre></div><p>Similarly, it is wise to change the names of your nodes. As you’ve probably
noticed by now, Elasticsearch assigns a random Marvel superhero name
to your nodes at startup.  This is cute in development—but less cute when it is
3a.m. and you are trying to remember which physical machine was Tagak the Leopard Lord.</p><p>More important, since these names are generated on startup, each time you
restart your node, it will get a new name. This can make logs confusing,
since the names of all the nodes are constantly changing.</p><p>Boring as it might be, we recommend you give each node a name that makes sense
to you—a plain, descriptive name.  This is also configured in your <code class="literal">elasticsearch.yml</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-yaml">node.name: elasticsearch_005_data</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_paths"></a>Paths</h3></div></div></div><p>By default, Elasticsearch will place the plug-ins,<a id="id-1.10.4.9.7.2.1" class="indexterm"></a>
<a id="id-1.10.4.9.7.2.2" class="indexterm"></a><a id="id-1.10.4.9.7.2.3" class="indexterm"></a> logs, and—most important—your data in the installation directory.  This can lead to
unfortunate accidents, whereby the installation directory is accidentally overwritten
by a new installation of Elasticsearch. If you aren’t careful, you can erase all your data.</p><p>Don’t laugh—we’ve seen it happen more than a few times.</p><p>The best thing to do is relocate your data directory outside the installation
location.  You can optionally move your plug-in and log directories as well.</p><p>This can be changed as follows:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-yaml">path.data: /path/to/data1,/path/to/data2 <a id="CO308-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

# Path to log files:
path.logs: /path/to/logs

# Path to where plugins are installed:
path.plugins: /path/to/plugins</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO308-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Notice that you can specify more than one directory for data by using comma-separated lists.
</p></td></tr></table></div><p>Data can be saved to multiple directories, and if each directory
is mounted on a different hard drive, this is a simple and effective way to
set up a software RAID 0.  Elasticsearch will automatically stripe
data between the different directories, boosting performance</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_minimum_master_nodes"></a>Minimum Master Nodes</h3></div></div></div><p>The <code class="literal">minimum_master_nodes</code> setting is <span class="emphasis"><em>extremely</em></span> important to the
stability of your cluster.<a id="id-1.10.4.9.8.2.3" class="indexterm"></a>
<a id="id-1.10.4.9.8.2.4" class="indexterm"></a><a id="id-1.10.4.9.8.2.5" class="indexterm"></a>  This setting helps prevent <span class="emphasis"><em>split brains</em></span>, the existence of two masters in a single cluster.</p><p>When you have a split brain, your cluster is at danger of losing data.  Because
the master is considered the supreme ruler of the cluster, it decides
when new indices can be created, how shards are moved, and so forth.  If you have <span class="emphasis"><em>two</em></span>
masters, data integrity becomes perilous, since you have two nodes
that think they are in charge.</p><p>This setting tells Elasticsearch to not elect a master unless there are enough
master-eligible nodes available.  Only then will an election take place.</p><p>This setting should always be configured to a quorum (majority) of your master-eligible nodes.<a id="id-1.10.4.9.8.5.1" class="indexterm"></a>  A quorum is <code class="literal">(number of master-eligible nodes / 2) + 1</code>.
Here are some examples:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
If you have ten regular nodes (can hold data, can become master), a quorum is
<code class="literal">6</code>.
</li><li class="listitem">
If you have three dedicated master nodes and a hundred data nodes, the quorum is <code class="literal">2</code>,
since you need to count only nodes that are master eligible.
</li><li class="listitem">
If you have two regular nodes, you are in a conundrum.  A quorum would be
<code class="literal">2</code>, but this means a loss of one node will make your cluster inoperable.  A
setting of <code class="literal">1</code> will allow your cluster to function, but doesn’t protect against
split brain.  It is best to have a minimum of three nodes in situations like this.
</li></ul></div><p>This setting can be configured in your <code class="literal">elasticsearch.yml</code> file:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-yaml">discovery.zen.minimum_master_nodes: 2</pre></div><p>But because Elasticsearch clusters are dynamic, you could easily add or remove
nodes that will change the quorum.  It would be extremely irritating if you had
to push new configurations to each node and restart your whole cluster just to
change the setting.</p><p>For this reason, <code class="literal">minimum_master_nodes</code> (and other settings) can be configured
via a dynamic API call.  You can change the setting while your cluster is online:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-js">PUT /_cluster/settings
{
    "persistent" : {
        "discovery.zen.minimum_master_nodes" : 2
    }
}</pre></div><p>This will become a persistent setting that takes precedence over whatever is
in the static configuration.  You should modify this setting whenever you add or
remove master-eligible nodes.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_recovery_settings"></a>Recovery Settings</h3></div></div></div><p>Several settings affect the behavior of shard recovery when
your cluster restarts.<a id="id-1.10.4.9.9.2.1" class="indexterm"></a><a id="id-1.10.4.9.9.2.2" class="indexterm"></a>
<a id="id-1.10.4.9.9.2.3" class="indexterm"></a>  First, we need to understand what happens if nothing is
configured.</p><p>Imagine you have ten nodes, and each node holds a single shard—either a primary
or a replica—in a 5 primary / 1 replica index.  You take your
entire cluster offline for maintenance (installing new drives, for example).  When you
restart your cluster, it just so happens that five nodes come online before
the other five.</p><p>Maybe the switch to the other five is being flaky, and they didn’t
receive the restart command right away.  Whatever the reason, you have five nodes
online.  These five nodes will gossip with each other, elect a master, and form a
cluster.  They notice that data is no longer evenly distributed, since five
nodes are missing from the cluster, and immediately start replicating new
shards between each other.</p><p>Finally, your other five nodes turn on and join the cluster.  These nodes see
that <span class="emphasis"><em>their</em></span> data is being replicated to other nodes, so they delete their local
data (since it is now redundant, and may be outdated).  Then the cluster starts
to rebalance even more, since the cluster size just went from five to ten.</p><p>During this whole process, your nodes are thrashing the disk and network, moving
data around—for no good reason. For large clusters with terabytes of data,
this useless shuffling of data can take a <span class="emphasis"><em>really long time</em></span>.  If all the nodes
had simply waited for the cluster to come online, all the data would have been
local and nothing would need to move.</p><p>Now that we know the problem, we can configure a few settings to alleviate it.
First, we need to give Elasticsearch a hard limit:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-yaml">gateway.recover_after_nodes: 8</pre></div><p>This will prevent Elasticsearch from starting a recovery until at least eight (data or master) nodes
are present.  The value for this setting is a matter of personal preference: how
many nodes do you want present before you consider your cluster functional?
In this case, we are setting it to <code class="literal">8</code>, which means the cluster is inoperable
unless there are at least eight nodes.</p><p>Then we tell Elasticsearch how many nodes <span class="emphasis"><em>should</em></span> be in the cluster, and how
long we want to wait for all those nodes:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-yaml">gateway.expected_nodes: 10
gateway.recover_after_time: 5m</pre></div><p>What this means is that Elasticsearch will do the following:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Wait for eight nodes to be present
</li><li class="listitem">
Begin recovering after 5 minutes <span class="emphasis"><em>or</em></span> after ten nodes have joined the cluster,
whichever comes first.
</li></ul></div><p>These three settings allow you to avoid the excessive shard swapping that can
occur on cluster restarts.  It can literally make recovery take seconds instead
of hours.</p><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>These settings can only be set in the <code class="literal">config/elasticsearch.yml</code> file or on
the command line (they are not dynamically updatable) and they are only relevant
during a full cluster restart.</p></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_prefer_unicast_over_multicast"></a>Prefer Unicast over Multicast</h3></div></div></div><p>Elasticsearch is configured to use multicast discovery out of the box.  Multicast<a id="id-1.10.4.9.10.2.1" class="indexterm"></a>
<a id="id-1.10.4.9.10.2.2" class="indexterm"></a><a id="id-1.10.4.9.10.2.3" class="indexterm"></a><a id="id-1.10.4.9.10.2.4" class="indexterm"></a>
works by sending UDP pings across your local network to discover nodes.  Other
Elasticsearch nodes will receive these pings and respond.  A cluster is formed
shortly after.</p><p>Multicast is excellent for development, since you don’t need to do anything.  Turn
a few nodes on, and they automatically find each other and form a cluster.</p><p>This ease of use is the exact reason you should disable it in production.  The
last thing you want is for nodes to accidentally join your production network, simply
because they received an errant multicast ping.  There is nothing wrong with
multicast <span class="emphasis"><em>per se</em></span>.  Multicast simply leads to silly problems, and can be a bit
more fragile (for example, a network engineer fiddles with the network without telling
you—and all of a sudden nodes can’t find each other anymore).</p><p>In production, it is recommended to use unicast instead of multicast.  This works
by providing Elasticsearch a list of nodes that it should try to contact.  Once
the node contacts a member of the unicast list, it will receive a full cluster
state that lists all nodes in the cluster.  It will then proceed to contact
the master and join.</p><p>This means your unicast list does not need to hold all the nodes in your cluster.
It just needs enough nodes that a new node can find someone to talk to.  If you
use dedicated masters, just list your three dedicated masters and call it a day.
This setting is configured in your <code class="literal">elasticsearch.yml</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-yaml">discovery.zen.ping.multicast.enabled: false <a id="CO309-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO309-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Make sure you disable multicast, since it can operate in parallel with unicast.
</p></td></tr></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="_don_8217_t_touch_these_settings"></a>Don’t Touch These Settings!</h2></div></div></div><p>There are a few hotspots in Elasticsearch that people just can’t seem to avoid
tweaking. <a id="id-1.10.4.10.2.1" class="indexterm"></a>
<a id="id-1.10.4.10.2.2" class="indexterm"></a> We understand:  knobs just beg to be turned. But of all the knobs to turn, these you should <span class="emphasis"><em>really</em></span> leave alone. They are
often abused and will contribute to terrible stability or terrible performance.
Or both.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_garbage_collector"></a>Garbage Collector</h3></div></div></div><p>As briefly introduced in <a class="xref" href="cluster-admin.html#garbage_collector_primer" title="Garbage Collection Primer">Garbage Collection Primer</a>, the JVM uses a garbage
collector to free unused memory.<a id="id-1.10.4.10.3.2.2" class="indexterm"></a>  This tip is really an extension of the last tip,
but deserves its own section for emphasis:</p><p>Do not change the default garbage collector!</p><p>The default GC for Elasticsearch is Concurrent-Mark and Sweep (CMS).<a id="id-1.10.4.10.3.4.1" class="indexterm"></a>  This GC
runs concurrently with the execution of the application so that it can minimize
pauses.  It does, however, have two stop-the-world phases.  It also has trouble
collecting large heaps.</p><p>Despite these downsides, it is currently the best GC for low-latency server software
like Elasticsearch.  The official recommendation is to use CMS.</p><p>There is a newer GC called the Garbage First GC (G1GC). <a id="id-1.10.4.10.3.6.1" class="indexterm"></a> This newer GC is designed
to minimize pausing even more than CMS, and operate on large heaps.  It works
by dividing the heap into regions and predicting which regions contain the most
reclaimable space.  By collecting those regions first (<span class="emphasis"><em>garbage first</em></span>), it can
minimize pauses and operate on very large heaps.</p><p>Sounds great!  Unfortunately, G1GC is still new, and fresh bugs are found routinely.
These bugs are usually of the segfault variety, and will cause hard crashes.
The Lucene test suite is brutal on GC algorithms, and it seems that G1GC hasn’t
had the kinks worked out yet.</p><p>We would like to recommend G1GC someday, but for now, it is simply not stable
enough to meet the demands of Elasticsearch and Lucene.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_threadpools"></a>Threadpools</h3></div></div></div><p>Everyone <span class="emphasis"><em>loves</em></span> to tweak threadpools.<a id="id-1.10.4.10.4.2.2" class="indexterm"></a>  For whatever reason, it seems people
cannot resist increasing thread counts.  Indexing a lot?  More threads!  Searching
a lot? More threads!  Node idling 95% of the time?  More threads!</p><p>The default threadpool settings in Elasticsearch are very sensible.  For all
threadpools (except <code class="literal">search</code>) the threadcount is set to the number of CPU cores.
If you have eight cores, you can be running only eight threads simultaneously.  It makes
sense to assign only eight threads to any particular threadpool.</p><p>Search gets a larger threadpool, and is configured to <code class="literal"># cores * 3</code>.</p><p>You might argue that some threads can block (such as on a disk I/O operation),
which is why you need more threads.  This is not a problem in Elasticsearch:
much of the disk I/O is handled by threads managed by Lucene, not Elasticsearch.</p><p>Furthermore, threadpools cooperate by passing work between each other.  You don’t
need to worry about a networking thread blocking because it is waiting on a disk
write.  The networking thread will have long since handed off that work unit to
another threadpool and gotten back to networking.</p><p>Finally, the compute capacity of your process is finite.  Having more threads just forces
the processor to switch thread contexts.  A processor can run only one thread
at a time, so when it needs to switch to a different thread, it stores the current
state (registers, and so forth) and loads another thread.  If you are lucky, the switch
will happen on the same core.  If you are unlucky, the switch may migrate to a
different core and require transport on an inter-core communication bus.</p><p>This context switching eats up cycles simply by doing administrative housekeeping; estimates can peg it as high as 30μs on modern CPUs.  So unless the thread
will be blocked for longer than 30μs, it is highly likely that that time would
have been better spent just processing and finishing early.</p><p>People routinely set threadpools to silly values.  On eight core machines, we have
run across configs with 60, 100, or even 1000 threads.  These settings will simply
thrash the CPU more than getting real work done.</p><p>So. Next time you want to tweak a threadpool, please don’t.  And if you
<span class="emphasis"><em>absolutely cannot resist</em></span>, please keep your core count in mind and perhaps set
the count to double.  More than that is just a waste.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="heap-sizing"></a>Heap: Sizing and Swapping</h2></div></div></div><p>The default installation of Elasticsearch is configured with a 1 GB heap. <a id="id-1.10.4.11.2.1" class="indexterm"></a>
<a id="id-1.10.4.11.2.2" class="indexterm"></a><a id="id-1.10.4.11.2.3" class="indexterm"></a>
<a id="id-1.10.4.11.2.4" class="indexterm"></a> For
just about every deployment, this number is far too small.  If you are using the
default heap values, your cluster is probably configured incorrectly.</p><p>There are two ways to change the heap size in Elasticsearch.  The easiest is to
set an environment variable called <code class="literal">ES_HEAP_SIZE</code>.<a id="id-1.10.4.11.3.2" class="indexterm"></a>  When the server process
starts, it will read this environment variable and set the heap accordingly.
As an example, you can set it via the command line as follows:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-bash">export ES_HEAP_SIZE=10g</pre></div><p>Alternatively, you can pass in the heap size via a command-line argument when starting
the process, if that is easier for your setup:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-bash">./bin/elasticsearch -Xmx10g -Xms10g <a id="CO310-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO310-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Ensure that the min (<code class="literal">Xms</code>) and max (<code class="literal">Xmx</code>) sizes are the same to prevent
the heap from resizing at runtime, a very costly process.
</p></td></tr></table></div><p>Generally, setting the <code class="literal">ES_HEAP_SIZE</code> environment variable is preferred over setting
explicit <code class="literal">-Xmx</code> and <code class="literal">-Xms</code> values.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_give_half_your_memory_to_lucene"></a>Give Half Your Memory to Lucene</h3></div></div></div><p>A common problem is configuring a heap that is <span class="emphasis"><em>too</em></span> large. <a id="id-1.10.4.11.9.2.2" class="indexterm"></a>
<a id="id-1.10.4.11.9.2.3" class="indexterm"></a>
<a id="id-1.10.4.11.9.2.4" class="indexterm"></a> You have a 64 GB
machine—and by golly, you want to give Elasticsearch all 64 GB of memory.  More
is better!</p><p>Heap is definitely important to Elasticsearch.  It is used by many in-memory data
structures to provide fast operation.  But with that said, there is another major
user of memory that is <span class="emphasis"><em>off heap</em></span>: Lucene.</p><p>Lucene is designed to leverage the underlying OS for caching in-memory data structures.<a id="id-1.10.4.11.9.4.1" class="indexterm"></a>
<a id="id-1.10.4.11.9.4.2" class="indexterm"></a>
Lucene segments are stored in individual files.  Because segments are immutable,
these files never change.  This makes them very cache friendly, and the underlying
OS will happily keep hot segments resident in memory for faster access.</p><p>Lucene’s performance relies on this interaction with the OS.  But if you give all
available memory to Elasticsearch’s heap, there won’t be any left over for Lucene.
This can seriously impact the performance of full-text search.</p><p>The standard recommendation is to give 50% of the available memory to Elasticsearch
heap, while leaving the other 50% free.  It won’t go unused; Lucene will happily
gobble up whatever is left over.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="compressed_oops"></a>Don’t Cross 30.5 GB!</h3></div></div></div><p>There is another reason to not allocate enormous heaps to Elasticsearch. As it turns<a id="id-1.10.4.11.10.2.1" class="indexterm"></a>
<a id="id-1.10.4.11.10.2.2" class="indexterm"></a>
<a id="id-1.10.4.11.10.2.3" class="indexterm"></a><a id="id-1.10.4.11.10.2.4" class="indexterm"></a>
out, the JVM uses a trick to compress object pointers when heaps are 30.5 GB or less.</p><p>In Java, all objects are allocated on the heap and referenced by a pointer.
Ordinary object pointers (OOP) point at these objects, and are traditionally
the size of the CPU’s native <span class="emphasis"><em>word</em></span>: either 32 bits or 64 bits, depending on the
processor.  The pointer references the exact byte location of the value.</p><p>For 32-bit systems, this means the maximum heap size is 4 GB.  For 64-bit systems,
the heap size can get much larger, but the overhead of 64-bit pointers means there
is more wasted space simply because the pointer is larger.  And worse than wasted
space, the larger pointers eat up more bandwidth when moving values between
main memory and various caches (LLC, L1, and so forth).</p><p>Java uses a trick called <a class="ulink" href="https://wikis.oracle.com/display/HotSpotInternals/CompressedOops" target="_top">compressed oops</a><a id="id-1.10.4.11.10.5.2" class="indexterm"></a>
to get around this problem.  Instead of pointing at exact byte locations in
memory, the pointers reference <span class="emphasis"><em>object offsets</em></span>.<a id="id-1.10.4.11.10.5.4" class="indexterm"></a>  This means a 32-bit pointer can
reference four billion <span class="emphasis"><em>objects</em></span>, rather than four billion bytes.  Ultimately, this
means the heap can grow to around 32 GB of physical size while still using a 32-bit
pointer.</p><p>Once you cross that magical 30.5 GB boundary, the pointers switch back to
ordinary object pointers.  The size of each pointer grows, more CPU-memory
bandwidth is used, and you effectively lose memory.  In fact, it takes until around
40–50 GB of allocated heap before you have the same <span class="emphasis"><em>effective</em></span> memory of a 30.5 GB
heap using compressed oops.</p><p>The moral of the story is this: even when you have memory to spare, try to avoid
crossing the 30.5 GB heap boundary.  It wastes memory, reduces CPU performance, and
makes the GC struggle with large heaps.</p><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong>I Have a Machine with 1 TB RAM!</strong></p></div></div></div><p>The 30.5 GB line is fairly important.  So what do you do when your machine has a lot
of memory?  It is becoming increasingly common to see super-servers with 512–768 GB
of RAM.</p><p>First, we would recommend avoiding such large machines (see <a class="xref" href="deploy.html#hardware" title="Hardware">Hardware</a>).</p><p>But if you already have the machines, you have two practical options:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Are you doing mostly full-text search?  Consider giving 30.5 GB to Elasticsearch
and letting Lucene use the rest of memory via the OS filesystem cache.  All that
memory will cache segments and lead to blisteringly fast full-text search.
</li><li class="listitem"><p class="simpara">
Are you doing a lot of sorting/aggregations?  You’ll likely want that memory
in the heap then.  Instead of one node with more than 31.5 GB of RAM, consider running two or
more nodes on a single machine.  Still adhere to the 50% rule, though.  So if your
machine has 128 GB of RAM, run two nodes, each with 30.5 GB.  This means 61 GB will be
used for heaps, and 67 will be left over for Lucene.
</p><p class="simpara">If you choose this option, set <code class="literal">cluster.routing.allocation.same_shard.host: true</code>
in your config.  This will prevent a primary and a replica shard from colocating
to the same physical machine (since this would remove the benefits of replica high availability).</p></li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_swapping_is_the_death_of_performance"></a>Swapping Is the Death of Performance</h3></div></div></div><p>It should be obvious,<a id="id-1.10.4.11.11.2.1" class="indexterm"></a>
<a id="id-1.10.4.11.11.2.2" class="indexterm"></a>
<a id="id-1.10.4.11.11.2.3" class="indexterm"></a><a id="id-1.10.4.11.11.2.4" class="indexterm"></a>
<a id="id-1.10.4.11.11.2.5" class="indexterm"></a><a id="id-1.10.4.11.11.2.6" class="indexterm"></a> but it bears spelling out clearly: swapping main memory
to disk will <span class="emphasis"><em>crush</em></span> server performance.  Think about it: an in-memory operation
is one that needs to execute quickly.</p><p>If memory swaps to disk, a 100-microsecond operation becomes one that take 10
milliseconds.  Now repeat that increase in latency for all other 10us operations.
It isn’t difficult to see why swapping is terrible for performance.</p><p>The best thing to do is disable swap completely on your system.  This can be done
temporarily:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-bash">sudo swapoff -a</pre></div><p>To disable it permanently, you’ll likely need to edit your <code class="literal">/etc/fstab</code>.  Consult
the documentation for your OS.</p><p>If disabling swap completely is not an option, you can try to lower <code class="literal">swappiness</code>.
This value controls how aggressively the OS tries to swap memory.
This prevents swapping under normal circumstances, but still allows the OS to swap
under emergency memory situations.</p><p>For most Linux systems, this is configured using the <code class="literal">sysctl</code> value:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-bash">vm.swappiness = 1 <a id="CO311-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO311-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
A <code class="literal">swappiness</code> of <code class="literal">1</code> is better than <code class="literal">0</code>, since on some kernel versions a <code class="literal">swappiness</code>
of <code class="literal">0</code> can invoke the OOM-killer.
</p></td></tr></table></div><p>Finally, if neither approach is possible, you should enable <code class="literal">mlockall</code>.
 file.  This allows the JVM to lock its memory and prevent
it from being swapped by the OS.  In your <code class="literal">elasticsearch.yml</code>, set this:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-yaml">bootstrap.mlockall: true</pre></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="_file_descriptors_and_mmap"></a>File Descriptors and MMap</h2></div></div></div><p>Lucene uses a <span class="emphasis"><em>very</em></span> large number of files. <a id="id-1.10.4.12.2.2" class="indexterm"></a>
<a id="id-1.10.4.12.2.3" class="indexterm"></a> At the same time, Elasticsearch
uses a large number of sockets to communicate between nodes and HTTP clients.
All of this requires available file descriptors.<a id="id-1.10.4.12.2.4" class="indexterm"></a></p><p>Sadly, many modern Linux distributions ship with a paltry 1,024 file descriptors
allowed per process.  This is <span class="emphasis"><em>far</em></span> too low for even a small Elasticsearch
node, let alone one that is handling hundreds of indices.</p><p>You should increase your file descriptor count to something very large, such as
64,000.  This process is irritatingly difficult and highly dependent on your
particular OS and distribution.  Consult the documentation for your OS to determine
how best to change the allowed file descriptor count.</p><p>Once you think you’ve changed it, check Elasticsearch to make sure it really does
have enough file descriptors:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-js">GET /_nodes/process

{
   "cluster_name": "elasticsearch__zach",
   "nodes": {
      "TGn9iO2_QQKb0kavcLbnDw": {
         "name": "Zach",
         "transport_address": "inet[/192.168.1.131:9300]",
         "host": "zacharys-air",
         "ip": "192.168.1.131",
         "version": "2.0.0-SNAPSHOT",
         "build": "612f461",
         "http_address": "inet[/192.168.1.131:9200]",
         "process": {
            "refresh_interval_in_millis": 1000,
            "id": 19808,
            "max_file_descriptors": 64000, <a id="CO312-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
            "mlockall": true
         }
      }
   }
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO312-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The <code class="literal">max_file_descriptors</code> field shows the number of available descriptors that
the Elasticsearch process can access.
</p></td></tr></table></div><p>Elasticsearch also uses a mix of NioFS and MMapFS <a id="id-1.10.4.12.8.1" class="indexterm"></a>for the various files.  Ensure
that you configure the maximum map count so that there is ample virtual memory available for
mmapped files.  This can be set temporarily:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-js">sysctl -w vm.max_map_count=262144</pre></div><p>Or you can set it permanently by modifying <code class="literal">vm.max_map_count</code> setting in your <code class="literal">/etc/sysctl.conf</code>.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="_revisit_this_list_before_production"></a>Revisit This List Before Production</h2></div></div></div><p>You are likely reading this section before you go into production.
The details covered in this chapter are good to be generally aware of, but it is
critical to revisit this entire list right before deploying to production.</p><p>Some of the topics will simply stop you cold (such as too few available file
descriptors).  These are easy enough to debug because they are quickly apparent.
Other issues, such as split brains and memory settings, are visible only after
something bad happens.  At that point, the resolution is often messy and tedious.</p><p>It is much better to proactively prevent these situations from occurring by configuring
your cluster appropriately <span class="emphasis"><em>before</em></span> disaster strikes.  So if you are going to
dog-ear (or bookmark) one section from the entire book, this chapter would be
a good candidate.  The week before deploying to production, simply flip through
the list presented here and check off all the recommendations.</p></div></div><div class="navfooter"><span class="prev"><a href="cluster-admin.html">
              « 
              모니터링</a>
           
        </span><span class="next">
           
          <a href="post_deploy.html">배포 후
               »
            </a></span></div>
<!-- end body -->

            </section>

            <div id="rtpcontainer"
        >
	<h3>Top Videos</h3>
	<ul class="lists">
		<li><a href="https://www.elastic.co/webinars/get-started-with-elasticsearch/?baymax=default&elektra=docs&storm=top-video">Getting Started</a></li>
		<li><a href="http://www.elastic.co/webinars/elasticsearch-performance-optimization-tale-two-tickets/?elektra=docs">Scaling</a></li>
		<li><a href="http://www.elastic.co/webinars/shield-securing-your-data-in-elasticsearch/?elektra=docs">Security</a></li>
	</ul>
</div>


	  	




<script src="//app-lon02.marketo.com/js/forms2/js/forms2.min.js"></script>
	<!-- subscribe-newsletter -->
	<div id="subscribe-newsletter">
		<div class="container">
			<div class="row">
				<div class="12u">
					<div class="subscribe-wrapper" id="rtp-newsletter">
						<header>
							<h3>Subscribe to our newsletter</h3>
						</header>
						<div class="subscribe-form">
							<form id="mktoForm_1398" class="container"></form>
							<div class="form_thanks hide">
								
									<p>Thanks for subscribing! We'll keep you updated with new releases.</p>
								
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>

  	<!-- Footer -->
	<div id="footer-wrapper">
		<section id="footer" class="container">
			<div class="row">
				<div class="12u">
					<!-- Copyright -->
					<div id="copyright">
	<p>
		© 2016. All Rights Reserved - Elasticsearch
	</p>
	<ul class="links">
		<li>Elasticsearch is a trademark of Elasticsearch BV, registered in the U.S. and in other countries</li>
		<li><a href="/legal/trademarks" id="footer_trademarks">Trademarks</a></li>
		<li><a href="/legal/terms-of-use" id="footer_terms">Terms</a></li>
		<li><a href="/legal/privacy-policy" id="footer_privacy">Privacy</a></li>
	</ul>
	<p>
		Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of the <a href="http://www.apache.org/" target="_blank">Apache Software Foundation</a> in the United States and/or other&nbsp;countries.
	</p>
</div><!--<div class="row">
{replace4}
</div>-->
				</div>
			</div>					
		</section>
	</div>
			
	<!-- Social Media -->
	<div id="socialmedia-wrapper">
		<section id="socialmedia" class="container">
			<!-- social icons -->
			<div id="social">
				<ul class="links">
					
						<li class="facebook grayscale"><a href="http://www.facebook.com/elastic.co" target="_blank" id="footer_facebook"></a></li>
					
						<li class="twitter grayscale"><a href="https://www.twitter.com/elastic" target="_blank" id="footer_twitter"></a></li>
					
						<li class="linkedin grayscale"><a href="https://www.linkedin.com/company/elastic-co" target="_blank" id="footer_linkedin"></a></li>
					
						<li class="xing grayscale"><a href="https://www.xing.com/companies/elastic.co" target="_blank" id="footer_xing"></a></li>
					
						<li class="youtube grayscale"><a href="https://www.youtube.com/user/elasticsearch" target="_blank" id="footer_youtube"></a></li>
					
				</ul>
			</div>
		</section>
	</div>


<style type="text/css">
	
		.facebook{background:url("https://static-www.elastic.co/assets/bltda40eebd7c7cae9a/facebook.svg?q=100") no-repeat;}		
	
		.twitter{background:url("https://static-www.elastic.co/assets/bltfd3b1511512f4632/twitter.svg?q=100") no-repeat;}		
	
		.linkedin{background:url("https://static-www.elastic.co/assets/blt6b4f6f8d6b96f814/linkedin.svg?q=100") no-repeat;}		
	
		.xing{background:url("https://static-www.elastic.co/assets/blta76b2ae2b2f10a98/xing.svg?q=100") no-repeat;}		
	
		.youtube{background:url("https://static-www.elastic.co/assets/blt5761648c9581fb53/youtube.svg?q=100") no-repeat;}		
	
</style>

<script>
MktoForms2.loadForm("//app-lon02.marketo.com", "813-MAM-392", 1398,function(form){
	form.onSuccess(function(form){
		$('#mktoForm_1398').css('display','none');
		$('#subscribe-newsletter header').hide();
		$('#mktoForm_1398').siblings('.form_thanks').removeClass('hide')
		dataLayer.push({'event': 'mktoFormSubmit'});
	    return false;
  	});
});
</script>	

  	</div>
  	<!-- <div class="preloader">
    	<div class="status"></div>
	</div> -->
  	<style></style>
   	<script type="text/javascript">
	var suggestionsUrl = "https://search.elastic.co/suggest";	
</script>
<script src="https://static-www.elastic.co/static/js/jquery.autocomplete.js?q=100"></script>
<script src="https://static-www.elastic.co/static/js/script.js?q=100"></script>


   	
   	
   	
   	<script type="application/ld+json">
	
		
	
	{
	  	"@context": "http://schema.org",
	  	"@type": "Organization",
	  	"name" : "Elastic",
	  	"url": "https://www.elastic.co/",
	  	"logo": "https://www.elastic.co/static/img/elastic-logo-200.png",
		"sameAs" : [ "https://www.facebook.com/elastic.co",
			"https://twitter.com/elastic",
			"https://plus.google.com/105178019064686397293",
			"https://www.youtube.com/user/elasticsearch",
			"https://www.linkedin.com/company/elasticsearch"
		],
	  	"potentialAction": {
	    	"@type": "SearchAction",
	    	"target": "https://www.elastic.co/search?q={query_string}",
	    	"query-input": "required name=query_string"
	  	}
	}
</script>	

            <script type="text/javascript" src="docs.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script type='text/javascript' src='https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js'></script>

            </body>
        

