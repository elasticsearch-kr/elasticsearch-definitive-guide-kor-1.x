
<!DOCTYPE HTML>
<html lang="en-us">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Designing for Scale</title><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><link rel="home" href="index.html" title="Elasticsearch: The Definitive Guide" /><link rel="up" href="modeling-your-data.html" title="Modeling Your Data" /><link rel="prev" href="parent-child.html" title="Parent-Child Relationship" /><link rel="next" href="administration.html" title="관리, 모니터링, 배포" />
  
 	<meta http-equiv="content-type" content="text/html; charset=utf-8" />

 	<!-- RTP tag --> 
	<script type='text/javascript' async>
	(function(c,h,a,f,i,e){c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
	 c[a].a=i;c[a].e=e;var g=h.createElement("script");g.async=true;g.type="text/javascript";
	 g.src=f+'?aid='+i;var b=h.getElementsByTagName("script")[0];b.parentNode.insertBefore(g,b);
	 })(window,document,"rtp","//sjrtp2-cdn.marketo.com/rtp-api/v1/rtp.js","elasticco");
	 
	rtp('send','view');
	rtp('get', 'campaign',true);
	</script>
	<!-- End of RTP tag -->
	
	
 	<link rel="apple-touch-icon" sizes="57x57"  href="https://www.elastic.co/apple-icon-57x57.png?change=123">
<link rel="apple-touch-icon" sizes="60x60"  href="https://www.elastic.co/apple-icon-60x60.png?change=123">
<link rel="apple-touch-icon" sizes="72x72"  href="https://www.elastic.co/apple-icon-72x72.png?change=123">
<link rel="apple-touch-icon" sizes="76x76"  href="https://www.elastic.co/apple-icon-76x76.png?change=123">
<link rel="apple-touch-icon" sizes="114x114"  href="https://www.elastic.co/apple-icon-114x114.png?change=123">
<link rel="apple-touch-icon" sizes="120x120"  href="https://www.elastic.co/apple-icon-120x120.png?change=123">
<link rel="apple-touch-icon" sizes="144x144"  href="https://www.elastic.co/apple-icon-144x144.png?change=123">
<link rel="apple-touch-icon" sizes="152x152"  href="https://www.elastic.co/apple-icon-152x152.png?change=123">
<link rel="apple-touch-icon" sizes="180x180"  href="https://www.elastic.co/apple-icon-180x180.png?change=123">
<link rel="icon" type="image/png" sizes="192x192"   href="https://www.elastic.co/android-icon-192x192.png?change=123">
<link rel="icon" type="image/png" sizes="32x32"  href="https://www.elastic.co/favicon-32x32.png?change=123">
<link rel="icon" type="image/png" sizes="96x96"  href="https://www.elastic.co/favicon-96x96.png?change=123">
<link rel="icon" type="image/png" sizes="16x16"  href="https://www.elastic.co/favicon-16x16.png?change=123">
<link rel="manifest"  href="https://www.elastic.co/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<!-- DC tags section -->
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/" >
<link rel="schema.DCTERMS" href="http://purl.org/dc/terms/" >
<meta name="DC.title" content="Guide template" >

<!-- end DC tags -->

	<meta name="description" content="" />

	<link rel="canonical" href="" />

	<meta property="og:image" content="https://www.elastic.co/static/img/elastic-logo-200.png" />
	


	
	<link type="text/css" rel="stylesheet" href="https://static-www.elastic.co/static/css/skel.css?q=100" />
	<link type="text/css" rel="stylesheet" href="https://static-www.elastic.co/static/css/style.css?q=100" />	
	<script src="https://static-www.elastic.co/static/js/skel.min.js?q=100"></script>
	
		<script type="text/javascript">form_name = "" ; cdn_url="https://static-www.elastic.co";cdn_slug = "100"</script>
	
	<script src="https://static-www.elastic.co/static/js/init.js?q=100"></script>
	<script src="https://static-www.elastic.co/static/js/jquery.min.js?q=100"></script>
	<script src="https://static-www.elastic.co/static/js/jquery.cookie.js?q=100"></script>
	<!--<script type="text/javascript">
	    (function() {
	        var path = '//easy.myfonts.net/v2/js?sid=10336(font-family=Avenir+35+Light)&sid=10338(font-family=Avenir+55+Roman)&sid=10340(font-family=Avenir+85+Heavy)&sid=10344(font-family=Avenir+65+Medium)&key=nAD6PCOPrX',
	            protocol = ('https:' == document.location.protocol ? 'https:' : 'http:'),
	            trial = document.createElement('script');
	        trial.type = 'text/javascript';
	        trial.async = true;
	        trial.src = protocol + path;
	        var head = document.getElementsByTagName("head")[0];
	        head.appendChild(trial);
	    })();
	</script>-->

	
	<link type="text/css" rel="stylesheet"  href="https://www.elastic.co/static/css/prettify.css" />
	<script type="text/javascript"  src="https://www.elastic.co/static/js/prettify.js"></script>



<link rel="stylesheet" type="text/css" href="styles.css" />

</head>
<body >
	<!-- Google Tag Manager -->
	<script>dataLayer = [];</script><noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-58RLH5"
													  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
			new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
			j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
			'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-58RLH5');</script>
	<!-- End Google Tag Manager -->
	
	<div class="header-cont">
		<div id="header-wrapper">
			


	<script  src="https://www.elastic.co/static/js/jquery.tokeninput.js?change=123"></script>
	<link rel="stylesheet" type="text/css"  href="https://www.elastic.co/static/css/token-input.css">

<div class="shortcuts-wrapper">
	<div class="container">
		<!-- Shortcuts -->
		<nav id="shortcuts">
			<ul class="links">
				
					<li class="shortcut-1">
						<a href="/downloads" id="header_downloads" >
							<span class="txt">downloads</span>	
						</a>
					</li>
				
					<li class="shortcut-2">
						<a href="/guide" id="header_guide" >
							<span class="txt">docs</span>	
						</a>
					</li>
				
					<li class="shortcut-3">
						<a href="/subscriptions" id="header_subscriptions" >
							<span class="txt">support</span>	
						</a>
					</li>
				
					<li class="shortcut-4">
						<a href="https://discuss.elastic.co" id="header_discuss" target="_blank">
							<span class="txt">discuss</span>	
						</a>
					</li>
				
					<li class="shortcut-5">
						<a href="/contact" id="header_contact" >
							<span class="txt">contact</span>	
						</a>
					</li>
				
			</ul>
		</nav>
		<div class="languages-wrapper">
			<div class="global-language">
				<ul class="language">
					<!-- <li class="active-language"><a class="active" href="#">EN</a></li> -->
					
						<li><a href="#">EN</a></li>
					
				</ul>
				<ul class="all-languages">
					
						
						<li><a href="/guide_template">English</a></li>
					
						
						<li><a href="/fr/guide_template">French</a></li>
					
						
						<li><a href="/de/guide_template">German</a></li>
					
						
						<li><a href="/jp/guide_template">Japanese</a></li>
					
						
						<li><a href="/kr/guide_template">Korean</a></li>
					
				</ul>
			</div>
		</div>
	</div>
</div>
<div class="container w100">
	<div class="row 0%">
		<div class="12u">
			

<section class="first-container desktop-main-nav">
	<div class="row 0%">
		<div class="11u 6u(small) 6u(xsmall)">
			<div id="elastic">
				<h1>
					<a href="/">
						<div id="elastic-logo">
						<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="0 0 400 130" preserveAspectRatio="xMinYMin meet" enable-background="new 0 0 400 130" xml:space="preserve">
    <g>
    	<g>
    		<path fill="#FFFFFF" d="M122.5,67.8c0-10.3-6.4-19.2-15.9-22.7c0.4-2.2,0.6-4.3,0.6-6.6c0-19.1-15.5-34.6-34.6-34.6
    			c-11.1,0-21.5,5.3-28,14.4c-3.2-2.5-7.1-3.8-11.2-3.8c-10.1,0-18.4,8.2-18.4,18.4c0,2.2,0.4,4.4,1.1,6.4C6.5,42.6,0,51.8,0,62
    			c0,10.3,6.4,19.3,16,22.8c-0.4,2.1-0.6,4.3-0.6,6.6c0,19,15.5,34.5,34.5,34.5c11.2,0,21.5-5.4,28-14.4c3.2,2.5,7.2,3.9,11.3,3.9
    			c10.1,0,18.4-8.2,18.4-18.4c0-2.2-0.4-4.4-1.1-6.4C115.9,87.2,122.5,78,122.5,67.8z"/>
    		<g>
    			<path fill="#F4BD19" d="M47.9,56.3l27.3,12.5l27.6-24.2c0.4-2,0.6-4,0.6-6.1c0-17-13.8-30.8-30.8-30.8c-10.2,0-19.7,5-25.4,13.4
    				l-4.6,23.8L47.9,56.3z"/>
    		</g>
    		<g>
    			<path fill="#3CBEB1" d="M19.6,85.2c-0.4,2-0.6,4.1-0.6,6.2c0,17,13.9,30.9,30.9,30.9c10.3,0,19.8-5.1,25.6-13.5L80,85l-6.1-11.6
    				L46.5,60.9L19.6,85.2z"/>
    		</g>
    		<g>
    			<path fill="#E9478C" d="M19.4,37.9l18.7,4.4L42.3,21c-2.6-2-5.7-3-9-3c-8.1,0-14.8,6.6-14.8,14.8C18.5,34.5,18.8,36.3,19.4,37.9z
    				"/>
    		</g>
    		<g>
    			<path fill="#2C458F" d="M17.8,42.4C9.4,45.1,3.6,53.2,3.6,62c0,8.6,5.3,16.3,13.3,19.3l26.3-23.8l-4.8-10.3L17.8,42.4z"/>
    		</g>
    		<g>
    			<path fill="#95C63D" d="M80.3,108.7c2.6,2,5.7,3.1,8.9,3.1c8.1,0,14.8-6.6,14.8-14.8c0-1.8-0.3-3.5-0.9-5.1l-18.7-4.4L80.3,108.7
    				z"/>
    		</g>
    		<g>
    			<path fill="#176655" d="M84.1,82.6l20.6,4.8c8.4-2.8,14.2-10.8,14.2-19.6c0-8.6-5.3-16.2-13.3-19.3l-27,23.6L84.1,82.6z"/>
    		</g>
    	</g>
    	<path d="M162.8,76.3c0,0.1,0,0.6,0.1,1.4c0.1,0.8,0.3,1.8,0.6,3c0.3,1.1,0.8,2.4,1.4,3.7c0.6,1.3,1.4,2.5,2.5,3.6
    		c1,1.1,2.3,2,3.8,2.7c1.5,0.7,3.4,1.1,5.6,1.1c1.8,0,3.3-0.1,4.6-0.4c1.3-0.3,2.4-0.7,3.4-1.3c1-0.6,1.9-1.3,2.7-2.2
    		c0.8-0.9,1.6-1.9,2.4-3.1c0.5-0.7,1-1.2,1.6-1.5c0.6-0.3,1.2-0.4,1.8-0.4c1,0,2,0.4,2.8,1.1c0.9,0.7,1.3,1.7,1.3,2.8
    		c0,0.7-0.4,1.7-1.1,3.1c-0.7,1.4-1.9,2.8-3.6,4.1c-1.6,1.4-3.7,2.6-6.3,3.6c-2.6,1-5.7,1.5-9.5,1.5c-3.8,0-7.2-0.7-10.2-2.1
    		c-3-1.4-5.5-3.2-7.5-5.6c-2-2.3-3.6-5-4.7-8.1c-1.1-3.1-1.6-6.3-1.6-9.7c0-3.5,0.5-6.8,1.6-9.9c1.1-3.1,2.7-5.8,4.7-8.1
    		c2-2.3,4.5-4.1,7.4-5.5c2.9-1.3,6.1-2,9.6-2c3.3,0,6.3,0.5,9.2,1.5c2.8,1,5.3,2.4,7.4,4.3c2.1,1.9,3.7,4.1,5,6.8
    		c1.2,2.7,1.8,5.8,1.8,9.2c0,2.1-0.5,3.7-1.6,4.8c-1,1.1-2.2,1.6-3.6,1.6H162.8z M170.9,56.4c-1.5,0.6-2.7,1.4-3.7,2.4
    		c-1,0.9-1.8,2-2.4,3.1c-0.6,1.1-1,2.2-1.3,3.2c-0.3,1-0.5,1.8-0.5,2.5c-0.1,0.7-0.1,1.1-0.1,1.2h27.9c0-1.8-0.3-3.5-0.9-5.1
    		c-0.6-1.6-1.5-3-2.6-4.3c-1.2-1.2-2.7-2.2-4.5-3c-1.8-0.7-4-1.1-6.5-1.1C174.2,55.5,172.4,55.8,170.9,56.4z"/>
    	<path d="M214.3,94.6c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V35.1
    		c0-1.3,0.4-2.3,1.3-3.2c0.9-0.9,2-1.3,3.2-1.3c1.3,0,2.3,0.4,3.2,1.3c0.9,0.9,1.3,1.9,1.3,3.2V94.6z"/>
    	<path d="M229.8,63.5c-0.6,0.3-1.2,0.5-1.9,0.5c-1.2,0-2.2-0.4-3.2-1.1c-1-0.7-1.5-1.8-1.5-3.1c0-0.5,0.1-1.1,0.4-1.9
    		c0.5-1.2,1.2-2.4,2.1-3.6c0.9-1.2,2-2.2,3.5-3.1s3.3-1.7,5.4-2.2c2.2-0.5,4.8-0.8,7.9-0.8c5.9,0,10.5,1.2,14,3.5
    		c3.5,2.3,5.2,5.8,5.2,10.3v32.8c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2v-2
    		c-1.2,2-4.7,4.5-7.5,5.3c-2.8,0.8-5.7,1.2-8.8,1.2c-2,0-3.9-0.3-5.9-0.8c-2-0.5-3.8-1.4-5.3-2.5c-1.6-1.1-2.9-2.6-3.8-4.3
    		c-1-1.7-1.5-3.8-1.5-6.2c0-2.8,0.5-5.7,1.6-7.5c1.1-1.9,2.6-3.4,4.4-4.6c1.8-1.2,3.9-2.1,6.3-2.8c2.4-0.6,4.9-1.1,7.5-1.4
    		c3-0.3,5.4-0.7,7.2-1.2c1.7-0.4,3-0.9,3.9-1.5c0.9-0.6,1.4-1.2,1.6-2c0.2-0.7,0.3-1.6,0.3-2.5c0-1.1-0.3-2-0.9-2.8
    		c-0.6-0.8-1.4-1.5-2.4-2c-1-0.5-2.1-0.9-3.3-1.2s-2.4-0.4-3.5-0.4c-2.1,0-3.7,0.2-5,0.5c-1.3,0.3-2.3,0.8-3.1,1.4
    		c-0.8,0.6-1.5,1.3-1.9,2.1c-0.5,0.8-0.9,1.6-1.3,2.5C230.9,62.6,230.4,63.1,229.8,63.5z M250.6,74.4c-1.1,0.3-2.4,0.6-3.8,0.8
    		c-1.5,0.2-3,0.4-4.6,0.6c-1.6,0.2-3.1,0.5-4.6,0.9c-1.5,0.3-2.8,0.8-4,1.5c-1.2,0.7-2.2,1.5-2.9,2.5c-0.7,1-1.1,3-1.1,4.6
    		c0,1.3,0.3,2.3,0.8,3.1c0.5,0.8,1.2,1.5,2.1,2c0.9,0.5,1.8,0.8,2.9,1c1.1,0.2,2.2,0.3,3.3,0.3c1.7,0,3.3-0.2,5-0.6
    		c1.7-0.4,3.2-1,4.5-1.9s2.4-2,3.3-3.5c0.8-1.4,1.2-3.8,1.2-5.9v-6.1L250.6,74.4z"/>
    	<path d="M307.6,90.9c-1.1,2-2.7,3.6-4.6,4.8s-4.1,2.1-6.6,2.7c-2.5,0.5-5.2,0.8-7.9,0.8c-3.9,0-7.2-0.6-9.9-1.8
    		c-2.7-1.2-4.9-2.6-6.5-4.1c-1.7-1.6-2.9-3.1-3.7-4.7c-0.7-1.6-1.1-2.7-1.1-3.5c0-1.3,0.5-2.4,1.4-3.2c0.9-0.8,2-1.2,3.1-1.2
    		c0.7,0,1.3,0.2,2,0.6c0.6,0.4,1.1,1,1.5,1.9c1,2.5,2.5,4.5,4.7,6.2c2.2,1.7,5.2,2.5,9.3,2.5c1.8,0,3.4-0.2,4.7-0.6
    		c1.3-0.4,2.4-1,3.2-1.7c0.8-0.7,1.4-1.5,1.9-2.4c0.4-0.9,0.6-1.8,0.6-2.8c0-2-0.8-3.4-2.3-4.4c-1.5-1-3.5-1.7-5.8-2.3
    		c-2.3-0.6-4.8-1.1-7.5-1.6c-2.7-0.5-5.2-1.2-7.5-2.2c-2.3-0.9-4.2-2.3-5.8-4c-1.6-1.8-2.3-4.2-2.3-7.3c0-4.5,1.6-8,4.9-10.5
    		c3.3-2.5,7.8-3.8,13.7-3.8c3.3,0,6,0.3,8.3,1c2.2,0.7,4.1,1.6,5.5,2.7c0.6,0.5,1.2,1,1.8,1.7c0.6,0.7,1.2,1.4,1.8,2.2
    		c0.6,0.8,1,1.5,1.4,2.3c0.4,0.7,0.6,1.4,0.6,2c0,1.3-0.5,2.4-1.6,3.1c-1,0.8-2.2,1.2-3.4,1.2c-0.7,0-1.5-0.1-2.1-0.6
    		c-0.6-0.6-1-1.3-1.3-1.8c-0.5-1.1-1.1-2-1.7-2.8c-0.5-0.8-1.2-1.5-2-2c-0.8-0.5-1.8-0.9-3-1.2c-1.2-0.3-2.7-0.4-4.4-0.4
    		c-3.5,0-6,0.6-7.6,1.7c-1.6,1.1-2.4,2.5-2.4,4.2c0,1.3,0.5,2.3,1.4,3.1c0.9,0.8,2.1,1.4,3.7,2c1.5,0.5,3.3,1,5.2,1.4
    		c2,0.4,3.9,0.8,5.9,1.3c2,0.5,4,1.1,5.9,1.7c1.9,0.7,3.7,1.6,5.2,2.7c1.5,1.1,2.7,2.5,3.7,4.1c0.9,1.6,1.4,3.6,1.4,6
    		C309.4,86.4,308.8,88.9,307.6,90.9z"/>
    	<path d="M327.4,48.8h4.7c1.1,0,2,0.4,2.8,1.2c0.8,0.8,1.2,1.7,1.2,2.8c0,1.1-0.4,2.1-1.2,2.8s-1.7,1.1-2.8,1.1h-4.7v37.9
    		c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-2,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V56.7h-4.6c-1.1,0-2-0.4-2.8-1.1
    		c-0.8-0.7-1.2-1.7-1.2-2.8c0-1.1,0.4-2,1.2-2.8c0.8-0.8,1.7-1.2,2.8-1.2h4.6V39c0-1.3,0.4-2.3,1.3-3.2s1.9-1.3,3.2-1.3
    		c1.3,0,2.3,0.4,3.2,1.3s1.3,1.9,1.3,3.2V48.8z"/>
    	<path d="M341.5,32.1c1-1,2.2-1.5,3.7-1.5c1.4,0,2.6,0.5,3.6,1.5c1,1,1.5,2.2,1.5,3.6c0,1.4-0.5,2.6-1.5,3.6c-1,1-2.2,1.5-3.6,1.5
    		c-1.4,0-2.7-0.5-3.7-1.5c-1-1-1.5-2.2-1.5-3.6C340,34.2,340.5,33,341.5,32.1z M349.6,94.6c0,1.3-0.4,2.3-1.3,3.2
    		c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V53.3c0-1.3,0.4-2.3,1.3-3.2c0.9-0.9,2-1.3,3.2-1.3
    		c1.3,0,2.3,0.4,3.2,1.3c0.9,0.9,1.3,2,1.3,3.2V94.6z"/>
    	<path d="M369.1,97.3c-2.9-1.2-5.4-3-7.5-5.2c-2.1-2.2-3.8-4.9-5-8.1c-1.2-3.1-1.8-6.6-1.8-10.4c0-3.8,0.6-7.3,1.8-10.5
    		c1.2-3.1,2.9-5.8,5-8.1c2.1-2.2,4.6-4,7.5-5.2c2.9-1.2,6-1.9,9.4-1.9c3.4,0,6.4,0.5,9,1.5c2.5,1,4.6,2.2,6.2,3.6
    		c1.6,1.4,3.2,2.9,4,4.4c0.5,0.9,0.9,1.5,0.9,2.6c0,1.3-0.5,2.3-1.4,3.1c-0.9,0.7-2,1.1-3.1,1.1c-0.8,0-1.6-0.2-2.2-0.6
    		c-0.7-0.4-1.2-1.3-1.8-2.1c-1-1.4-0.5-0.6-1.1-1.5c-0.7-0.9-1.5-1.6-2.5-2.3c-1-0.7-2.2-1.2-3.5-1.6c-1.4-0.4-2.8-0.6-4.4-0.6
    		c-1.8,0-3.5,0.3-5.3,1c-1.8,0.7-3.3,1.7-4.7,3.1c-1.4,1.4-2.5,3.3-3.4,5.6s-1.3,5.1-1.3,8.4c0,3.3,0.4,6.1,1.3,8.4s2,4.2,3.4,5.6
    		c1.4,1.4,3,2.5,4.7,3.1c1.8,0.7,3.5,1,5.3,1c1.6,0,3.1-0.2,4.4-0.6c1.3-0.4,2.5-0.9,3.5-1.6c1-0.7,1.8-1.5,2.5-2.3
    		c0.7-0.9,1.1-1.8,1.4-2.7c0.3-1,0.8-1.7,1.5-2.1c0.7-0.4,1.4-0.6,2.2-0.6c1.1,0,2.2,0.4,3.1,1.1c0.9,0.7,1.4,1.7,1.4,3
    		c0,0.6-0.3,1.7-0.9,3.1c-0.6,1.5-1.6,2.9-3.1,4.4c-1.5,1.5-3.5,2.8-6.2,4c-2.6,1.1-6,1.7-10,1.7C375.1,99.2,372,98.5,369.1,97.3z"
    		/>
    </g>
</svg>
						</div>
					</a>
				</h1>
			</div>
			<nav id="nav">
				<ul>
					
					<li>
						
							
						
						<a href="/products" id="nav_products"   >Products</a>
						
						<ul class="dropdown">
							
								<li><a href="/products/elasticsearch" id="nav_elasticsearch" >elasticsearch</a></li>								
							
								<li><a href="/found" id="nav_hosted-elasticsearch" >elasticsearch as a service</a></li>								
							
								<li><a href="/products/logstash" id="nav_logstash" >logstash</a></li>								
							
								<li><a href="/products/kibana" id="nav_kibana" >kibana</a></li>								
							
								<li><a href="/products/beats" id="nav_beats" >beats</a></li>								
							
								<li><a href="/products/watcher" id="nav_watcher" >watcher</a></li>								
							
								<li><a href="/products/shield" id="nav_shield" >shield</a></li>								
							
								<li><a href="/products/marvel" id="nav_marvel" >marvel</a></li>								
							
								<li><a href="/products/hadoop" id="nav_hadoop" >hadoop</a></li>								
							
								<li><a href="/downloads" id="nav_downloads" >downloads</a></li>								
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/subscriptions" id="nav_subscriptions"   >Subscriptions</a>
						
						<ul class="dropdown">
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/learn" id="nav_learn"   >Learn</a>
						
						<ul class="dropdown">
							
								<li><a href="/guide" id="nav_guide" >docs</a></li>								
							
								<li><a href="/videos" id="nav_videos" >videos</a></li>								
							
								<li><a href="/training" id="nav_training" >training</a></li>								
							
								<li><a href="/services" id="nav_services" >services</a></li>								
							
								<li><a href="/blog" id="nav_learn_blog" >blog</a></li>								
							
								<li><a href="/elasticon" id="nav_learn_elasticon" >elastic{ON}</a></li>								
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/community" id="nav_community"   >Community</a>
						
						<ul class="dropdown">
							
								<li><a href="/community/meetups" id="nav_meetups" >meetups</a></li>								
							
								<li><a href="https://discuss.elastic.co" id="nav_discuss" target="_blank">discuss</a></li>								
							
								<li><a href="/community/codeofconduct" id="nav_codeofconduct" >code of conduct</a></li>								
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/use-cases" id="nav_usecases"   >Use Cases</a>
						
						<ul class="dropdown">
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/blog" id="nav_blog"   >Blog</a>
						
						<ul class="dropdown">
							
								<li><a href="/blog/category/news" id="blog_news" >News</a></li>								
							
								<li><a href="/blog/category/engineering" id="blog_engineering" >Engineering</a></li>								
							
								<li><a href="/blog/category/user-stories" id="blog_user-stories" >User Stories</a></li>								
							
								<li><a href="/blog/category/releases" id="blog_releases" >Releases</a></li>								
							
								<li><a href="/blog/archive" id="blog_archive" >Archive</a></li>								
								
						</ul>							
					</li>
					
					<li>
						
							
						
						<a href="/about" id="nav_about"   >About</a>
						
						<ul class="dropdown">
							
								<li><a href="/about/leadership" id="nav_leadership" >leadership</a></li>								
							
								<li><a href="/about/board" id="nav_board" >board of directors</a></li>								
							
								<li><a href="/about/careers" id="nav_careers" >careers</a></li>								
							
								<li><a href="/about/partners" id="nav_partners" >partners</a></li>								
							
								<li><a href="/about/press" id="nav_press" >press</a></li>								
								
						</ul>							
					</li>
					
				</ul>
			</nav>			
			<!-- Searchbar Input box -->
			 <!-- <form method="get" action="/search" name="searchForm" class="header-search-form" id="searchMobileFrm">
			   <input type="text" id="search-header-autocomplete" name="q" class="form-control global-input" placeholder="elastic{search}">
			</form>
			<div class="mobile-auto-complete"></div> -->  
		</div>
		<div class="1u 6u(small) 6u(xsmall)">
			<div id="searchbar">
				<a href="#" class="button icon"></a>
			</div>
			<!-- <div class="nav-auto-complete"></div> -->
		</div>
	</div>
</section>
<div class="mobile-menu-wrapper">
	<div class="row 0%">
		<div class="8u 6u(small) 6u(xsmall)">
			<div id="elastic">
				<h1>
					<a href="/">
						<div id="elastic-logo">
						<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 viewBox="0 0 400 130" preserveAspectRatio="xMinYMin meet" enable-background="new 0 0 400 130" xml:space="preserve">
    <g>
    	<g>
    		<path fill="#FFFFFF" d="M122.5,67.8c0-10.3-6.4-19.2-15.9-22.7c0.4-2.2,0.6-4.3,0.6-6.6c0-19.1-15.5-34.6-34.6-34.6
    			c-11.1,0-21.5,5.3-28,14.4c-3.2-2.5-7.1-3.8-11.2-3.8c-10.1,0-18.4,8.2-18.4,18.4c0,2.2,0.4,4.4,1.1,6.4C6.5,42.6,0,51.8,0,62
    			c0,10.3,6.4,19.3,16,22.8c-0.4,2.1-0.6,4.3-0.6,6.6c0,19,15.5,34.5,34.5,34.5c11.2,0,21.5-5.4,28-14.4c3.2,2.5,7.2,3.9,11.3,3.9
    			c10.1,0,18.4-8.2,18.4-18.4c0-2.2-0.4-4.4-1.1-6.4C115.9,87.2,122.5,78,122.5,67.8z"/>
    		<g>
    			<path fill="#F4BD19" d="M47.9,56.3l27.3,12.5l27.6-24.2c0.4-2,0.6-4,0.6-6.1c0-17-13.8-30.8-30.8-30.8c-10.2,0-19.7,5-25.4,13.4
    				l-4.6,23.8L47.9,56.3z"/>
    		</g>
    		<g>
    			<path fill="#3CBEB1" d="M19.6,85.2c-0.4,2-0.6,4.1-0.6,6.2c0,17,13.9,30.9,30.9,30.9c10.3,0,19.8-5.1,25.6-13.5L80,85l-6.1-11.6
    				L46.5,60.9L19.6,85.2z"/>
    		</g>
    		<g>
    			<path fill="#E9478C" d="M19.4,37.9l18.7,4.4L42.3,21c-2.6-2-5.7-3-9-3c-8.1,0-14.8,6.6-14.8,14.8C18.5,34.5,18.8,36.3,19.4,37.9z
    				"/>
    		</g>
    		<g>
    			<path fill="#2C458F" d="M17.8,42.4C9.4,45.1,3.6,53.2,3.6,62c0,8.6,5.3,16.3,13.3,19.3l26.3-23.8l-4.8-10.3L17.8,42.4z"/>
    		</g>
    		<g>
    			<path fill="#95C63D" d="M80.3,108.7c2.6,2,5.7,3.1,8.9,3.1c8.1,0,14.8-6.6,14.8-14.8c0-1.8-0.3-3.5-0.9-5.1l-18.7-4.4L80.3,108.7
    				z"/>
    		</g>
    		<g>
    			<path fill="#176655" d="M84.1,82.6l20.6,4.8c8.4-2.8,14.2-10.8,14.2-19.6c0-8.6-5.3-16.2-13.3-19.3l-27,23.6L84.1,82.6z"/>
    		</g>
    	</g>
    	<path d="M162.8,76.3c0,0.1,0,0.6,0.1,1.4c0.1,0.8,0.3,1.8,0.6,3c0.3,1.1,0.8,2.4,1.4,3.7c0.6,1.3,1.4,2.5,2.5,3.6
    		c1,1.1,2.3,2,3.8,2.7c1.5,0.7,3.4,1.1,5.6,1.1c1.8,0,3.3-0.1,4.6-0.4c1.3-0.3,2.4-0.7,3.4-1.3c1-0.6,1.9-1.3,2.7-2.2
    		c0.8-0.9,1.6-1.9,2.4-3.1c0.5-0.7,1-1.2,1.6-1.5c0.6-0.3,1.2-0.4,1.8-0.4c1,0,2,0.4,2.8,1.1c0.9,0.7,1.3,1.7,1.3,2.8
    		c0,0.7-0.4,1.7-1.1,3.1c-0.7,1.4-1.9,2.8-3.6,4.1c-1.6,1.4-3.7,2.6-6.3,3.6c-2.6,1-5.7,1.5-9.5,1.5c-3.8,0-7.2-0.7-10.2-2.1
    		c-3-1.4-5.5-3.2-7.5-5.6c-2-2.3-3.6-5-4.7-8.1c-1.1-3.1-1.6-6.3-1.6-9.7c0-3.5,0.5-6.8,1.6-9.9c1.1-3.1,2.7-5.8,4.7-8.1
    		c2-2.3,4.5-4.1,7.4-5.5c2.9-1.3,6.1-2,9.6-2c3.3,0,6.3,0.5,9.2,1.5c2.8,1,5.3,2.4,7.4,4.3c2.1,1.9,3.7,4.1,5,6.8
    		c1.2,2.7,1.8,5.8,1.8,9.2c0,2.1-0.5,3.7-1.6,4.8c-1,1.1-2.2,1.6-3.6,1.6H162.8z M170.9,56.4c-1.5,0.6-2.7,1.4-3.7,2.4
    		c-1,0.9-1.8,2-2.4,3.1c-0.6,1.1-1,2.2-1.3,3.2c-0.3,1-0.5,1.8-0.5,2.5c-0.1,0.7-0.1,1.1-0.1,1.2h27.9c0-1.8-0.3-3.5-0.9-5.1
    		c-0.6-1.6-1.5-3-2.6-4.3c-1.2-1.2-2.7-2.2-4.5-3c-1.8-0.7-4-1.1-6.5-1.1C174.2,55.5,172.4,55.8,170.9,56.4z"/>
    	<path d="M214.3,94.6c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V35.1
    		c0-1.3,0.4-2.3,1.3-3.2c0.9-0.9,2-1.3,3.2-1.3c1.3,0,2.3,0.4,3.2,1.3c0.9,0.9,1.3,1.9,1.3,3.2V94.6z"/>
    	<path d="M229.8,63.5c-0.6,0.3-1.2,0.5-1.9,0.5c-1.2,0-2.2-0.4-3.2-1.1c-1-0.7-1.5-1.8-1.5-3.1c0-0.5,0.1-1.1,0.4-1.9
    		c0.5-1.2,1.2-2.4,2.1-3.6c0.9-1.2,2-2.2,3.5-3.1s3.3-1.7,5.4-2.2c2.2-0.5,4.8-0.8,7.9-0.8c5.9,0,10.5,1.2,14,3.5
    		c3.5,2.3,5.2,5.8,5.2,10.3v32.8c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2v-2
    		c-1.2,2-4.7,4.5-7.5,5.3c-2.8,0.8-5.7,1.2-8.8,1.2c-2,0-3.9-0.3-5.9-0.8c-2-0.5-3.8-1.4-5.3-2.5c-1.6-1.1-2.9-2.6-3.8-4.3
    		c-1-1.7-1.5-3.8-1.5-6.2c0-2.8,0.5-5.7,1.6-7.5c1.1-1.9,2.6-3.4,4.4-4.6c1.8-1.2,3.9-2.1,6.3-2.8c2.4-0.6,4.9-1.1,7.5-1.4
    		c3-0.3,5.4-0.7,7.2-1.2c1.7-0.4,3-0.9,3.9-1.5c0.9-0.6,1.4-1.2,1.6-2c0.2-0.7,0.3-1.6,0.3-2.5c0-1.1-0.3-2-0.9-2.8
    		c-0.6-0.8-1.4-1.5-2.4-2c-1-0.5-2.1-0.9-3.3-1.2s-2.4-0.4-3.5-0.4c-2.1,0-3.7,0.2-5,0.5c-1.3,0.3-2.3,0.8-3.1,1.4
    		c-0.8,0.6-1.5,1.3-1.9,2.1c-0.5,0.8-0.9,1.6-1.3,2.5C230.9,62.6,230.4,63.1,229.8,63.5z M250.6,74.4c-1.1,0.3-2.4,0.6-3.8,0.8
    		c-1.5,0.2-3,0.4-4.6,0.6c-1.6,0.2-3.1,0.5-4.6,0.9c-1.5,0.3-2.8,0.8-4,1.5c-1.2,0.7-2.2,1.5-2.9,2.5c-0.7,1-1.1,3-1.1,4.6
    		c0,1.3,0.3,2.3,0.8,3.1c0.5,0.8,1.2,1.5,2.1,2c0.9,0.5,1.8,0.8,2.9,1c1.1,0.2,2.2,0.3,3.3,0.3c1.7,0,3.3-0.2,5-0.6
    		c1.7-0.4,3.2-1,4.5-1.9s2.4-2,3.3-3.5c0.8-1.4,1.2-3.8,1.2-5.9v-6.1L250.6,74.4z"/>
    	<path d="M307.6,90.9c-1.1,2-2.7,3.6-4.6,4.8s-4.1,2.1-6.6,2.7c-2.5,0.5-5.2,0.8-7.9,0.8c-3.9,0-7.2-0.6-9.9-1.8
    		c-2.7-1.2-4.9-2.6-6.5-4.1c-1.7-1.6-2.9-3.1-3.7-4.7c-0.7-1.6-1.1-2.7-1.1-3.5c0-1.3,0.5-2.4,1.4-3.2c0.9-0.8,2-1.2,3.1-1.2
    		c0.7,0,1.3,0.2,2,0.6c0.6,0.4,1.1,1,1.5,1.9c1,2.5,2.5,4.5,4.7,6.2c2.2,1.7,5.2,2.5,9.3,2.5c1.8,0,3.4-0.2,4.7-0.6
    		c1.3-0.4,2.4-1,3.2-1.7c0.8-0.7,1.4-1.5,1.9-2.4c0.4-0.9,0.6-1.8,0.6-2.8c0-2-0.8-3.4-2.3-4.4c-1.5-1-3.5-1.7-5.8-2.3
    		c-2.3-0.6-4.8-1.1-7.5-1.6c-2.7-0.5-5.2-1.2-7.5-2.2c-2.3-0.9-4.2-2.3-5.8-4c-1.6-1.8-2.3-4.2-2.3-7.3c0-4.5,1.6-8,4.9-10.5
    		c3.3-2.5,7.8-3.8,13.7-3.8c3.3,0,6,0.3,8.3,1c2.2,0.7,4.1,1.6,5.5,2.7c0.6,0.5,1.2,1,1.8,1.7c0.6,0.7,1.2,1.4,1.8,2.2
    		c0.6,0.8,1,1.5,1.4,2.3c0.4,0.7,0.6,1.4,0.6,2c0,1.3-0.5,2.4-1.6,3.1c-1,0.8-2.2,1.2-3.4,1.2c-0.7,0-1.5-0.1-2.1-0.6
    		c-0.6-0.6-1-1.3-1.3-1.8c-0.5-1.1-1.1-2-1.7-2.8c-0.5-0.8-1.2-1.5-2-2c-0.8-0.5-1.8-0.9-3-1.2c-1.2-0.3-2.7-0.4-4.4-0.4
    		c-3.5,0-6,0.6-7.6,1.7c-1.6,1.1-2.4,2.5-2.4,4.2c0,1.3,0.5,2.3,1.4,3.1c0.9,0.8,2.1,1.4,3.7,2c1.5,0.5,3.3,1,5.2,1.4
    		c2,0.4,3.9,0.8,5.9,1.3c2,0.5,4,1.1,5.9,1.7c1.9,0.7,3.7,1.6,5.2,2.7c1.5,1.1,2.7,2.5,3.7,4.1c0.9,1.6,1.4,3.6,1.4,6
    		C309.4,86.4,308.8,88.9,307.6,90.9z"/>
    	<path d="M327.4,48.8h4.7c1.1,0,2,0.4,2.8,1.2c0.8,0.8,1.2,1.7,1.2,2.8c0,1.1-0.4,2.1-1.2,2.8s-1.7,1.1-2.8,1.1h-4.7v37.9
    		c0,1.3-0.4,2.3-1.3,3.2c-0.9,0.9-2,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V56.7h-4.6c-1.1,0-2-0.4-2.8-1.1
    		c-0.8-0.7-1.2-1.7-1.2-2.8c0-1.1,0.4-2,1.2-2.8c0.8-0.8,1.7-1.2,2.8-1.2h4.6V39c0-1.3,0.4-2.3,1.3-3.2s1.9-1.3,3.2-1.3
    		c1.3,0,2.3,0.4,3.2,1.3s1.3,1.9,1.3,3.2V48.8z"/>
    	<path d="M341.5,32.1c1-1,2.2-1.5,3.7-1.5c1.4,0,2.6,0.5,3.6,1.5c1,1,1.5,2.2,1.5,3.6c0,1.4-0.5,2.6-1.5,3.6c-1,1-2.2,1.5-3.6,1.5
    		c-1.4,0-2.7-0.5-3.7-1.5c-1-1-1.5-2.2-1.5-3.6C340,34.2,340.5,33,341.5,32.1z M349.6,94.6c0,1.3-0.4,2.3-1.3,3.2
    		c-0.9,0.9-1.9,1.3-3.2,1.3c-1.3,0-2.3-0.4-3.2-1.3c-0.9-0.9-1.3-2-1.3-3.2V53.3c0-1.3,0.4-2.3,1.3-3.2c0.9-0.9,2-1.3,3.2-1.3
    		c1.3,0,2.3,0.4,3.2,1.3c0.9,0.9,1.3,2,1.3,3.2V94.6z"/>
    	<path d="M369.1,97.3c-2.9-1.2-5.4-3-7.5-5.2c-2.1-2.2-3.8-4.9-5-8.1c-1.2-3.1-1.8-6.6-1.8-10.4c0-3.8,0.6-7.3,1.8-10.5
    		c1.2-3.1,2.9-5.8,5-8.1c2.1-2.2,4.6-4,7.5-5.2c2.9-1.2,6-1.9,9.4-1.9c3.4,0,6.4,0.5,9,1.5c2.5,1,4.6,2.2,6.2,3.6
    		c1.6,1.4,3.2,2.9,4,4.4c0.5,0.9,0.9,1.5,0.9,2.6c0,1.3-0.5,2.3-1.4,3.1c-0.9,0.7-2,1.1-3.1,1.1c-0.8,0-1.6-0.2-2.2-0.6
    		c-0.7-0.4-1.2-1.3-1.8-2.1c-1-1.4-0.5-0.6-1.1-1.5c-0.7-0.9-1.5-1.6-2.5-2.3c-1-0.7-2.2-1.2-3.5-1.6c-1.4-0.4-2.8-0.6-4.4-0.6
    		c-1.8,0-3.5,0.3-5.3,1c-1.8,0.7-3.3,1.7-4.7,3.1c-1.4,1.4-2.5,3.3-3.4,5.6s-1.3,5.1-1.3,8.4c0,3.3,0.4,6.1,1.3,8.4s2,4.2,3.4,5.6
    		c1.4,1.4,3,2.5,4.7,3.1c1.8,0.7,3.5,1,5.3,1c1.6,0,3.1-0.2,4.4-0.6c1.3-0.4,2.5-0.9,3.5-1.6c1-0.7,1.8-1.5,2.5-2.3
    		c0.7-0.9,1.1-1.8,1.4-2.7c0.3-1,0.8-1.7,1.5-2.1c0.7-0.4,1.4-0.6,2.2-0.6c1.1,0,2.2,0.4,3.1,1.1c0.9,0.7,1.4,1.7,1.4,3
    		c0,0.6-0.3,1.7-0.9,3.1c-0.6,1.5-1.6,2.9-3.1,4.4c-1.5,1.5-3.5,2.8-6.2,4c-2.6,1.1-6,1.7-10,1.7C375.1,99.2,372,98.5,369.1,97.3z"
    		/>
    </g>
</svg>
						</div>
					</a>
				</h1>
			</div>
		</div>
		<div class="4u 6u(small) 6u(xsmall) text-right">
			<!-- <div class="mobile-menu-wrapper"> -->
				<ul class="m-shortcuts">
					
					<li><a href="#" class="m-search" id="header-search"></a></li>
					
					<li><a href="/guide" class="m-docs" id="mobile-docs"></a></li>
					
					<li><a href="/contact" class="m-contact" id="mobile-contact"></a></li>
					
				</ul>	
				<nav class="nav-menu">
					<div class="menu-button">
						<span></span>
					</div>
					<div class="nav-menu-list-wrap">
						<div class="nav-menu-list">
							
								<h3><a href="/products" id="nav_products" >Products</a></h3>

								
								<ul>
									
									<li><a href="/products/elasticsearch" id="nav_elasticsearch" >elasticsearch</a></li>
									
									<li><a href="/found" id="nav_hosted-elasticsearch" >elasticsearch as a service</a></li>
									
									<li><a href="/products/logstash" id="nav_logstash" >logstash</a></li>
									
									<li><a href="/products/kibana" id="nav_kibana" >kibana</a></li>
									
									<li><a href="/products/beats" id="nav_beats" >beats</a></li>
									
									<li><a href="/products/watcher" id="nav_watcher" >watcher</a></li>
									
									<li><a href="/products/shield" id="nav_shield" >shield</a></li>
									
									<li><a href="/products/marvel" id="nav_marvel" >marvel</a></li>
									
									<li><a href="/products/hadoop" id="nav_hadoop" >hadoop</a></li>
									
									<li><a href="/downloads" id="nav_downloads" >downloads</a></li>
									
								</ul>
								
							
								<h3><a href="/subscriptions" id="nav_subscriptions" >Subscriptions</a></h3>

								
							
								<h3><a href="/learn" id="nav_learn" >Learn</a></h3>

								
								<ul>
									
									<li><a href="/guide" id="nav_guide" >docs</a></li>
									
									<li><a href="/videos" id="nav_videos" >videos</a></li>
									
									<li><a href="/training" id="nav_training" >training</a></li>
									
									<li><a href="/services" id="nav_services" >services</a></li>
									
									<li><a href="/blog" id="nav_learn_blog" >blog</a></li>
									
									<li><a href="/elasticon" id="nav_learn_elasticon" >elastic{ON}</a></li>
									
								</ul>
								
							
								<h3><a href="/community" id="nav_community" >Community</a></h3>

								
								<ul>
									
									<li><a href="/community/meetups" id="nav_meetups" >meetups</a></li>
									
									<li><a href="/https:/discuss.elastic.co" id="nav_discuss" target="_blank">discuss</a></li>
									
									<li><a href="/community/codeofconduct" id="nav_codeofconduct" >code of conduct</a></li>
									
								</ul>
								
							
								<h3><a href="/use-cases" id="nav_usecases" >Use Cases</a></h3>

								
							
								<h3><a href="/blog" id="nav_blog" >Blog</a></h3>

								
								<ul>
									
									<li><a href="/blog/category/news" id="blog_news" >News</a></li>
									
									<li><a href="/blog/category/engineering" id="blog_engineering" >Engineering</a></li>
									
									<li><a href="/blog/category/user-stories" id="blog_user-stories" >User Stories</a></li>
									
									<li><a href="/blog/category/releases" id="blog_releases" >Releases</a></li>
									
									<li><a href="/blog/archive" id="blog_archive" >Archive</a></li>
									
								</ul>
								
							
								<h3><a href="/about" id="nav_about" >About</a></h3>

								
								<ul>
									
									<li><a href="/about/leadership" id="nav_leadership" >leadership</a></li>
									
									<li><a href="/about/board" id="nav_board" >board of directors</a></li>
									
									<li><a href="/about/careers" id="nav_careers" >careers</a></li>
									
									<li><a href="/about/partners" id="nav_partners" >partners</a></li>
									
									<li><a href="/about/press" id="nav_press" >press</a></li>
									
								</ul>
								
							
						</div>
					</div>
				</nav>
			<!-- </div> -->
		</div>
	</div>
</div>
<!-- Mobile Menu -->


		</div>
	</div>
</div>
<div class="header-search-wrapper">	
	<div class="container">
		<div class="big-search">
			<i class="big-search-icon"></i>
			<form method="get" action="/search" autocomplete="on" name="searchForm" id="searchfrm">
				<ul class="tags-wrapper">
					<li class="search-field"><input type="text" class="form-control global-input" id="autocomplete" name="q"></li>
				</ul>
			</form>
			<a href="#" class="header-search-cancel"></a>
		</div>
		<div class="nav-auto-complete"></div>
	</div>
</div>
<style>	
	
	
		.m-shortcuts li a.m-search {
			background: url("https://static-www.elastic.co/assets/blt174b13be89ff803c/m-search-icon.png?q=100") no-repeat scroll center center rgba(0, 0, 0, 0);
			background-size: contain;}
	
		.m-shortcuts li a.m-docs {
			background: url("https://static-www.elastic.co/assets/blt5ba03f594a7d610a/m-guide-icon.png?q=100") no-repeat scroll center center rgba(0, 0, 0, 0);
			background-size: contain;}
	
		.m-shortcuts li a.m-contact {
			background: url("https://static-www.elastic.co/assets/blt38c3e853c4b1174d/m-contact-icon.png?q=100") no-repeat scroll center center rgba(0, 0, 0, 0);
			background-size: contain;}
		
</style>
		</div>
	</div>
  	<div id="content">
  		
  			
            <div id="pageheader">
    <div class="container">
        <header>
            <h1><a href="/learn" title="Learn">Learn</a> |</h1>
            <h2><a href="/guide" title="Docs">Docs</a></h2>
        </header>
    </div>
</div>
<section id="guide">

            <!-- start body -->
<div class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elasticsearch: The Definitive Guide</a></span> » <span class="breadcrumb-link"><a href="modeling-your-data.html">Modeling Your Data </a></span> » <span class="breadcrumb-node">Designing for Scale</span></div><div class="navheader"><span class="prev"><a href="parent-child.html">
              « 
              Parent-Child Relationship</a>
           
        </span><span class="next">
           
          <a href="administration.html">관리, 모니터링, 배포 
               »
            </a></span></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="scale"></a>Designing for Scale</h2></div></div></div><p>Elasticsearch is used by some companies to index <a id="id-1.9.6.2.1" class="indexterm"></a>
<a id="id-1.9.6.2.2" class="indexterm"></a>and search petabytes of data
every day, but most of us start out with something a little more humble in
size. Even if we aspire to be the next Facebook, it is unlikely that our bank
balance matches our aspirations.  We need to build for what we have today, but
in a way that will allow us to scale out flexibly and rapidly.</p><p>Elasticsearch is built to scale.  It will run very happily on your laptop or
in a cluster containing hundreds of nodes, and the experience is almost
identical. Growing from a small cluster to a large cluster is almost entirely
automatic and painless. Growing from a large cluster to a very large cluster
requires a bit more planning and design, but it is still relatively painless.</p><p>Of course, it is not magic.  Elasticsearch has its limitations too.  If you
are aware of those limitations and work with them, the growing process will be
pleasant.  If you treat Elasticsearch badly, you could be in for a world of
pain.</p><p>The default settings in Elasticsearch will take you a long way, but to get the
most bang for your buck, you need to think about how data flows through your
system.  We will talk about two common data flows: time-based data (such as log
events or social network streams, where relevance is driven by recency), and
user-based data (where a large document collection can be subdivided by user or
customer).</p><p>This chapter will help you make the right decisions up front, to avoid
nasty surprises later.</p><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="shard-scale"></a>The Unit of Scale</h2></div></div></div><p>In <a class="xref" href="inside-a-shard.html#dynamic-indices" title="Dynamically Updatable Indices">Dynamically Updatable Indices</a>, we explained that a shard is a <span class="emphasis"><em>Lucene index</em></span> and that
an Elasticsearch index is a collection of shards.<a id="id-1.9.6.7.2.3" class="indexterm"></a>
<a id="id-1.9.6.7.2.4" class="indexterm"></a> Your application talks to an
index, and Elasticsearch routes your requests to the appropriate shards.</p><p>A shard is the <span class="emphasis"><em>unit of scale</em></span>. <a id="id-1.9.6.7.3.2" class="indexterm"></a>
<a id="id-1.9.6.7.3.3" class="indexterm"></a> The smallest index you can have is one with a
single shard. This may be more than sufficient for your needs—a single
shard can hold a lot of data—but it limits your ability to scale.</p><p>Imagine that our cluster consists of one node, and in our cluster we have one
index, which has only one shard:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /my_index
{
  "settings": {
    "number_of_shards":   1, <a id="CO292-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
    "number_of_replicas": 0
  }
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO292-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create an index with one primary shard and zero replica shards.
</p></td></tr></table></div><p>This setup may be small, but it serves our current needs and is cheap to run.</p><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>At the moment we are talking about only <span class="emphasis"><em>primary</em></span> shards.<a id="id-1.9.6.7.8.1.2" class="indexterm"></a>  We discuss
<span class="emphasis"><em>replica</em></span> shards in <a class="xref" href="scale.html#replica-shards" title="Replica Shards">Replica Shards</a>.</p></div></div><p>One glorious day, the Internet discovers us, and a single node just can’t keep up with
the traffic.  We decide to add a second node, as per <a class="xref" href="scale.html#img-one-shard" title="Figure 49. An index with one shard has no scale factor">Figure 49, “An index with one shard has no scale factor”</a>. What happens?</p><div class="figure"><a id="img-one-shard"></a><p class="title"><strong>Figure 49. An index with one shard has no scale factor</strong></p><div class="figure-contents"><div class="mediaobject"><img src="images/elas_4401.png" alt="An index with one shard has no scale factor" /></div></div></div><br class="figure-break" /><p>The answer is: nothing.  Because we have only one shard, there is nothing to
put on the second node. We can’t increase the number of shards in the index,
because the number of shards is an important element in the algorithm used to
<a class="link" href="distributed-docs.html#routing-value" title="Routing a Document to a Shard">route documents to shards</a>:</p><pre class="literallayout">shard = hash(routing) % number_of_primary_shards</pre><p>Our only option now is to reindex our data into a new, bigger index that has
more shards, but that will take time that we can ill afford.  By planning
ahead, we could have avoided this problem completely by <span class="emphasis"><em>overallocating</em></span>.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="overallocation"></a>Shard Overallocation</h2></div></div></div><p>A shard lives on a single node,<a id="id-1.9.6.8.2.1" class="indexterm"></a>
<a id="id-1.9.6.8.2.2" class="indexterm"></a><a id="id-1.9.6.8.2.3" class="indexterm"></a>
<a id="id-1.9.6.8.2.4" class="indexterm"></a> but a node can hold multiple shards. Imagine
that we created our index with two primary shards instead of one:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /my_index
{
  "settings": {
    "number_of_shards":   2, <a id="CO293-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
    "number_of_replicas": 0
  }
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO293-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create an index with two primary shards and zero replica shards.
</p></td></tr></table></div><p>With a single node, both shards would be assigned to the same node. From the
point of view of our application, everything functions as it did before.  The
application communicates with the index, not the shards, and there is still
only one index.</p><p>This time, when we add a second node, Elasticsearch will automatically move
one shard from the first node to the second node, as depicted in <a class="xref" href="scale.html#img-two-shard" title="Figure 50. An index with two shards can take advantage of a second node">Figure 50, “An index with two shards can take advantage of a second node”</a>. Once the relocation has
finished, each shard will have access to twice the computing power that it had
before.</p><div class="figure"><a id="img-two-shard"></a><p class="title"><strong>Figure 50. An index with two shards can take advantage of a second node</strong></p><div class="figure-contents"><div class="mediaobject"><img src="images/elas_4402.png" alt="An index with two shards can take advantage of a second node" /></div></div></div><br class="figure-break" /><p>We have been able to double our capacity by simply copying a shard across the
network to the new node. The best part is, we achieved this with zero
downtime.  All indexing and search requests continued to function normally
while the shard was being moved.</p><p>A new index in Elasticsearch is allotted five primary shards by default.  That
means that we can spread that index out over a maximum of five nodes, with one
shard on each node.  That’s a lot of capacity, and it happens without you
having to think about it at all!</p><div class="sidebar"><div class="titlepage"><div><div><p class="title"><strong>Shard Splitting</strong></p></div></div></div><p>Users often ask why Elasticsearch doesn’t support <span class="emphasis"><em>shard-splitting</em></span>—the
ability to split each shard into two or more pieces. <a id="id-1.9.6.8.10.2.2" class="indexterm"></a> The reason is that
shard-splitting is a bad idea:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Splitting a shard is almost equivalent to reindexing your data. It’s a much
  heavier process than just copying a shard from one node to another.
</li><li class="listitem">
Splitting is exponential. You start with one shard, then split into two, and then
  four, eight, sixteen, and so on. Splitting doesn’t allow you to increase capacity
  by just 50%.
</li><li class="listitem">
Shard splitting requires you to have enough capacity to hold a second copy
  of your index. Usually, by the time you realize that you need to scale out,
  you don’t have enough free space left to perform the split.
</li></ul></div><p>In a way, Elasticsearch does support shard splitting.  You can always reindex
your data to a new index with the appropriate number of shards (see
<a class="xref" href="index-management.html#reindex" title="Reindexing Your Data">Reindexing Your Data</a>).  It is still a more intensive process than moving shards around,
and still requires enough free space to complete, but at least you can control
the number of shards in the new index.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="kagillion-shards"></a>Kagillion Shards</h2></div></div></div><p>The first thing that new users do when they learn about
<a class="link" href="scale.html#overallocation" title="Shard Overallocation">shard overallocation</a> is<a id="id-1.9.6.9.2.2" class="indexterm"></a>
<a id="id-1.9.6.9.2.3" class="indexterm"></a>
<a id="id-1.9.6.9.2.4" class="indexterm"></a><a id="id-1.9.6.9.2.5" class="indexterm"></a>
<a id="id-1.9.6.9.2.6" class="indexterm"></a>
<a id="id-1.9.6.9.2.7" class="indexterm"></a> to say to themselves:</p><div class="blockquote"><table border="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top"> </td><td width="80%" valign="top"><p class="alignmeright">I don’t know how big this is going to be, and I can’t change the index size
later on, so to be on the safe side, I’ll just give this index 1,000 shards…</p></td><td width="10%" valign="top"> </td></tr><tr><td width="10%" valign="top"> </td><td colspan="2" align="right" valign="top">--<span class="attribution">
A new user
</span></td></tr></table></div><p>One thousand shards—really? And you don’t think that, perhaps, between now
and the time you need to buy <span class="emphasis"><em>one thousand nodes</em></span>, that you may need to
rethink your data model once or twice and have to reindex?</p><p>A shard is not free.  Remember:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
A shard is a Lucene index under the covers, which uses file handles,
    memory, and CPU cycles.
</li><li class="listitem">
Every search request needs to hit a copy of every shard in the index.
    That’s fine if every shard is sitting on a different node, but not if many
    shards have to compete for the same resources.
</li><li class="listitem">
Term statistics, used to calculate relevance, are per shard.  Having a small
    amount of data in many shards leads to poor relevance.
</li></ul></div><div class="tip admon"><div class="icon"><img alt="Tip" src="images/icons/tip.png" /></div><div class="admon_content"><p>A little overallocation is good. A kagillion shards is bad. It is difficult to
define what constitutes too many shards, as it depends on their size and how
they are being used. A hundred shards that are seldom used may be fine, while
two shards experiencing very heavy usage could be too many. Monitor your nodes
to ensure that they have enough spare capacity to deal with exceptional
conditions.</p></div></div><p>Scaling out should be done in phases.  Build in enough capacity to get to the
next phase. Once you get to the next phase, you have time to think about the
changes you need to make to reach the phase after that.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="capacity-planning"></a>Capacity Planning</h2></div></div></div><p>If 1 shard is too few and 1,000 shards are too many, how do I know how many
shards I need?<a id="id-1.9.6.10.2.1" class="indexterm"></a>
<a id="id-1.9.6.10.2.2" class="indexterm"></a><a id="id-1.9.6.10.2.3" class="indexterm"></a><a id="id-1.9.6.10.2.4" class="indexterm"></a>
<a id="id-1.9.6.10.2.5" class="indexterm"></a> This is a question that is impossible to answer in the general case. There are
just too many variables:  the hardware that you use, the size and complexity
of your documents, how you index and analyze those documents, the types of
queries that you run, the aggregations that you perform, how you model your
data, and more.</p><p>Fortunately, it is an easy question to answer in the specific case—yours:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
Create a cluster consisting of a single server, with the hardware that you
    are considering using in production.
</li><li class="listitem">
Create an index with the same settings and analyzers that you plan to use
    in production, but with only one primary shard and no replicas.
</li><li class="listitem">
Fill it with real documents (or as close to real as you can get).
</li><li class="listitem">
Run real queries and aggregations (or as close to real as you can get).
</li></ol></div><p>Essentially, you want to replicate real-world usage and to push this single
shard until it “breaks.”  Even the definition of <span class="emphasis"><em>breaks</em></span> depends on you:
some users require that all responses return within 50ms; others are quite
happy to wait for 5 seconds.</p><p>Once you define the capacity of a single shard, it is easy to extrapolate that
number to your whole index.  Take the total amount of data that you need to
index, plus some extra for future growth, and divide by the capacity of a
single shard.  The result is the number of primary shards that you will need.</p><div class="tip admon"><div class="icon"><img alt="Tip" src="images/icons/tip.png" /></div><div class="admon_content"><p>Capacity planning should not be your first step.</p><p>First look for ways to optimize how you are using Elasticsearch.  Perhaps you
have inefficient queries, not enough RAM, or you have left swap enabled?</p><p>We have seen new users who, frustrated by initial performance, immediately
start trying to tune the garbage collector or adjust the number of threads,
instead of tackling the simple problems like removing wildcard queries.</p></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="replica-shards"></a>Replica Shards</h2></div></div></div><p>Up until now we have spoken only about primary shards, but we have another
tool in our belt: replica shards.<a id="id-1.9.6.11.2.1" class="indexterm"></a>
<a id="id-1.9.6.11.2.2" class="indexterm"></a><a id="id-1.9.6.11.2.3" class="indexterm"></a>
<a id="id-1.9.6.11.2.4" class="indexterm"></a><a id="id-1.9.6.11.2.5" class="indexterm"></a>  The main purpose of replicas is for
failover, as discussed in <a class="xref" href="distributed-cluster.html" title="Cluster 내부 동작"><em>Cluster 내부 동작</em></a>: if the node holding a
primary shard dies, a replica is promoted to the role of primary.</p><p>At index time, a replica shard does the same amount of work as the primary
shard.  New documents are first indexed on the primary and then on any
replicas.  Increasing the number of replicas does not change the capacity of
the index.</p><p>However, replica shards can serve read requests.  If, as is often the case,
your index is search heavy, you can increase search performance by increasing
the number of replicas, but only if you also <span class="emphasis"><em>add extra hardware</em></span>.</p><p>Let’s return to our example of an index with two primary shards.  We increased
capacity of the index by adding a second node. Adding more nodes would not
help us to add indexing capacity, but we could take advantage of the extra
hardware at search time by increasing the number of replicas:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /my_index/_settings
{
  "number_of_replicas": 1
}</pre></div><p>Having two primary shards, plus a replica of each primary, would give us a
total of four shards: one for each node, as shown in <a class="xref" href="scale.html#img-four-nodes" title="Figure 51. An index with two primary shards and one replica can scale out across four nodes">Figure 51, “An index with two primary shards and one replica can scale out across four nodes”</a>.</p><div class="figure"><a id="img-four-nodes"></a><p class="title"><strong>Figure 51. An index with two primary shards and one replica can scale out across four nodes</strong></p><div class="figure-contents"><div class="mediaobject"><img src="images/elas_4403.png" alt="An index with two primary shards and one replica can scale out across four nodes" /></div></div></div><br class="figure-break" /><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_balancing_load_with_replicas"></a>Balancing Load with Replicas</h3></div></div></div><p>Search performance depends on the response times of the slowest node, so it is a good idea to try to balance out the load across all nodes.<a id="id-1.9.6.11.9.2.1" class="indexterm"></a>
<a id="id-1.9.6.11.9.2.2" class="indexterm"></a><a id="id-1.9.6.11.9.2.3" class="indexterm"></a> If we
added just one extra node instead of two, we would end up with two nodes having one shard each, and one node doing double the work with two shards.</p><p>We can even things out by adjusting the number of replicas.  By allocating two
replicas instead of one, we end up with a total of six shards, which can be
evenly divided between three nodes, as shown in <a class="xref" href="scale.html#img-three-nodes" title="Figure 52. Adjust the number of replicas to balance the load between nodes">Figure 52, “Adjust the number of replicas to balance the load between nodes”</a>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /my_index/_settings
{
  "number_of_replicas": 2
}</pre></div><p>As a bonus, we have also increased our availability.  We can now afford to
lose two nodes and still have a copy of all our data.</p><div class="figure"><a id="img-three-nodes"></a><p class="title"><strong>Figure 52. Adjust the number of replicas to balance the load between nodes</strong></p><div class="figure-contents"><div class="mediaobject"><img src="images/elas_4404.png" alt="Adjust the number of replicas to balance the load between nodes" /></div></div></div><br class="figure-break" /><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>The fact that node 3 holds two replicas and no primaries is not
important.  Replicas and primaries do the same amount of work; they just play
slightly different roles.  There is no need to ensure that primaries are
distributed evenly across all nodes.</p></div></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="multiple-indices"></a>Multiple Indices</h2></div></div></div><p>Finally, remember that there is no rule that limits your application to using
only a single index.<a id="id-1.9.6.12.2.1" class="indexterm"></a>
<a id="id-1.9.6.12.2.2" class="indexterm"></a><a id="id-1.9.6.12.2.3" class="indexterm"></a>
<a id="id-1.9.6.12.2.4" class="indexterm"></a>  When we issue a search request, it is forwarded to a
copy (a primary or a replica) of all the shards in an index.  If we issue the
same search request on multiple indices, the exact same thing happens—there
are just more shards involved.</p><div class="tip admon"><div class="icon"><img alt="Tip" src="images/icons/tip.png" /></div><div class="admon_content"><p>Searching 1 index of 50 shards is exactly equivalent to searching
50 indices with 1 shard each: both search requests hit 50 shards.</p></div></div><p>This can be a useful fact to remember when you need to add capacity on the
fly.  Instead of having to reindex your data into a bigger index, you can
just do the following:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Create a new index to hold new data.
</li><li class="listitem">
Search across both indices to retrieve new and old data.
</li></ul></div><p>In fact, with a little forethought, adding a new index can be done in a
completely transparent way, without your application ever knowing that
anything has changed.</p><p>In <a class="xref" href="index-management.html#index-aliases" title="Index Aliases and Zero Downtime">Index Aliases and Zero Downtime</a>, we spoke about using an index alias to point to the
current version of your index. <a id="id-1.9.6.12.7.2" class="indexterm"></a><a id="id-1.9.6.12.7.3" class="indexterm"></a> For instance, instead of naming your index
<code class="literal">tweets</code>, name it <code class="literal">tweets_v1</code>.  Your application would still talk to <code class="literal">tweets</code>,
but in reality that would be an alias that points to <code class="literal">tweets_v1</code>. This allows
you to switch the alias to point to a newer version of the index on the fly.</p><p>A similar technique can be used to expand capacity by adding a new index.  It
requires a bit of planning because you will need two aliases: one for
searching and one for indexing:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /tweets_1/_alias/tweets_search <a id="CO294-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
PUT /tweets_1/_alias/tweets_index <a id="CO294-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO294-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> <a href="#CO294-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Both the <code class="literal">tweets_search</code> and the <code class="literal">tweets_index</code> alias point to
    index <code class="literal">tweets_1</code>.
</p></td></tr></table></div><p>New documents should be indexed into <code class="literal">tweets_index</code>,  and searches should be
performed against <code class="literal">tweets_search</code>.  For the moment, these two aliases point to
the same index.</p><p>When we need extra capacity, we can create a new index called <code class="literal">tweets_2</code> and
update the aliases as follows:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">POST /_aliases
{
  "actions": [
    { "add":    { "index": "tweets_2", "alias": "tweets_search" }}, <a id="CO295-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
    { "remove": { "index": "tweets_1", "alias": "tweets_index"  }}, <a id="CO295-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
    { "add":    { "index": "tweets_2", "alias": "tweets_index"  }}  <a id="CO295-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
  ]
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO295-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Add index <code class="literal">tweets_2</code> to the <code class="literal">tweets_search</code> alias.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO295-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> <a href="#CO295-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Switch <code class="literal">tweets_index</code> from <code class="literal">tweets_1</code> to <code class="literal">tweets_2</code>.
</p></td></tr></table></div><p>A search request can target multiple indices, so having the search alias point
to <code class="literal">tweets_1</code> and <code class="literal">tweets_2</code> is perfectly valid.  However, indexing requests can
target only a single index. For this reason, we have to switch the index alias
to point to only the new index.</p><div class="tip admon"><div class="icon"><img alt="Tip" src="images/icons/tip.png" /></div><div class="admon_content"><p>A document <code class="literal">GET</code> request, like<a id="id-1.9.6.12.16.1.2" class="indexterm"></a>
<a id="id-1.9.6.12.16.1.3" class="indexterm"></a><a id="id-1.9.6.12.16.1.4" class="indexterm"></a> an indexing request, can target only one index.
This makes retrieving a document by ID a bit more complicated in this
scenario.  Instead, run a search request with the
<a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-ids-query.html" target="_top"><code class="literal">ids</code> query</a>, or do a<a id="id-1.9.6.12.16.1.6" class="indexterm"></a>
<a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html" target="_top"><code class="literal">multi-get</code></a> request on <code class="literal">tweets_1</code> and <code class="literal">tweets_2</code>.</p></div></div><p>Using multiple indices to expand index capacity on the fly is of particular
benefit when dealing with time-based data such as logs or social-event
streams, which we discuss in the next section.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="time-based"></a>Time-Based Data</h2></div></div></div><p>One of the most common use cases for Elasticsearch is for logging,<a id="id-1.9.6.13.2.1" class="indexterm"></a>
<a id="id-1.9.6.13.2.2" class="indexterm"></a><a id="id-1.9.6.13.2.3" class="indexterm"></a><a id="id-1.9.6.13.2.4" class="indexterm"></a>
<a id="id-1.9.6.13.2.5" class="indexterm"></a> so common
in fact that Elasticsearch provides an integrated<a id="id-1.9.6.13.2.6" class="indexterm"></a> logging platform called the
<span class="emphasis"><em>ELK stack</em></span>—Elasticsearch, Logstash, and Kibana—to make the process easy.</p><p><a class="ulink" href="https://www.elastic.co/guide/en/logstash/current/index.html" target="_top">Logstash</a> collects, parses, and
enriches logs before indexing them into Elasticsearch.<a id="id-1.9.6.13.3.2" class="indexterm"></a>  Elasticsearch acts as
a centralized logging server, and
<a class="ulink" href="https://www.elastic.co/guide/en/kibana/current/index.html" target="_top">Kibana</a> is a<a id="id-1.9.6.13.3.4" class="indexterm"></a> graphic frontend
that makes it easy to query and visualize what is happening across your
network in near real-time.</p><p>Most traditional use cases for search engines involve a relatively static
collection of documents that grows slowly. Searches look for the most relevant
documents, regardless of when they were created.</p><p>Logging—and other time-based data streams such as social-network activity—are very different in nature. <a id="id-1.9.6.13.5.1" class="indexterm"></a> The number of documents in the index grows
rapidly, often accelerating with time.  Documents are almost never updated,
and searches mostly target the most recent documents.  As documents age, they
lose value.</p><p>We need to adapt our index design to function with the flow of time-based
data.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="index-per-timeframe"></a>Index per Time Frame</h3></div></div></div><p>If we were to have one big index for documents of this type, we would soon run
out of space. Logging events just keep on coming, without pause or
interruption. We could delete the old events, with a <code class="literal">delete-by-query</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">DELETE /logs/event/_query
{
  "query": {
    "range": {
      "@timestamp": { <a id="CO296-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
        "lt": "now-90d"
      }
    }
  }
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO296-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Deletes all documents where Logstash’s <code class="literal">@timestamp</code> field is
    older than 90 days.
</p></td></tr></table></div><p>But this approach is <span class="emphasis"><em>very inefficient</em></span>.  Remember that when you delete a
document, it is only <span class="emphasis"><em>marked</em></span> as deleted (see <a class="xref" href="inside-a-shard.html#deletes-and-updates" title="Deletes and Updates">Deletes and Updates</a>). It won’t
be physically deleted until the segment containing it is merged away.</p><p>Instead, use an <span class="emphasis"><em>index per time frame</em></span>. <a id="id-1.9.6.13.7.6.2" class="indexterm"></a>
<a id="id-1.9.6.13.7.6.3" class="indexterm"></a>You could start out with an index per
year (<code class="literal">logs_2014</code>) or per month (<code class="literal">logs_2014-10</code>).  Perhaps, when your
website gets really busy, you need to switch to an index per day
(<code class="literal">logs_2014-10-24</code>).  Purging old data is easy: just delete old indices.</p><p>This approach has the advantage of allowing you to scale as and when you need
to.  You don’t have to make any difficult decisions up front.  Every day is a
new opportunity to change your indexing time frames to suit the current demand.
Apply the same logic to how big you make each index.  Perhaps all you need is
one primary shard per week initially.  Later, maybe you need five primary shards
per day.  It doesn’t matter—you can adjust to new circumstances at any
time.</p><p>Aliases can help make switching indices more transparent.<a id="id-1.9.6.13.7.8.1" class="indexterm"></a>  For indexing,
you can point <code class="literal">logs_current</code> to the index currently accepting new log events,
and for searching, update <code class="literal">last_3_months</code> to point to all indices for the
previous three months:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">POST /_aliases
{
  "actions": [
    { "add":    { "alias": "logs_current",  "index": "logs_2014-10" }}, <a id="CO297-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
    { "remove": { "alias": "logs_current",  "index": "logs_2014-09" }}, <a id="CO297-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
    { "add":    { "alias": "last_3_months", "index": "logs_2014-10" }}, <a id="CO297-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
    { "remove": { "alias": "last_3_months", "index": "logs_2014-07" }}  <a id="CO297-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
  ]
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO297-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> <a href="#CO297-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Switch <code class="literal">logs_current</code> from September to October.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO297-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> <a href="#CO297-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Add October to <code class="literal">last_3_months</code> and remove July.
</p></td></tr></table></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="index-templates"></a>Index Templates</h2></div></div></div><p>Elasticsearch doesn’t require you to create an index before using it.<a id="id-1.9.6.14.2.1" class="indexterm"></a>
<a id="id-1.9.6.14.2.2" class="indexterm"></a><a id="id-1.9.6.14.2.3" class="indexterm"></a>
<a id="id-1.9.6.14.2.4" class="indexterm"></a><a id="id-1.9.6.14.2.5" class="indexterm"></a>
<a id="id-1.9.6.14.2.6" class="indexterm"></a>  With
logging, it is often more convenient to rely on index autocreation than to
have to create indices manually.</p><p>Logstash uses the timestamp<a id="id-1.9.6.14.3.1" class="indexterm"></a><a id="id-1.9.6.14.3.2" class="indexterm"></a> from an event to derive the index name.  By
default, it indexes into a different index every day, so an event with a
<code class="literal">@timestamp</code> of <code class="literal">2014-10-01 00:00:01</code> will be sent to the index
<code class="literal">logstash-2014.10.01</code>.  If that index doesn’t already exist, it will be
created for us.</p><p>Usually we want some control over the settings and mappings of the new index.
Perhaps we want to limit the number of shards to <code class="literal">1</code>, and we want to disable the
<code class="literal">_all</code> field.  Index templates can be used to control which settings should be
applied to newly created indices:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /_template/my_logs <a id="CO298-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
{
  "template": "logstash-*", <a id="CO298-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
  "order":    1, <a id="CO298-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
  "settings": {
    "number_of_shards": 1 <a id="CO298-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
  },
  "mappings": {
    "_default_": { <a id="CO298-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>
      "_all": {
        "enabled": false
      }
    }
  },
  "aliases": {
    "last_3_months": {} <a id="CO298-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span>
  }
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO298-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a template called <code class="literal">my_logs</code>.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO298-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Apply this template to all indices beginning with <code class="literal">logstash-</code>.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO298-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
This template should override the default <code class="literal">logstash</code> template that has
    a lower <code class="literal">order</code>.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO298-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Limit the number of primary shards to <code class="literal">1</code>.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO298-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Disable the <code class="literal">_all</code> field for all types.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO298-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Add this index to the <code class="literal">last_3_months</code> alias.
</p></td></tr></table></div><p>This template specifies the default settings that will be applied to any index
whose name begins with <code class="literal">logstash-</code>, whether it is created manually or
automatically. If we think the index for tomorrow will need more capacity than
today, we can update the index to use a higher number of shards.</p><p>The template even adds the newly created index into the <code class="literal">last_3_months</code> alias, although
removing the old indices from that alias will have to be done manually.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="retiring-data"></a>Retiring Data</h2></div></div></div><p>As time-based data ages, it becomes less relevant.<a id="id-1.9.6.15.2.1" class="indexterm"></a>
<a id="id-1.9.6.15.2.2" class="indexterm"></a>  It’s possible that we
will want to see what happened last week, last month, or even last year, but
for the most part, we’re interested in only the here and now.</p><p>The nice thing about an index per time frame <a id="id-1.9.6.15.3.1" class="indexterm"></a>
<a id="id-1.9.6.15.3.2" class="indexterm"></a>
<a id="id-1.9.6.15.3.3" class="indexterm"></a><a id="id-1.9.6.15.3.4" class="indexterm"></a>
<a id="id-1.9.6.15.3.5" class="indexterm"></a>is that it enables us to easily
delete old data: just delete the indices that are no longer relevant:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">DELETE /logs_2013*</pre></div><p>Deleting a whole index is much more efficient than deleting individual
documents: Elasticsearch just removes whole directories.</p><p>But deleting an index is very <span class="emphasis"><em>final</em></span>.  There are a number of things we can
do to help data age gracefully, before we decide to delete it completely.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="migrate-indices"></a>Migrate Old Indices</h3></div></div></div><p>With logging data, there is likely to be one <span class="emphasis"><em>hot</em></span> index—the index for
today.<a id="id-1.9.6.15.7.2.2" class="indexterm"></a>
<a id="id-1.9.6.15.7.2.3" class="indexterm"></a>  All new documents will be added to that index, and almost all queries
will target that index.  It should use your best hardware.</p><p>How does Elasticsearch know which servers are your best servers? You tell it,
by assigning arbitrary tags to each server.  For instance, you could start a
node as follows:</p><pre class="literallayout">./bin/elasticsearch --node.box_type strong</pre><p>The <code class="literal">box_type</code> parameter is completely arbitrary—you could have named it
whatever you like—but you can use these arbitrary values to tell
Elasticsearch where to allocate an index.</p><p>We can ensure that today’s index is on our strongest boxes by creating it with
the following settings:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /logs_2014-10-01
{
  "settings": {
    "index.routing.allocation.include.box_type" : "strong"
  }
}</pre></div><p>Yesterday’s index no longer needs to be on our strongest boxes, so we can move
it to the nodes tagged as <code class="literal">medium</code> by updating its index settings:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">POST /logs_2014-09-30/_settings
{
  "index.routing.allocation.include.box_type" : "medium"
}</pre></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="optimize-indices"></a>Optimize Indices</h3></div></div></div><p>Yesterday’s index is unlikely to change.<a id="id-1.9.6.15.8.2.1" class="indexterm"></a>
<a id="id-1.9.6.15.8.2.2" class="indexterm"></a>  Log events are static: what
happened in the past stays in the past.  If we merge each shard down to just a
single segment, it’ll use fewer resources and will be quicker to query. We
can do this with the <a class="xref" href="inside-a-shard.html#optimize-api" title="optimize API">optimize API</a>.</p><p>It would be a bad idea to optimize the index while it was still allocated to
the <code class="literal">strong</code> boxes, as the optimization process could swamp the I/O on those
nodes and impact the indexing of today’s logs.  But the <code class="literal">medium</code> boxes aren’t
doing very much at all, so we are safe to optimize.</p><p>Yesterday’s index may have replica shards.<a id="id-1.9.6.15.8.4.1" class="indexterm"></a>
<a id="id-1.9.6.15.8.4.2" class="indexterm"></a> If we issue an optimize request, it
will optimize the primary shard and the replica shards, which is a waste.
Instead, we can remove the replicas temporarily, optimize, and then restore the
replicas:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">POST /logs_2014-09-30/_settings
{ "number_of_replicas": 0 }

POST /logs_2014-09-30/_optimize?max_num_segments=1

POST /logs_2014-09-30/_settings
{ "number_of_replicas": 1 }</pre></div><p>Of course, without replicas, we run the risk of losing data if a disk suffers
catastrophic failure.  You may<a id="id-1.9.6.15.8.6.1" class="indexterm"></a> want to back up the data first, with the
<a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html" target="_top"><code class="literal">snapshot-restore</code> API</a>.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="close-indices"></a>Closing Old Indices</h3></div></div></div><p>As indices get even older, they reach a point where they are almost never
accessed.<a id="id-1.9.6.15.9.2.1" class="indexterm"></a>
<a id="id-1.9.6.15.9.2.2" class="indexterm"></a>  We could delete them at this stage, but perhaps you want to keep
them around just in case somebody asks for them in six months.</p><p>These indices can be closed. They will still exist in the cluster, but they
won’t consume resources other than disk space.  Reopening an index is much
quicker than restoring it from backup.</p><p>Before closing, it is worth flushing the index to make sure that there are no
transactions left in the transaction log.  An empty transaction log will make
index recovery faster when it is reopened:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">POST /logs_2014-01-*/_flush <a id="CO299-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
POST /logs_2014-01-*/_close <a id="CO299-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
POST /logs_2014-01-*/_open <a id="CO299-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO299-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Flush all indices from January to empty the transaction logs.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO299-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Close all indices from January.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO299-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
When you need access to them again, reopen them with the <code class="literal">open</code> API.
</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="archive-indices"></a>Archiving Old Indices</h3></div></div></div><p>Finally, very old indices <a id="id-1.9.6.15.10.2.1" class="indexterm"></a>
<a id="id-1.9.6.15.10.2.2" class="indexterm"></a>can be archived off to some long-term storage like a
shared disk or Amazon’s S3 using the
<a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html" target="_top"><code class="literal">snapshot-restore</code> API</a>, just in case you may need
to access them in the future.  Once a backup exists, the index can be deleted
from the cluster.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="user-based"></a>User-Based Data</h2></div></div></div><p>Often, users start using Elasticsearch because they need to add full-text
search or analytics to an existing application.<a id="id-1.9.6.16.2.1" class="indexterm"></a>
<a id="id-1.9.6.16.2.2" class="indexterm"></a><a id="id-1.9.6.16.2.3" class="indexterm"></a>  They create a single index
that holds all of their documents.  Gradually, others in the company realize
how much benefit Elasticsearch brings, and they want to add their data to
Elasticsearch as well.</p><p>Fortunately, Elasticsearch supports
<a class="ulink" href="http://en.wikipedia.org/wiki/Multitenancy" target="_top">multitenancy</a> so each new user can
have her own index in the same cluster.<a id="id-1.9.6.16.3.2" class="indexterm"></a>  Occasionally, somebody will want to
search across the documents for all users, which they can do by searching
across all indices, but most of the time, users are interested in only their
own documents.</p><p>Some users have more documents than others, and some users will have heavier
search loads than others, so the ability to specify the number of primary shards
and replica shards that each index should have fits well with the index-per-user
model.<a id="id-1.9.6.16.4.1" class="indexterm"></a>
<a id="id-1.9.6.16.4.2" class="indexterm"></a><a id="id-1.9.6.16.4.3" class="indexterm"></a>
<a id="id-1.9.6.16.4.4" class="indexterm"></a> Similarly, busier indices can be allocated to stronger boxes with shard
allocation filtering. (See <a class="xref" href="scale.html#migrate-indices" title="Migrate Old Indices">Migrate Old Indices</a>.)</p><div class="tip admon"><div class="icon"><img alt="Tip" src="images/icons/tip.png" /></div><div class="admon_content"><p>Don’t just use the default number of primary shards for every index.
Think about how much data that index needs to hold.  It may be that all you
need is one shard—any more is a waste of resources.</p></div></div><p>Most users of Elasticsearch can stop here.  A simple index-per-user approach
is sufficient for the majority of cases.</p><p>In exceptional cases, you may find that you need to support a large number of
users, all with similar needs.  An example might be hosting a search engine
for thousands of email forums. <a id="id-1.9.6.16.7.1" class="indexterm"></a> Some forums may have a huge amount of traffic,
but the majority of forums are quite small.  Dedicating an index with a single
shard to a small forum is overkill—a single shard could hold the data for
many forums.</p><p>What we need is a way to share resources across users, to give the impression
that each user has his own index without wasting resources on small users.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="shared-index"></a>Shared Index</h2></div></div></div><p>We can use a large shared index for the many smaller <a id="id-1.9.6.17.2.1" class="indexterm"></a>
<a id="id-1.9.6.17.2.2" class="indexterm"></a><a id="id-1.9.6.17.2.3" class="indexterm"></a>
<a id="id-1.9.6.17.2.4" class="indexterm"></a>forums by indexing
the forum identifier in a field and using it as a filter:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /forums
{
  "settings": {
    "number_of_shards": 10 <a id="CO300-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
  },
  "mappings": {
    "post": {
      "properties": {
        "forum_id": { <a id="CO300-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
          "type":  "string",
          "index": "not_analyzed"
        }
      }
    }
  }
}

PUT /forums/post/1
{
  "forum_id": "baking", <a id="CO300-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
  "title":    "Easy recipe for ginger nuts",
  ...
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO300-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create an index large enough to hold thousands of smaller forums.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO300-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> <a href="#CO300-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Each post must include a <code class="literal">forum_id</code> to identify which forum it belongs
    to.
</p></td></tr></table></div><p>We can use the <code class="literal">forum_id</code> as a filter to search within a single forum.  The
filter will exclude most of the documents in the index (those from other
forums), and filter caching will ensure that responses are fast:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">GET /forums/post/_search
{
  "query": {
    "filtered": {
      "query": {
        "match": {
          "title": "ginger nuts"
        }
      },
      "filter": {
        "term": { <a id="CO301-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
          "forum_id": {
            "baking"
          }
        }
      }
    }
  }
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO301-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The <code class="literal">term</code> filter is cached by default.
</p></td></tr></table></div><p>This approach works, but we can do better. <a id="id-1.9.6.17.8.1" class="indexterm"></a>
<a id="id-1.9.6.17.8.2" class="indexterm"></a> The posts from a single forum
would fit easily onto one shard, but currently they are scattered across all ten
shards in the index. This means that every search request has to be forwarded
to a primary or replica of all ten shards. What would be ideal is to ensure
that all the posts from a single forum are stored on the same shard.</p><p>In <a class="xref" href="distributed-docs.html#routing-value" title="Routing a Document to a Shard">Routing a Document to a Shard</a>, we explained<a id="id-1.9.6.17.9.2" class="indexterm"></a> that a document is allocated to a
particular shard by using this formula:</p><pre class="literallayout">shard = hash(routing) % number_of_primary_shards</pre><p>The <code class="literal">routing</code> value defaults to the document’s <code class="literal">_id</code>, but we can override that
and provide our own custom routing value, such as <code class="literal">forum_id</code>.  All
documents with the same <code class="literal">routing</code> value will be stored on the same shard:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /forums/post/1?routing=baking <a id="CO302-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
{
  "forum_id": "baking", <a id="CO302-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
  "title":    "Easy recipe for ginger nuts",
  ...
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO302-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> <a href="#CO302-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Using <code class="literal">forum_id</code> as the routing value ensures that all posts from the
    same forum are stored on the same shard.
</p></td></tr></table></div><p>When we search for posts in a particular forum, we can pass the same <code class="literal">routing</code>
value to ensure that the search request is run on only the single shard that
holds our documents:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">GET /forums/post/_search?routing=baking <a id="CO303-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
{
  "query": {
    "filtered": {
      "query": {
        "match": {
          "title": "ginger nuts"
        }
      },
      "filter": {
        "term": { <a id="CO303-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
          "forum_id": {
            "baking"
          }
        }
      }
    }
  }
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO303-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The query is run on only the shard that corresponds to this <code class="literal">routing</code> value.
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO303-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
We still need the filter, as a single shard can hold posts from many forums.
</p></td></tr></table></div><p>Multiple forums can be queried by passing a comma-separated list of <code class="literal">routing</code>
values, and including each <code class="literal">forum_id</code> in a <code class="literal">terms</code> filter:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">GET /forums/post/_search?routing=baking,cooking,recipes
{
  "query": {
    "filtered": {
      "query": {
        "match": {
          "title": "ginger nuts"
        }
      },
      "filter": {
        "terms": {
          "forum_id": {
            [ "baking", "cooking", "recipes" ]
          }
        }
      }
    }
  }
}</pre></div><p>While this approach is technically efficient, it looks a bit clumsy because of
the need to specify <code class="literal">routing</code> values and <code class="literal">terms</code> filters on every query or
indexing request.  Index aliases to the rescue!</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="faking-it"></a>Faking Index per User with Aliases</h2></div></div></div><p>To keep our design simple and clean, we would<a id="id-1.9.6.18.2.1" class="indexterm"></a>
<a id="id-1.9.6.18.2.2" class="indexterm"></a><a id="id-1.9.6.18.2.3" class="indexterm"></a><a id="id-1.9.6.18.2.4" class="indexterm"></a> like our application to believe that
we have a dedicated index per user—or per forum in our example—even if
the reality is that we are using one big <a class="link" href="scale.html#shared-index" title="Shared Index">shared index</a>. To do
that, we need some way to hide the <code class="literal">routing</code> value and the filter on
<code class="literal">forum_id</code>.</p><p>Index aliases allow us to do just that. When you associate an alias with an
index, you can also specify a filter and routing values:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /forums/_alias/baking
{
  "routing": "baking",
  "filter": {
    "term": {
      "forum_id": "baking"
    }
  }
}</pre></div><p>Now, we can treat the <code class="literal">baking</code> alias as if it were its own index.  Documents
indexed into the <code class="literal">baking</code> alias automatically get the custom routing value
applied:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /baking/post/1 <a id="CO304-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
{
  "forum_id": "baking", <a id="CO304-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
  "title":    "Easy recipe for ginger nuts",
  ...
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO304-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> <a href="#CO304-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
We still need the <code class="literal">forum_id</code> field for the filter to work, but
    the custom routing value is now implicit.
</p></td></tr></table></div><p>Queries run against the <code class="literal">baking</code> alias are run just on the shard associated
with the custom routing value, and the results are automatically filtered by
the filter we specified:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">GET /baking/post/_search
{
  "query": {
    "match": {
      "title": "ginger nuts"
    }
  }
}</pre></div><p>Multiple aliases can be specified when searching across multiple forums:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">GET /baking,recipes/post/_search <a id="CO305-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
{
  "query": {
    "match": {
      "title": "ginger nuts"
    }
  }
}</pre></div><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO305-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Both <code class="literal">routing</code> values are applied, and results can match either filter.
</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="one-big-user"></a>One Big User</h2></div></div></div><p>Big, popular forums start out as small forums.<a id="id-1.9.6.19.2.1" class="indexterm"></a>
<a id="id-1.9.6.19.2.2" class="indexterm"></a>  One day we will find that one
shard in our shared index is doing a lot more work than the other shards,
because it holds the documents for a forum that has become very popular. That
forum now needs its own index.</p><p>The index aliases that we’re using to fake an index per user give us a clean
migration path for the big forum.<a id="id-1.9.6.19.3.1" class="indexterm"></a>
<a id="id-1.9.6.19.3.2" class="indexterm"></a>
<a id="id-1.9.6.19.3.3" class="indexterm"></a></p><p>The first step is to create a new index dedicated to the forum, and with the
appropriate number of shards to allow for expected growth:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">PUT /baking_v1
{
  "settings": {
    "number_of_shards": 3
  }
}</pre></div><p>The next step is to migrate the data from the shared index into the dedicated
index, which can be done using <a class="link" href="distributed-search.html#scan-scroll" title="scan and scroll">scan-and-scroll</a> and the
<a class="link" href="data-in-data-out.html#bulk" title="비용이 저렴한 bulk"><code class="literal">bulk</code> API</a>.  As soon as the migration is finished, the index alias
can be updated to point to the new index:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">POST /_aliases
{
  "actions": [
    { "remove": { "alias": "baking", "index": "forums"    }},
    { "add":    { "alias": "baking", "index": "baking_v1" }}
  ]
}</pre></div><p>Updating the alias is atomic; it’s like throwing a switch.  Your application
continues talking to the <code class="literal">baking</code> API and is completely unaware that it now
points to a new dedicated index.</p><p>The dedicated index no longer needs the filter or the routing values. We can
just rely on the default sharding that Elasticsearch does using each
document’s <code class="literal">_id</code> field.</p><p>The last step is to remove the old documents from the shared index, which can
be done with a <code class="literal">delete-by-query</code> request, using the original routing value and
forum ID:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">DELETE /forums/post/_query?routing=baking
{
  "query": {
    "term": {
      "forum_id": "baking"
    }
  }
}</pre></div><p>The beauty of this index-per-user model is that it allows you to reduce
resources, keeping costs low, while still giving you the flexibility to scale
out when necessary, and with zero downtime.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title"><a id="finite-scale"></a>Scale Is Not Infinite</h2></div></div></div><p>Throughout this chapter we have spoken about many of the ways that
Elasticsearch can scale. <a id="id-1.9.6.20.2.1" class="indexterm"></a>
<a id="id-1.9.6.20.2.2" class="indexterm"></a>Most scaling problems can be solved by adding more
nodes. But one resource is finite and should be treated with
respect: the cluster state.<a id="id-1.9.6.20.2.3" class="indexterm"></a></p><p>The <span class="emphasis"><em>cluster state</em></span> is a data structure that holds the following cluster-level information:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Cluster-level settings
</li><li class="listitem">
Nodes that are part of the cluster
</li><li class="listitem">
Indices, plus their settings, mappings, analyzers, warmers, and aliases
</li><li class="listitem">
The shards associated with each index, plus the node on which they are
  allocated
</li></ul></div><p>You can view the current cluster state with this request:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">GET /_cluster/state</pre></div><p>The cluster state exists on every node in the cluster,<a id="id-1.9.6.20.7.1" class="indexterm"></a>
<a id="id-1.9.6.20.7.2" class="indexterm"></a> including client nodes.
This is how any node can forward a request directly to the node that holds the
requested data—every node knows where every document lives.</p><p>Only the master node is allowed to update the cluster state.  Imagine that an
indexing request introduces a previously unknown field.  The node holding the
primary shard for the document must forward the new mapping to the master
node.  The master node incorporates the changes in the cluster state, and
publishes a new version to all of the nodes in the cluster.</p><p>Search requests <span class="emphasis"><em>use</em></span> the cluster state, but they don’t change it.  The same
applies to document-level CRUD requests unless, of course, they introduce a
new field that requires a mapping update. By and large, the cluster state is
static and is not a bottleneck.</p><p>However, remember that this same data structure has to exist in memory on
every node, and must be published to every node whenever it is updated.  The
bigger it is, the longer that process will take.</p><p>The most common problem that we see with the cluster state is the introduction
of too many fields. A user might decide to use a separate field for every IP
address, or every referer URL.  The following example keeps track of the number of
times a page has been visited by using a different field name for every unique
referer:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">POST /counters/pageview/home_page/_update
{
  "script": "ctx._source[referer]++",
  "params": {
    "referer": "http://www.foo.com/links?bar=baz"
  }
}</pre></div><p>This approach is catastrophically bad! It will result in millions of fields,
all of which have to be stored in the cluster state.  Every time a new referer
is seen, a new field is added to the already bloated cluster state, which then
has to be published to every node in the cluster.</p><p>A much better approach <a id="id-1.9.6.20.14.1" class="indexterm"></a><a id="id-1.9.6.20.14.2" class="indexterm"></a>
<a id="id-1.9.6.20.14.3" class="indexterm"></a>is to use <a class="link" href="nested-objects.html" title="Nested Objects">nested objects</a>, with one
field for the parameter name—<code class="literal">referer</code>— and another field for its
associated value—<code class="literal">count</code>:</p><div class="pre_wrapper"><pre class="programlisting prettyprint lang-json">    "counters": [
      { "referer": "http://www.foo.com/links?bar=baz",  "count": 2 },
      { "referer": "http://www.linkbait.com/article_3", "count": 10 },
      ...
    ]</pre></div><p>The nested approach may increase the number of documents, but Elasticsearch is
built to handle that.  The important thing is that it keeps the cluster state
small and agile.</p><p>Eventually, despite your best intentions, you may find that the number of
nodes and indices and mappings that you have is just too much for one cluster.
At this stage, it is probably worth dividing the problem into multiple
clusters.  Thanks to <a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-tribe.html" target="_top"><code class="literal">tribe</code> nodes</a>, you can even run
searches across multiple clusters, as if they were one big cluster.</p></div></div><div class="navfooter"><span class="prev"><a href="parent-child.html">
              « 
              Parent-Child Relationship</a>
           
        </span><span class="next">
           
          <a href="administration.html">관리, 모니터링, 배포 
               »
            </a></span></div>
<!-- end body -->

            </section>

            <div id="rtpcontainer"
        >
	<h3>Top Videos</h3>
	<ul class="lists">
		<li><a href="https://www.elastic.co/webinars/get-started-with-elasticsearch/?baymax=default&elektra=docs&storm=top-video">Getting Started</a></li>
		<li><a href="http://www.elastic.co/webinars/elasticsearch-performance-optimization-tale-two-tickets/?elektra=docs">Scaling</a></li>
		<li><a href="http://www.elastic.co/webinars/shield-securing-your-data-in-elasticsearch/?elektra=docs">Security</a></li>
	</ul>
</div>


	  	




<script src="//app-lon02.marketo.com/js/forms2/js/forms2.min.js"></script>
	<!-- subscribe-newsletter -->
	<div id="subscribe-newsletter">
		<div class="container">
			<div class="row">
				<div class="12u">
					<div class="subscribe-wrapper" id="rtp-newsletter">
						<header>
							<h3>Subscribe to our newsletter</h3>
						</header>
						<div class="subscribe-form">
							<form id="mktoForm_1398" class="container"></form>
							<div class="form_thanks hide">
								
									<p>Thanks for subscribing! We'll keep you updated with new releases.</p>
								
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>

  	<!-- Footer -->
	<div id="footer-wrapper">
		<section id="footer" class="container">
			<div class="row">
				<div class="12u">
					<!-- Copyright -->
					<div id="copyright">
	<p>
		© 2016. All Rights Reserved - Elasticsearch
	</p>
	<ul class="links">
		<li>Elasticsearch is a trademark of Elasticsearch BV, registered in the U.S. and in other countries</li>
		<li><a href="/legal/trademarks" id="footer_trademarks">Trademarks</a></li>
		<li><a href="/legal/terms-of-use" id="footer_terms">Terms</a></li>
		<li><a href="/legal/privacy-policy" id="footer_privacy">Privacy</a></li>
	</ul>
	<p>
		Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of the <a href="http://www.apache.org/" target="_blank">Apache Software Foundation</a> in the United States and/or other&nbsp;countries.
	</p>
</div><!--<div class="row">
{replace4}
</div>-->
				</div>
			</div>					
		</section>
	</div>
			
	<!-- Social Media -->
	<div id="socialmedia-wrapper">
		<section id="socialmedia" class="container">
			<!-- social icons -->
			<div id="social">
				<ul class="links">
					
						<li class="facebook grayscale"><a href="http://www.facebook.com/elastic.co" target="_blank" id="footer_facebook"></a></li>
					
						<li class="twitter grayscale"><a href="https://www.twitter.com/elastic" target="_blank" id="footer_twitter"></a></li>
					
						<li class="linkedin grayscale"><a href="https://www.linkedin.com/company/elastic-co" target="_blank" id="footer_linkedin"></a></li>
					
						<li class="xing grayscale"><a href="https://www.xing.com/companies/elastic.co" target="_blank" id="footer_xing"></a></li>
					
						<li class="youtube grayscale"><a href="https://www.youtube.com/user/elasticsearch" target="_blank" id="footer_youtube"></a></li>
					
				</ul>
			</div>
		</section>
	</div>


<style type="text/css">
	
		.facebook{background:url("https://static-www.elastic.co/assets/bltda40eebd7c7cae9a/facebook.svg?q=100") no-repeat;}		
	
		.twitter{background:url("https://static-www.elastic.co/assets/bltfd3b1511512f4632/twitter.svg?q=100") no-repeat;}		
	
		.linkedin{background:url("https://static-www.elastic.co/assets/blt6b4f6f8d6b96f814/linkedin.svg?q=100") no-repeat;}		
	
		.xing{background:url("https://static-www.elastic.co/assets/blta76b2ae2b2f10a98/xing.svg?q=100") no-repeat;}		
	
		.youtube{background:url("https://static-www.elastic.co/assets/blt5761648c9581fb53/youtube.svg?q=100") no-repeat;}		
	
</style>

<script>
MktoForms2.loadForm("//app-lon02.marketo.com", "813-MAM-392", 1398,function(form){
	form.onSuccess(function(form){
		$('#mktoForm_1398').css('display','none');
		$('#subscribe-newsletter header').hide();
		$('#mktoForm_1398').siblings('.form_thanks').removeClass('hide')
		dataLayer.push({'event': 'mktoFormSubmit'});
	    return false;
  	});
});
</script>	

  	</div>
  	<!-- <div class="preloader">
    	<div class="status"></div>
	</div> -->
  	<style></style>
   	<script type="text/javascript">
	var suggestionsUrl = "https://search.elastic.co/suggest";	
</script>
<script src="https://static-www.elastic.co/static/js/jquery.autocomplete.js?q=100"></script>
<script src="https://static-www.elastic.co/static/js/script.js?q=100"></script>


   	
   	
   	
   	<script type="application/ld+json">
	
		
	
	{
	  	"@context": "http://schema.org",
	  	"@type": "Organization",
	  	"name" : "Elastic",
	  	"url": "https://www.elastic.co/",
	  	"logo": "https://www.elastic.co/static/img/elastic-logo-200.png",
		"sameAs" : [ "https://www.facebook.com/elastic.co",
			"https://twitter.com/elastic",
			"https://plus.google.com/105178019064686397293",
			"https://www.youtube.com/user/elasticsearch",
			"https://www.linkedin.com/company/elasticsearch"
		],
	  	"potentialAction": {
	    	"@type": "SearchAction",
	    	"target": "https://www.elastic.co/search?q={query_string}",
	    	"query-input": "required name=query_string"
	  	}
	}
</script>	

            <script type="text/javascript" src="docs.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script type='text/javascript' src='https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js'></script>

            </body>
        

